{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91717,"databundleVersionId":12184666,"sourceType":"competition"},{"sourceId":12064209,"sourceType":"datasetVersion","datasetId":7593561}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"histgradientboostingclassifier","metadata":{"editable":false}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n# No xgboost needed\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nimport gc\nimport warnings\nimport os\n# Optuna is no longer used for tuning, but keep import if it's potentially used elsewhere in the notebook\n# import optuna \n# Import HistGradientBoostingClassifier\nfrom sklearn.ensemble import HistGradientBoostingClassifier\n# log_loss is not explicitly used for HGB evaluation here, so it's not strictly needed.\n# from sklearn.metrics import log_loss \n\nwarnings.filterwarnings('ignore')\n\n# --- Reusable Functions ---\ndef calculate_map3(y_true, y_pred_proba):\n    \"\"\"Calculates the Mean Average Precision @ 3 score.\"\"\"\n    # Get the indices of the top 3 predictions for each sample\n    top3_preds_indices = np.argsort(y_pred_proba, axis=1)[:, ::-1][:, :3]\n    \n    scores = []\n    for i, true_label in enumerate(y_true):\n        top3 = top3_preds_indices[i]\n        score = 0.0\n        # Check if the true label is in the top 3 predictions\n        if true_label in top3:\n            # Find the rank (1, 2, or 3)\n            rank = np.where(top3 == true_label)[0][0] + 1\n            if rank == 1:\n                score = 1.0\n            elif rank == 2:\n                score = 0.5\n            elif rank == 3:\n                score = 1/3\n        scores.append(score)\n    return np.mean(scores)\n\n# --- Feature Engineering Function (Converts Numerical to Binned Categorical) ---\ndef feature_eng(df, target_col=None):\n    # Identify numerical columns to be binned\n    numerical_features = [col for col in df.select_dtypes(include=['int64', 'float64']).columns\n                            if col != 'id'] # Exclude 'id'\n    if target_col and target_col in numerical_features: # Exclude target if it's numerical\n        numerical_features.remove(target_col)\n\n    for col in numerical_features:\n        # Create a new column with '_Binned' suffix\n        # Convert numerical values to string, then to 'category' dtype\n        if f'{col}_Binned' not in df.columns: # Avoid recreating if already exists\n            df[f'{col}_Binned'] = df[col].astype(str).astype('category')\n    return df\n\n# --- Main Execution ---\nif __name__ == \"__main__\":\n    # Define target column name\n    TARGET_COL = 'Fertilizer Name'\n\n    # --- 1. Load Data ---\n    print(\"Loading datasets...\")\n    train_synthetic_df = pd.read_csv(\"/kaggle/input/playground-series-s5e6/train.csv\")\n    train_original_df = pd.read_csv(\"/kaggle/input/original/Fertilizer Prediction .csv\")\n    test_df_raw = pd.read_csv(\"/kaggle/input/playground-series-s5e6/test.csv\")\n    submission_df = pd.read_csv(\"/kaggle/input/playground-series-s5e6/sample_submission.csv\")\n\n    # Drop 'id' columns\n    train_synthetic_df = train_synthetic_df.drop(columns=['id'])\n    if 'id' in test_df_raw.columns:\n        test_df_raw = test_df_raw.drop(columns=['id'])\n\n    # --- Apply Feature Engineering to make all numerical features categorical ---\n    print(\"Applying feature engineering (binning numerical features into categories)...\")\n    train_synthetic_df = feature_eng(train_synthetic_df, target_col=TARGET_COL)\n    train_original_df = feature_eng(train_original_df, target_col=TARGET_COL)\n    test_df_raw = feature_eng(test_df_raw)\n\n    # --- 2. Preprocessing and Feature Preparation ---\n    print(\"Applying preprocessing...\")\n    # Target Encoding\n    le_fertilizer = LabelEncoder()\n    # Fit on combined labels to ensure all possible classes are covered\n    le_fertilizer.fit(pd.concat([train_synthetic_df[TARGET_COL], train_original_df[TARGET_COL]]))\n    y_synth = le_fertilizer.transform(train_synthetic_df[TARGET_COL])\n    y_orig = le_fertilizer.transform(train_original_df[TARGET_COL])\n    num_classes = len(le_fertilizer.classes_)\n\n    # Dynamically determine the common feature columns after feature engineering\n    base_categorical_features = ['Soil Type', 'Crop Type']\n    \n    # Collect all binned columns that are present across all three dataframes\n    all_processed_dfs = [train_synthetic_df, train_original_df, test_df_raw]\n    common_binned_cols = set()\n\n    # Get binned columns from the first dataframe\n    if all_processed_dfs:\n        first_df_binned = [col for col in all_processed_dfs[0].columns if col.endswith('_Binned')]\n        common_binned_cols.update(first_df_binned)\n\n        # Intersect with binned columns from subsequent dataframes\n        for df_item in all_processed_dfs[1:]:\n            current_df_binned = [col for col in df_item.columns if col.endswith('_Binned')]\n            common_binned_cols.intersection_update(current_df_binned)\n\n    # The final feature_cols will be the base categorical plus the common binned ones\n    feature_cols = base_categorical_features + sorted(list(common_binned_cols))\n    \n    print(f\"Selected feature columns (all categorical): {feature_cols}\")\n\n    # Feature Sets - now containing only categorical/binned features\n    X_synth = train_synthetic_df[feature_cols].copy()\n    X_orig = train_original_df[feature_cols].copy()\n    test_df = test_df_raw[feature_cols].copy() # Ensure test_df also has only selected features\n\n    # Categorical Feature Encoding (Using LabelEncoder for consistent integer mapping)\n    for col in feature_cols:\n        # Concatenate all categories from all datasets for consistent mapping\n        full_vocab = pd.concat([X_synth[col].astype(str), X_orig[col].astype(str), test_df[col].astype(str)], axis=0).unique()\n        \n        label_enc = LabelEncoder()\n        label_enc.fit(full_vocab.astype(str)) # Fit on the full vocabulary of strings\n        \n        X_synth[col] = label_enc.transform(X_synth[col].astype(str))\n        X_orig[col] = label_enc.transform(X_orig[col].astype(str))\n        test_df[col] = label_enc.transform(test_df[col].astype(str))\n        \n        # Cast to 'category' dtype after encoding. This is crucial for HGB's native categorical handling.\n        X_synth[col] = X_synth[col].astype(\"category\")\n        X_orig[col] = X_orig[col].astype(\"category\")\n        test_df[col] = test_df[col].astype(\"category\")\n\n    # --- 3. Define Fixed Hyperparameters for HistGradientBoostingClassifier ---\n    # These parameters are provided by the user.\n    final_params = {\n        'max_iter': 1236,\n        'learning_rate': 0.07913270952323785,\n        'max_depth': 4,\n        'min_samples_leaf': 55,\n        'l2_regularization': 0.045227288910538066,\n        'max_leaf_nodes': 39,\n        'random_state': 42, # Keep seed fixed\n        'early_stopping': True, # Enable early stopping\n        'n_iter_no_change': 50, # Number of iterations with no improvement to trigger early stopping\n        'verbose': 0, # Suppress verbose output during training, or set to 1 for progress bar\n    }\n    \n    print(\"\\nUsing fixed HistGradientBoostingClassifier parameters:\")\n    for key, value in final_params.items():\n        print(f\"  {key}: {value}\")\n\n    # --- 4. Train HistGradientBoostingClassifier Model with 5-Fold CV & 5x Original Data Augmentation ---\n    print(\"\\nTraining HistGradientBoostingClassifier with 5x original data (5 Folds)...\")\n    NFOLDS = 5 # Fixed to 5 folds for final training and prediction\n    skf = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=42)\n\n    oof_preds = np.zeros((len(X_synth), num_classes))\n    test_preds = np.zeros((len(test_df), num_classes))\n\n    for fold, (train_idx, val_idx) in enumerate(skf.split(X_synth, y_synth)):\n        print(f\"--- Fold {fold+1}/{NFOLDS} ---\")\n        X_synth_train, y_synth_train = X_synth.iloc[train_idx], y_synth[train_idx]\n        X_synth_val, y_synth_val = X_synth.iloc[val_idx], y_synth[val_idx]\n\n        # Octuple the original data to give it more weight (5x)\n        X_orig_oct = pd.concat([X_orig] * 5, ignore_index=True)\n        y_orig_oct = np.concatenate([y_orig] * 5)\n\n        # Augment the training data with the OCTUPLED original dataset\n        X_train_aug = pd.concat([X_synth_train, X_orig_oct], ignore_index=True)\n        y_train_aug = np.concatenate([y_synth_train, y_orig_oct])\n\n        # Initialize HGB with fixed parameters\n        model = HistGradientBoostingClassifier(**final_params)\n        model.fit(X_train_aug, y_train_aug)\n\n        oof_preds[val_idx] = model.predict_proba(X_synth_val)\n        test_preds += model.predict_proba(test_df) / NFOLDS\n        gc.collect()\n\n    # --- 5. Evaluate and Save ---\n    map3_score = calculate_map3(y_synth, oof_preds)\n    print(f\"\\n--- Final Model CV MAP@3 (5x Original Data, Fixed Params): {map3_score:.5f} ---\\n\")\n\n    # Save OOF and Test Predictions\n    output_dir = './' # Save in current directory\n    os.makedirs(output_dir, exist_ok=True) # Ensure directory exists\n\n    oof_filename = os.path.join(output_dir, 'oof_preds_hgb_fixed_params.npy')\n    test_filename = os.path.join(output_dir, 'test_preds_hgb_fixed_params.npy')\n\n    np.save(oof_filename, oof_preds)\n    np.save(test_filename, test_preds)\n    print(f\"OOF predictions saved to: {oof_filename}\")\n    print(f\"Test predictions saved to: {test_filename}\")\n\n    print(\"Generating final submission file...\")\n    top3_preds_indices = np.argsort(test_preds, axis=1)[:, ::-1][:, :3]\n    top3_preds_labels = le_fertilizer.inverse_transform(top3_preds_indices.flatten()).reshape(top3_preds_indices.shape)\n\n    submission_df['Fertilizer Name'] = [' '.join(row) for row in top3_preds_labels]\n    submission_df.to_csv('submission_hgb_fixed_params.csv', index=False)\n\n    print(\"Submission file 'submission_hgb_fixed_params.csv' created successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T09:22:43.020039Z","iopub.execute_input":"2025-06-22T09:22:43.021482Z","iopub.status.idle":"2025-06-22T10:17:42.039421Z","shell.execute_reply.started":"2025-06-22T09:22:43.021440Z","shell.execute_reply":"2025-06-22T10:17:42.038359Z"},"editable":false},"outputs":[{"name":"stdout","text":"Loading datasets...\nApplying feature engineering (binning numerical features into categories)...\nApplying preprocessing...\nSelected feature columns (all categorical): ['Soil Type', 'Crop Type', 'Humidity_Binned', 'Moisture_Binned', 'Nitrogen_Binned', 'Phosphorous_Binned', 'Potassium_Binned', 'Temparature_Binned']\n\nUsing fixed HistGradientBoostingClassifier parameters:\n  max_iter: 1236\n  learning_rate: 0.07913270952323785\n  max_depth: 4\n  min_samples_leaf: 55\n  l2_regularization: 0.045227288910538066\n  max_leaf_nodes: 39\n  random_state: 42\n  early_stopping: True\n  n_iter_no_change: 50\n  verbose: 0\n\nTraining HistGradientBoostingClassifier with 5x original data (5 Folds)...\n--- Fold 1/5 ---\n--- Fold 2/5 ---\n--- Fold 3/5 ---\n--- Fold 4/5 ---\n--- Fold 5/5 ---\n\n--- Final Model CV MAP@3 (5x Original Data, Fixed Params): 0.34283 ---\n\nOOF predictions saved to: ./oof_preds_hgb_fixed_params.npy\nTest predictions saved to: ./test_preds_hgb_fixed_params.npy\nGenerating final submission file...\nSubmission file 'submission_hgb_fixed_params.csv' created successfully.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"GaussianNB\n","metadata":{"editable":false}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler # Added OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import StratifiedKFold\nimport gc\nimport warnings\nimport os\nimport optuna # Added for hyperparameter optimization\nfrom sklearn.naive_bayes import GaussianNB # Added for Naive Bayes classifier\n\n# Suppress warnings for cleaner output\nwarnings.filterwarnings('ignore')\n\n# --- Reusable Functions ---\ndef calculate_map3(y_true, y_pred_proba):\n    \"\"\"Calculates the Mean Average Precision @ 3 score.\"\"\"\n    # Get the indices of the top 3 predictions for each sample\n    top3_preds_indices = np.argsort(y_pred_proba, axis=1)[:, ::-1][:, :3]\n    \n    scores = []\n    for i, true_label in enumerate(y_true):\n        top3 = top3_preds_indices[i]\n        score = 0.0\n        # Check if the true label is in the top 3 predictions\n        if true_label in top3:\n            # Find the rank (1, 2, or 3)\n            rank = np.where(top3 == true_label)[0][0] + 1\n            if rank == 1:\n                score = 1.0\n            elif rank == 2:\n                score = 0.5\n            elif rank == 3:\n                score = 1/3\n        scores.append(score)\n    return np.mean(scores)\n\n# --- Feature Engineering Function (Converts Numerical to Binned Categorical) ---\ndef feature_eng(df, target_col=None):\n    # Identify numerical columns to be binned\n    numerical_features = [col for col in df.select_dtypes(include=['int64', 'float64']).columns\n                            if col != 'id'] # Exclude 'id'\n    if target_col and target_col in numerical_features: # Exclude target if it's numerical\n        numerical_features.remove(target_col)\n\n    for col in numerical_features:\n        # Create a new column with '_Binned' suffix\n        # Convert numerical values to string, then to 'category' dtype\n        if f'{col}_Binned' not in df.columns: # Avoid recreating if already exists\n            df[f'{col}_Binned'] = df[col].astype(str).astype('category')\n    return df\n\n# --- Optuna Objective Function for Naive Bayes Hyperparameter Tuning ---\ndef nb_objective(trial, X_synth_processed, y_synth, X_orig_processed, y_orig, num_classes, calculate_map3_func):\n    \"\"\"\n    Objective function for Optuna to optimize Naive Bayes hyperparameters.\n    Accepts already processed (scaled/encoded) dataframes.\n    \"\"\"\n    # GaussianNB has 'var_smoothing' as its main hyperparameter\n    var_smoothing = trial.suggest_float('var_smoothing', 1e-10, 1e-5, log=True)\n    \n    NFOLDS_OPTUNA = 5 # Fixed to 5 folds for Optuna\n    skf_optuna = StratifiedKFold(n_splits=NFOLDS_OPTUNA, shuffle=True, random_state=42)\n\n    oof_preds_trial = np.zeros((len(X_synth_processed), num_classes))\n    \n    # Octuple the original data for augmentation, consistent with previous models\n    # Note: X_orig_processed is already a NumPy array here.\n    X_orig_oct_obj = np.vstack([X_orig_processed] * 5)\n    y_orig_oct_obj = np.concatenate([y_orig] * 5)\n\n    for fold_num, (train_idx, val_idx) in enumerate(skf_optuna.split(X_synth_processed, y_synth)):\n        # Ensure that indexing on NumPy arrays works correctly (direct slicing)\n        X_synth_train, y_synth_train = X_synth_processed[train_idx], y_synth[train_idx]\n        X_synth_val, y_synth_val = X_synth_processed[val_idx], y_synth[val_idx]\n\n        # Augment the training data with the OCTUPLED original dataset\n        X_train_aug = np.vstack([X_synth_train, X_orig_oct_obj])\n        y_train_aug = np.concatenate([y_synth_train, y_orig_oct_obj])\n\n        model = GaussianNB(var_smoothing=var_smoothing)\n        model.fit(X_train_aug, y_train_aug)\n\n        oof_preds_trial[val_idx] = model.predict_proba(X_synth_val)\n        \n        del model\n        gc.collect()\n\n    map3_score_trial = calculate_map3_func(y_synth, oof_preds_trial)\n    return map3_score_trial\n\n\n# --- Main Execution ---\nif __name__ == \"__main__\":\n    # Define target column name\n    TARGET_COL = 'Fertilizer Name'\n\n    # --- 1. Load Data ---\n    print(\"Loading datasets...\")\n    train_synthetic_df = pd.read_csv(\"/kaggle/input/playground-series-s5e6/train.csv\")\n    train_original_df = pd.read_csv(\"/kaggle/input/original/Fertilizer Prediction .csv\") \n    test_df_raw = pd.read_csv(\"/kaggle/input/playground-series-s5e6/test.csv\")\n    submission_df = pd.read_csv(\"/kaggle/input/playground-series-s5e6/sample_submission.csv\")\n\n    # Drop 'id' columns\n    train_synthetic_df = train_synthetic_df.drop(columns=['id'])\n    if 'id' in test_df_raw.columns:\n        test_df_raw = test_df_raw.drop(columns=['id'])\n\n    # --- Apply Feature Engineering to make all numerical features categorical ---\n    print(\"Applying feature engineering (binning numerical features into categories)...\")\n    train_synthetic_df = feature_eng(train_synthetic_df, target_col=TARGET_COL)\n    train_original_df = feature_eng(train_original_df, target_col=TARGET_COL)\n    test_df_raw = feature_eng(test_df_raw)\n\n    # --- 2. Preprocessing and Feature Preparation ---\n    print(\"Applying preprocessing (Label Encoding target, One-Hot Encoding categorical features, Scaling numerical features)...\")\n    \n    # Target Encoding\n    le_fertilizer = LabelEncoder()\n    # Fit on combined labels to ensure all possible classes are covered\n    le_fertilizer.fit(pd.concat([train_synthetic_df[TARGET_COL], train_original_df[TARGET_COL]]))\n    y_synth = le_fertilizer.transform(train_synthetic_df[TARGET_COL])\n    y_orig = le_fertilizer.transform(train_original_df[TARGET_COL])\n    num_classes = len(le_fertilizer.classes_)\n\n    # Separate feature columns into numerical and categorical\n    # Use the original numerical columns before binning for potential scaling\n    original_numerical_features = [col for col in train_synthetic_df.select_dtypes(include=['int64', 'float64']).columns\n                                   if col != 'id' and col != TARGET_COL]\n    \n    # The categorical features are the base_categorical_features PLUS all the '_Binned' columns\n    base_categorical_features = ['Soil Type', 'Crop Type']\n    binned_categorical_features = [col for col in train_synthetic_df.columns if col.endswith('_Binned')]\n    all_categorical_features = base_categorical_features + binned_categorical_features\n    \n    print(f\"Original numerical features (for scaling): {original_numerical_features}\")\n    print(f\"All categorical features (for one-hot encoding): {all_categorical_features}\")\n\n    # Prepare dataframes for processing\n    # Drop the original numerical columns that are now binned if you don't want them alongside binned\n    # For Naive Bayes, often it's better to keep numerical if they are scaled.\n    # We will keep both the original numericals (scaled) and the binned categoricals (one-hot encoded).\n    \n    # --- Apply Scaling to Original Numerical Features ---\n    scaler = StandardScaler()\n    # Fit on combined numerical data for consistency\n    scaler.fit(pd.concat([train_synthetic_df[original_numerical_features], \n                          train_original_df[original_numerical_features], \n                          test_df_raw[original_numerical_features]], ignore_index=True))\n    \n    X_synth_numerical_scaled = scaler.transform(train_synthetic_df[original_numerical_features])\n    X_orig_numerical_scaled = scaler.transform(train_original_df[original_numerical_features])\n    X_test_numerical_scaled = scaler.transform(test_df_raw[original_numerical_features])\n\n    # --- Apply One-Hot Encoding to Categorical Features ---\n    # Concatenate all categorical data to fit the encoder for consistent columns\n    all_cat_data = pd.concat([train_synthetic_df[all_categorical_features].astype(str), \n                              train_original_df[all_categorical_features].astype(str), \n                              test_df_raw[all_categorical_features].astype(str)], ignore_index=True)\n    \n    one_hot_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False) # sparse_output=False for dense array\n    one_hot_encoder.fit(all_cat_data)\n    \n    X_synth_cat_encoded = one_hot_encoder.transform(train_synthetic_df[all_categorical_features].astype(str))\n    X_orig_cat_encoded = one_hot_encoder.transform(train_original_df[all_categorical_features].astype(str))\n    X_test_cat_encoded = one_hot_encoder.transform(test_df_raw[all_categorical_features].astype(str))\n\n    # --- Combine Scaled Numerical and One-Hot Encoded Categorical Features ---\n    # Concatenate horizontally\n    X_synth_processed = np.hstack([X_synth_numerical_scaled, X_synth_cat_encoded])\n    X_orig_processed = np.hstack([X_orig_numerical_scaled, X_orig_cat_encoded])\n    X_test_processed = np.hstack([X_test_numerical_scaled, X_test_cat_encoded])\n\n    print(f\"Processed synthetic training features shape: {X_synth_processed.shape}\")\n    print(f\"Processed original training features shape: {X_orig_processed.shape}\")\n    print(f\"Processed test features shape: {X_test_processed.shape}\")\n\n    # --- 3. Naive Bayes Model Training ---\n    # --- Optuna Hyperparameter Optimization for Naive Bayes ---\n    print(\"\\nStarting Optuna hyperparameter optimization for Naive Bayes (5-Fold CV)...\")\n    nb_study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))\n    nb_study.optimize(\n        lambda trial: nb_objective(trial, X_synth_processed, y_synth, X_orig_processed, y_orig, num_classes, calculate_map3),\n        n_trials=20, # Fewer trials for Naive Bayes as it has fewer hyperparameters\n        show_progress_bar=True\n    )\n\n    print(\"\\nNaive Bayes Optuna optimization finished.\")\n    print(f\"Best Naive Bayes trial: {nb_study.best_trial.value:.5f} (MAP@3)\")\n    print(\"Best Naive Bayes hyperparameters found:\")\n    for key, value in nb_study.best_params.items():\n        print(f\"  {key}: {value}\")\n\n    # --- Define Final Naive Bayes Hyperparameters using Optuna's Best ---\n    final_nb_params = {\n        'var_smoothing': nb_study.best_params['var_smoothing']\n    }\n\n    # --- Train Final Naive Bayes Model with 5-Fold CV & 5x Original Data Augmentation (using Best Params) ---\n    print(\"\\nTraining final Naive Bayes model with best hyperparameters (5-Fold CV & 5x original data augmentation)...\")\n    NFOLDS_NB = 5\n    skf_nb = StratifiedKFold(n_splits=NFOLDS_NB, shuffle=True, random_state=42)\n\n    oof_preds_nb = np.zeros((len(X_synth_processed), num_classes))\n    test_preds_nb = np.zeros((len(X_test_processed), num_classes)) # Use X_test_processed here\n\n    for fold, (train_idx, val_idx) in enumerate(skf_nb.split(X_synth_processed, y_synth)):\n        print(f\"--- Naive Bayes Fold {fold+1}/{NFOLDS_NB} ---\")\n        X_synth_train, y_synth_train = X_synth_processed[train_idx], y_synth[train_idx]\n        X_synth_val, y_synth_val = X_synth_processed[val_idx], y_synth[val_idx]\n\n        # Octuple the original data to give it more weight (5x)\n        X_orig_oct = np.vstack([X_orig_processed] * 5)\n        y_orig_oct = np.concatenate([y_orig] * 5)\n\n        # Augment the training data with the OCTUPLED original dataset\n        X_train_aug = np.vstack([X_synth_train, X_orig_oct])\n        y_train_aug = np.concatenate([y_synth_train, y_orig_oct])\n\n        model_nb = GaussianNB(**final_nb_params)\n        model_nb.fit(X_train_aug, y_train_aug)\n\n        oof_preds_nb[val_idx] = model_nb.predict_proba(X_synth_val)\n        test_preds_nb += model_nb.predict_proba(X_test_processed) / NFOLDS_NB # Use X_test_processed here\n        \n        del model_nb\n        gc.collect()\n\n    map3_score_nb = calculate_map3(y_synth, oof_preds_nb)\n    print(f\"\\n--- Final Naive Bayes CV MAP@3 (5x Original Data, Best Params): {map3_score_nb:.5f} ---\\n\")\n\n    # Save OOF and Test Predictions for Naive Bayes\n    output_dir = './' \n    os.makedirs(output_dir, exist_ok=True) \n\n    oof_filename_nb = os.path.join(output_dir, 'oof_preds_nb_onehot.npy')\n    test_filename_nb = os.path.join(output_dir, 'test_preds_nb_onehot.npy')\n\n    np.save(oof_filename_nb, oof_preds_nb)\n    np.save(test_filename_nb, test_preds_nb)\n    print(f\"Naive Bayes OOF predictions saved to: {oof_filename_nb}\")\n    print(f\"Naive Bayes Test predictions saved to: {test_filename_nb}\")\n\n    # --- 4. Generate Final Submission File ---\n    print(\"\\nGenerating final submission file (using Naive Bayes predictions)...\")\n    top3_preds_indices = np.argsort(test_preds_nb, axis=1)[:, ::-1][:, :3]\n    top3_preds_labels = le_fertilizer.inverse_transform(top3_preds_indices.flatten()).reshape(top3_preds_indices.shape)\n\n    submission_df['Fertilizer Name'] = [' '.join(row) for row in top3_preds_labels]\n    submission_df.to_csv('submission_naive_bayes_onehot.csv', index=False)\n\n    print(\"Submission file 'submission_naive_bayes_onehot.csv' created successfully.\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-22T10:46:40.674Z"},"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport xgboost as xgb\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nimport gc\nimport warnings\nimport os # Added for saving files\n\nwarnings.filterwarnings('ignore')\n\n# --- Reusable Functions ---\ndef calculate_map3(y_true, y_pred_proba):\n    \"\"\"Calculates the Mean Average Precision @ 3 score.\"\"\"\n    top3_preds_indices = np.argsort(y_pred_proba, axis=1)[:, ::-1][:, :3]\n    \n    scores = []\n    for i, true_label in enumerate(y_true):\n        top3 = top3_preds_indices[i]\n        score = 0.0\n        # Check if the true label is in the top 3 predictions\n        if true_label in top3:\n            # Find the rank (1, 2, or 3)\n            rank = np.where(top3 == true_label)[0][0] + 1\n            if rank == 1:\n                score = 1.0\n            elif rank == 2:\n                score = 0.5\n            elif rank == 3:\n                score = 1/3\n        scores.append(score)\n    return np.mean(scores)\n\n# --- Feature Engineering Function (Converts Numerical to Binned Categorical) ---\ndef feature_eng(df, target_col=None):\n    # Identify numerical columns to be binned\n    numerical_features = [col for col in df.select_dtypes(include=['int64', 'float64']).columns\n                            if col != 'id'] # Exclude 'id'\n    if target_col and target_col in numerical_features: # Exclude target if it's numerical\n        numerical_features.remove(target_col)\n\n    for col in numerical_features:\n        # Create a new column with '_Binned' suffix\n        # Convert numerical values to string, then to 'category' dtype\n        if f'{col}_Binned' not in df.columns: # Avoid recreating if already exists\n            df[f'{col}_Binned'] = df[col].astype(str).astype('category')\n    return df\n\n# --- Main Execution ---\nif __name__ == \"__main__\":\n    # Define target column name\n    TARGET_COL = 'Fertilizer Name'\n\n    # --- 1. Load Data ---\n    print(\"Loading datasets...\")\n    train_synthetic_df = pd.read_csv(\"/kaggle/input/playground-series-s5e6/train.csv\")\n    train_original_df = pd.read_csv(\"/kaggle/input/original/Fertilizer Prediction .csv\") # Corrected path\n    test_df_raw = pd.read_csv(\"/kaggle/input/playground-series-s5e6/test.csv\")\n    submission_df = pd.read_csv(\"/kaggle/input/playground-series-s5e6/sample_submission.csv\")\n\n    # Drop 'id' columns\n    train_synthetic_df = train_synthetic_df.drop(columns=['id'])\n    if 'id' in test_df_raw.columns:\n        test_df_raw = test_df_raw.drop(columns=['id'])\n\n    # --- Apply Feature Engineering to make all numerical features categorical ---\n    print(\"Applying feature engineering (binning numerical features into categories)...\")\n    train_synthetic_df = feature_eng(train_synthetic_df, target_col=TARGET_COL)\n    train_original_df = feature_eng(train_original_df, target_col=TARGET_COL)\n    test_df_raw = feature_eng(test_df_raw)\n\n    # --- 2. Preprocessing and Feature Preparation ---\n    print(\"Applying preprocessing...\")\n    # Target Encoding\n    le_fertilizer = LabelEncoder()\n    # Fit on combined labels to ensure all possible classes are covered\n    le_fertilizer.fit(pd.concat([train_synthetic_df[TARGET_COL], train_original_df[TARGET_COL]]))\n    y_synth = le_fertilizer.transform(train_synthetic_df[TARGET_COL])\n    y_orig = le_fertilizer.transform(train_original_df[TARGET_COL])\n    num_classes = len(le_fertilizer.classes_)\n\n    # Dynamically determine the common feature columns after feature engineering\n    base_categorical_features = ['Soil Type', 'Crop Type']\n    \n    # Collect all binned columns that are present across all three dataframes\n    all_processed_dfs = [train_synthetic_df, train_original_df, test_df_raw]\n    common_binned_cols = set()\n\n    # Get binned columns from the first dataframe\n    if all_processed_dfs:\n        first_df_binned = [col for col in all_processed_dfs[0].columns if col.endswith('_Binned')]\n        common_binned_cols.update(first_df_binned)\n\n        # Intersect with binned columns from subsequent dataframes\n        for df_item in all_processed_dfs[1:]:\n            current_df_binned = [col for col in df_item.columns if col.endswith('_Binned')]\n            common_binned_cols.intersection_update(current_df_binned)\n\n    # The final feature_cols will be the base categorical plus the common binned ones\n    feature_cols = base_categorical_features + sorted(list(common_binned_cols))\n    \n    print(f\"Selected feature columns (all categorical): {feature_cols}\")\n\n    # Feature Sets - now containing only categorical/binned features\n    X_synth = train_synthetic_df[feature_cols].copy()\n    X_orig = train_original_df[feature_cols].copy()\n    test_df = test_df_raw[feature_cols].copy()\n\n    # Categorical Feature Encoding (Using LabelEncoder for consistent integer mapping)\n    for col in feature_cols:\n        # Concatenate all categories from all datasets for consistent mapping\n        # Convert to string first to handle potential mixed types from .cat.codes or original data\n        full_vocab = pd.concat([X_synth[col].astype(str), X_orig[col].astype(str), test_df[col].astype(str)], axis=0).unique()\n        \n        label_enc = LabelEncoder()\n        label_enc.fit(full_vocab.astype(str)) # Fit on the full vocabulary of strings\n        \n        X_synth[col] = label_enc.transform(X_synth[col].astype(str))\n        X_orig[col] = label_enc.transform(X_orig[col].astype(str))\n        test_df[col] = label_enc.transform(test_df[col].astype(str))\n        \n        # Cast to 'category' dtype after encoding, as XGBoost's enable_categorical can leverage this\n        X_synth[col] = X_synth[col].astype(\"category\")\n        X_orig[col] = X_orig[col].astype(\"category\")\n        test_df[col] = test_df[col].astype(\"category\")\n\n    # --- 3. Define Final Hyperparameters (Corrected syntax and 'num_class') ---\n    params = {\n        'objective': 'multi:softprob',\n        'num_class': num_classes, # CORRECTED: Use 'num_classes' variable\n        'max_depth': 15,\n        'learning_rate': 0.010181857193698362,\n        'min_child_weight': 0.6343101283603367,\n        'colsample_bytree': 0.40213679386048695,\n        'subsample': 0.6936417386806593,\n        'gamma': 0.23727358231785983,\n        'reg_alpha': 9.642216011708818,\n        'reg_lambda': 1.1393721704644078,\n        'eval_metric': 'mlogloss',\n        'device': \"cuda\",\n        'seed': 42,\n        'tree_method': 'hist',\n        'enable_categorical': True,\n        # 'n_estimators' and 'early_stopping_rounds' are parameters for xgb.train, not part of params dict directly.\n        # They are correctly passed to xgb.train function call.\n    }\n\n    # --- 4. Train Model with 10-Fold CV & 5x Original Data Augmentation ---\n    print(\"\\nTraining XGBoost with 5x original data (5 Folds)...\")\n    NFOLDS = 5\n    skf = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=42)\n\n    oof_preds = np.zeros((len(X_synth), num_classes))\n    test_preds = np.zeros((len(test_df), num_classes))\n\n    for fold, (train_idx, val_idx) in enumerate(skf.split(X_synth, y_synth)):\n        print(f\"--- Fold {fold+1}/{NFOLDS} ---\")\n        X_synth_train, y_synth_train = X_synth.iloc[train_idx], y_synth[train_idx]\n        X_synth_val, y_synth_val = X_synth.iloc[val_idx], y_synth[val_idx]\n\n        # Octuple the original data to give it more weight (5x)\n        X_orig_oct = pd.concat([X_orig] * 5, ignore_index=True)\n        y_orig_oct = np.concatenate([y_orig] * 5)\n\n        # Augment the training data with the OCTUPLED original dataset\n        X_train_aug = pd.concat([X_synth_train, X_orig_oct], ignore_index=True)\n        y_train_aug = np.concatenate([y_synth_train, y_orig_oct])\n\n        # Use X_synth.columns.tolist() for feature_names to ensure consistency with trained features\n        dtrain = xgb.DMatrix(X_train_aug, label=y_train_aug, feature_names=X_synth.columns.tolist(), enable_categorical=True)\n        dval = xgb.DMatrix(X_synth_val, label=y_synth_val, feature_names=X_synth.columns.tolist(), enable_categorical=True)\n        dtest = xgb.DMatrix(test_df, feature_names=X_synth.columns.tolist(), enable_categorical=True) # Ensure test_df also uses these names\n\n        model = xgb.train(params, dtrain, num_boost_round=5000, evals=[(dval, 'eval')], early_stopping_rounds=50, verbose_eval=None)\n\n        oof_preds[val_idx] = model.predict(dval, iteration_range=(0, model.best_iteration + 1))\n        test_preds += model.predict(dtest, iteration_range=(0, model.best_iteration + 1)) / NFOLDS\n        gc.collect()\n\n    # --- 5. Evaluate and Save ---\n    map3_score = calculate_map3(y_synth, oof_preds)\n    print(f\"\\n--- Final Model CV MAP@3 (5x Original Data): {map3_score:.5f} ---\\n\")\n\n    # Save OOF and Test Predictions\n    output_dir = './' # Save in current directory\n    os.makedirs(output_dir, exist_ok=True) # Ensure directory exists\n\n    oof_filename = os.path.join(output_dir, 'oof_preds.npy')\n    test_filename = os.path.join(output_dir, 'test_preds.npy')\n\n    np.save(oof_filename, oof_preds)\n    np.save(test_filename, test_preds)\n    print(f\"OOF predictions saved to: {oof_filename}\")\n    print(f\"Test predictions saved to: {test_filename}\")\n\n    print(\"Generating final submission file...\")\n    top3_preds_indices = np.argsort(test_preds, axis=1)[:, ::-1][:, :3]\n    top3_preds_labels = le_fertilizer.inverse_transform(top3_preds_indices.flatten()).reshape(top3_preds_indices.shape)\n\n    submission_df['Fertilizer Name'] = [' '.join(row) for row in top3_preds_labels]\n    submission_df.to_csv('submission.csv', index=False)\n\n    print(\"Submission file 'submission.csv' created successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T07:45:37.306027Z","iopub.execute_input":"2025-06-21T07:45:37.306392Z","iopub.status.idle":"2025-06-21T09:06:08.401209Z","shell.execute_reply.started":"2025-06-21T07:45:37.306373Z","shell.execute_reply":"2025-06-21T09:06:08.400470Z"},"editable":false},"outputs":[{"name":"stdout","text":"Loading datasets...\nApplying feature engineering (binning numerical features into categories)...\nApplying preprocessing...\nSelected feature columns (all categorical): ['Soil Type', 'Crop Type', 'Humidity_Binned', 'Moisture_Binned', 'Nitrogen_Binned', 'Phosphorous_Binned', 'Potassium_Binned', 'Temparature_Binned']\n\nTraining XGBoost with 5x original data (5 Folds)...\n--- Fold 1/5 ---\n--- Fold 2/5 ---\n--- Fold 3/5 ---\n--- Fold 4/5 ---\n--- Fold 5/5 ---\n\n--- Final Model CV MAP@3 (5x Original Data): 0.37911 ---\n\nOOF predictions saved to: ./oof_preds.npy\nTest predictions saved to: ./test_preds.npy\nGenerating final submission file...\nSubmission file 'submission.csv' created successfully.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport xgboost as xgb\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nimport gc\nimport warnings\nimport os\nimport optuna # Added for hyperparameter optimization\n\n# Suppress warnings for cleaner output\nwarnings.filterwarnings('ignore')\n\n# --- Reusable Functions ---\ndef calculate_map3(y_true, y_pred_proba):\n    \"\"\"Calculates the Mean Average Precision @ 3 score.\"\"\"\n    # Get the indices of the top 3 predictions for each sample\n    top3_preds_indices = np.argsort(y_pred_proba, axis=1)[:, ::-1][:, :3]\n    \n    scores = []\n    for i, true_label in enumerate(y_true):\n        top3 = top3_preds_indices[i]\n        score = 0.0\n        # Check if the true label is in the top 3 predictions\n        if true_label in top3:\n            # Find the rank (1, 2, or 3)\n            rank = np.where(top3 == true_label)[0][0] + 1\n            if rank == 1:\n                score = 1.0\n            elif rank == 2:\n                score = 0.5\n            elif rank == 3:\n                score = 1/3\n        scores.append(score)\n    return np.mean(scores)\n\n# --- Feature Engineering Function (Converts Numerical to Binned Categorical) ---\ndef feature_eng(df, target_col=None):\n    # Identify numerical columns to be binned\n    numerical_features = [col for col in df.select_dtypes(include=['int64', 'float64']).columns\n                            if col != 'id'] # Exclude 'id'\n    if target_col and target_col in numerical_features: # Exclude target if it's numerical\n        numerical_features.remove(target_col)\n\n    for col in numerical_features:\n        # Create a new column with '_Binned' suffix\n        # Convert numerical values to string, then to 'category' dtype\n        if f'{col}_Binned' not in df.columns: # Avoid recreating if already exists\n            df[f'{col}_Binned'] = df[col].astype(str).astype('category')\n    return df\n\n# --- Optuna Objective Function for XGBoost Hyperparameter Tuning ---\ndef objective(trial, X_synth, y_synth, X_orig, y_orig, test_df_feature_names, num_classes, calculate_map3_func):\n    \"\"\"\n    Objective function for Optuna to optimize XGBoost hyperparameters.\n    \"\"\"\n    # Define hyperparameter search space\n    params = {\n        'objective': 'multi:softprob',\n        'num_class': num_classes,\n        'max_depth': trial.suggest_int('max_depth', 3, 15), # Reduced max_depth range based on previous discussion\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n        'min_child_weight': trial.suggest_float('min_child_weight', 0.1, 10.0, log=True), # Changed to float for more granular tuning\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.4, 1.0),\n        'subsample': trial.suggest_float('subsample', 0.4, 1.0),\n        'gamma': trial.suggest_float('gamma', 0.0, 0.5),\n        'reg_alpha': trial.suggest_float('reg_alpha', 0.01, 10.0, log=True),\n        'reg_lambda': trial.suggest_float('reg_lambda', 0.01, 10.0, log=True),\n        'eval_metric': 'mlogloss',\n        'device': \"cuda\", # Keep as fixed, assuming GPU environment\n        'seed': 42, # Keep seed fixed for reproducibility of trials\n        'tree_method': 'hist',\n        'enable_categorical': True,\n        # No 'n_estimators' or 'early_stopping_rounds' directly in params for xgb.train,\n        # these are passed to xgb.train call.\n    }\n\n    NFOLDS_OPTUNA = 5 # Fixed to 5 folds for Optuna\n    skf_optuna = StratifiedKFold(n_splits=NFOLDS_OPTUNA, shuffle=True, random_state=42)\n\n    oof_preds_trial = np.zeros((len(X_synth), num_classes))\n    \n    # Octuple the original data once for the objective function.\n    X_orig_oct_obj = pd.concat([X_orig] * 5, ignore_index=True)\n    y_orig_oct_obj = np.concatenate([y_orig] * 5)\n\n    for fold_num, (train_idx, val_idx) in enumerate(skf_optuna.split(X_synth, y_synth)):\n        X_synth_train, y_synth_train = X_synth.iloc[train_idx], y_synth[train_idx]\n        X_synth_val, y_synth_val = X_synth.iloc[val_idx], y_synth[val_idx]\n\n        # Augment the training data with the OCTUPLED original dataset\n        X_train_aug = pd.concat([X_synth_train, X_orig_oct_obj], ignore_index=True)\n        y_train_aug = np.concatenate([y_synth_train, y_orig_oct_obj])\n\n        dtrain = xgb.DMatrix(X_train_aug, label=y_train_aug, feature_names=test_df_feature_names, enable_categorical=True)\n        dval = xgb.DMatrix(X_synth_val, label=y_synth_val, feature_names=test_df_feature_names, enable_categorical=True)\n\n        model = xgb.train(params, dtrain, num_boost_round=5000, evals=[(dval, 'eval')], early_stopping_rounds=50, verbose_eval=None)\n\n        oof_preds_trial[val_idx] = model.predict(dval, iteration_range=(0, model.best_iteration + 1))\n        \n        del model, dtrain, dval\n        gc.collect()\n\n    map3_score_trial = calculate_map3_func(y_synth, oof_preds_trial)\n    return map3_score_trial\n\n\n# --- Main Execution ---\nif __name__ == \"__main__\":\n    # Define target column name\n    TARGET_COL = 'Fertilizer Name'\n\n    # --- 1. Load Data ---\n    print(\"Loading datasets...\")\n    train_synthetic_df = pd.read_csv(\"/kaggle/input/playground-series-s5e6/train.csv\")\n    train_original_df = pd.read_csv(\"/kaggle/input/original/Fertilizer Prediction .csv\")\n    test_df_raw = pd.read_csv(\"/kaggle/input/playground-series-s5e6/test.csv\")\n    submission_df = pd.read_csv(\"/kaggle/input/playground-series-s5e6/sample_submission.csv\")\n\n    # Drop 'id' columns\n    train_synthetic_df = train_synthetic_df.drop(columns=['id'])\n    if 'id' in test_df_raw.columns:\n        test_df_raw = test_df_raw.drop(columns=['id'])\n\n    # --- Apply Feature Engineering to make all numerical features categorical ---\n    print(\"Applying feature engineering (binning numerical features into categories)...\")\n    train_synthetic_df = feature_eng(train_synthetic_df, target_col=TARGET_COL)\n    train_original_df = feature_eng(train_original_df, target_col=TARGET_COL)\n    test_df_raw = feature_eng(test_df_raw)\n\n\n    # --- 2. Preprocessing and Feature Preparation ---\n    print(\"Applying preprocessing...\")\n    # Target Encoding\n    le_fertilizer = LabelEncoder()\n    # Fit on combined labels to ensure all possible classes are covered\n    le_fertilizer.fit(pd.concat([train_synthetic_df[TARGET_COL], train_original_df[TARGET_COL]]))\n    y_synth = le_fertilizer.transform(train_synthetic_df[TARGET_COL])\n    y_orig = le_fertilizer.transform(train_original_df[TARGET_COL])\n    num_classes = len(le_fertilizer.classes_)\n\n    # Dynamically determine the common feature columns after feature engineering\n    base_categorical_features = ['Soil Type', 'Crop Type']\n    \n    # Collect all binned columns that are present across all three dataframes\n    all_processed_dfs = [train_synthetic_df, train_original_df, test_df_raw]\n    common_binned_cols = set()\n\n    # Get binned columns from the first dataframe\n    if all_processed_dfs:\n        first_df_binned = [col for col in all_processed_dfs[0].columns if col.endswith('_Binned')]\n        common_binned_cols.update(first_df_binned)\n\n        # Intersect with binned columns from subsequent dataframes\n        for df_item in all_processed_dfs[1:]:\n            current_df_binned = [col for col in df_item.columns if col.endswith('_Binned')]\n            common_binned_cols.intersection_update(current_df_binned)\n\n    # The final feature_cols will be the base categorical plus the common binned ones\n    feature_cols = base_categorical_features + sorted(list(common_binned_cols))\n    \n    print(f\"Selected feature columns (all categorical): {feature_cols}\")\n\n    # Feature Sets - now containing only categorical/binned features\n    X_synth = train_synthetic_df[feature_cols].copy()\n    X_orig = train_original_df[feature_cols].copy()\n    test_df = test_df_raw[feature_cols].copy() # Ensure test_df also has only selected features\n\n    # Categorical Feature Encoding (Using LabelEncoder for consistent integer mapping)\n    for col in feature_cols:\n        # Concatenate all categories from all datasets for consistent mapping\n        full_vocab = pd.concat([X_synth[col].astype(str), X_orig[col].astype(str), test_df[col].astype(str)], axis=0).unique()\n        \n        label_enc = LabelEncoder()\n        label_enc.fit(full_vocab.astype(str)) # Fit on the full vocabulary of strings\n        \n        X_synth[col] = label_enc.transform(X_synth[col].astype(str))\n        X_orig[col] = label_enc.transform(X_orig[col].astype(str))\n        test_df[col] = label_enc.transform(test_df[col].astype(str))\n        \n        # Cast to 'category' dtype after encoding, as XGBoost's enable_categorical can leverage this\n        X_synth[col] = X_synth[col].astype(\"category\")\n        X_orig[col] = X_orig[col].astype(\"category\")\n        test_df[col] = test_df[col].astype(\"category\")\n\n\n    # --- 3. Optuna Hyperparameter Optimization ---\n    print(\"\\nStarting Optuna hyperparameter optimization (5-Fold CV)...\")\n    # Using TPESampler for more efficient exploration\n    study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))\n    study.optimize(\n        lambda trial: objective(trial, X_synth, y_synth, X_orig, y_orig, X_synth.columns.tolist(), num_classes, calculate_map3),\n        n_trials=50, # Number of trials for Optuna to run. Adjust as needed.\n        show_progress_bar=True\n    )\n\n    print(\"\\nOptuna optimization finished.\")\n    print(f\"Best trial: {study.best_trial.value:.5f} (MAP@3)\")\n    print(\"Best hyperparameters found:\")\n    for key, value in study.best_params.items():\n        print(f\"  {key}: {value}\")\n\n    # --- 4. Define Final Hyperparameters using Optuna's Best ---\n    # Start with fixed parameters, then override with best params from Optuna\n    final_params = {\n        'objective': 'multi:softprob',\n        'num_class': num_classes,\n        'eval_metric': 'mlogloss',\n        'device': \"cuda\",\n        'seed': 42,\n        'tree_method': 'hist',\n        'enable_categorical': True,\n        # These will be set by Optuna's best params\n        'max_depth': study.best_params['max_depth'],\n        'learning_rate': study.best_params['learning_rate'],\n        'min_child_weight': study.best_params['min_child_weight'],\n        'colsample_bytree': study.best_params['colsample_bytree'],\n        'subsample': study.best_params['subsample'],\n        'gamma': study.best_params['gamma'],\n        'reg_alpha': study.best_params['reg_alpha'],\n        'reg_lambda': study.best_params['reg_lambda'],\n        # 'n_estimators' and 'early_stopping_rounds' are for xgb.train call, not params dict\n    }\n\n    # --- 5. Train Final Model with 5-Fold CV & 5x Original Data Augmentation (using Best Params) ---\n    print(\"\\nTraining final XGBoost model with best hyperparameters (5-Fold CV & 5x original data augmentation)...\")\n    NFOLDS = 5 # Fixed to 5 folds for final training and prediction\n    skf = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=42)\n\n    oof_preds = np.zeros((len(X_synth), num_classes))\n    test_preds = np.zeros((len(test_df), num_classes))\n\n    for fold, (train_idx, val_idx) in enumerate(skf.split(X_synth, y_synth)):\n        print(f\"--- Fold {fold+1}/{NFOLDS} ---\")\n        X_synth_train, y_synth_train = X_synth.iloc[train_idx], y_synth[train_idx]\n        X_synth_val, y_synth_val = X_synth.iloc[val_idx], y_synth[val_idx]\n\n        # Octuple the original data to give it more weight (5x as per user's earlier snippet)\n        X_orig_oct = pd.concat([X_orig] * 5, ignore_index=True)\n        y_orig_oct = np.concatenate([y_orig] * 5)\n\n        # Augment the training data with the OCTUPLED original dataset\n        X_train_aug = pd.concat([X_synth_train, X_orig_oct], ignore_index=True)\n        y_train_aug = np.concatenate([y_synth_train, y_orig_oct])\n\n        dtrain = xgb.DMatrix(X_train_aug, label=y_train_aug, feature_names=X_synth.columns.tolist(), enable_categorical=True)\n        dval = xgb.DMatrix(X_synth_val, label=y_synth_val, feature_names=X_synth.columns.tolist(), enable_categorical=True)\n        dtest = xgb.DMatrix(test_df, feature_names=X_synth.columns.tolist(), enable_categorical=True)\n\n        model = xgb.train(final_params, dtrain, num_boost_round=5000, evals=[(dval, 'eval')], early_stopping_rounds=50, verbose_eval=None)\n\n        oof_preds[val_idx] = model.predict(dval, iteration_range=(0, model.best_iteration + 1))\n        test_preds += model.predict(dtest, iteration_range=(0, model.best_iteration + 1)) / NFOLDS\n        gc.collect()\n\n    # --- 6. Evaluate and Save ---\n    map3_score = calculate_map3(y_synth, oof_preds)\n    print(f\"\\n--- Final Model CV MAP@3 (5x Original Data, Best Params): {map3_score:.5f} ---\\n\")\n\n    # Save OOF and Test Predictions\n    output_dir = './' # Save in current directory\n    os.makedirs(output_dir, exist_ok=True) # Ensure directory exists\n\n    oof_filename = os.path.join(output_dir, 'oof_preds.npy')\n    test_filename = os.path.join(output_dir, 'test_preds.npy')\n\n    np.save(oof_filename, oof_preds)\n    np.save(test_filename, test_preds)\n    print(f\"OOF predictions saved to: {oof_filename}\")\n    print(f\"Test predictions saved to: {test_filename}\")\n\n    print(\"Generating final submission file...\")\n    top3_preds_indices = np.argsort(test_preds, axis=1)[:, ::-1][:, :3]\n    top3_preds_labels = le_fertilizer.inverse_transform(top3_preds_indices.flatten()).reshape(top3_preds_indices.shape)\n\n    submission_df['Fertilizer Name'] = [' '.join(row) for row in top3_preds_labels]\n    submission_df.to_csv('submission.csv', index=False)\n\n    print(\"Submission file 'submission.csv' created successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T10:29:22.726783Z","iopub.execute_input":"2025-06-20T10:29:22.727142Z","execution_failed":"2025-06-20T21:57:11.097Z"},"editable":false},"outputs":[{"name":"stdout","text":"Loading datasets...\nApplying feature engineering (binning numerical features into categories)...\nApplying preprocessing...\nSelected feature columns (all categorical): ['Soil Type', 'Crop Type', 'Humidity_Binned', 'Moisture_Binned', 'Nitrogen_Binned', 'Phosphorous_Binned', 'Potassium_Binned', 'Temparature_Binned']\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-06-20 10:29:32,951] A new study created in memory with name: no-name-97ca8832-00cd-4a30-ab25-67296a4ba27e\n","output_type":"stream"},{"name":"stdout","text":"\nStarting Optuna hyperparameter optimization (5-Fold CV)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/50 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1cf665bc88548e7b3ee2428f961713a"}},"metadata":{}},{"name":"stdout","text":"[I 2025-06-20 10:33:44,049] Trial 0 finished with value: 0.361878 and parameters: {'max_depth': 7, 'learning_rate': 0.08927180304353628, 'min_child_weight': 2.9106359131330697, 'colsample_bytree': 0.759195090518222, 'subsample': 0.4936111842654619, 'gamma': 0.07799726016810132, 'reg_alpha': 0.014936568554617643, 'reg_lambda': 3.9676050770529883}. Best is trial 0 with value: 0.361878.\n[I 2025-06-20 10:42:47,776] Trial 1 finished with value: 0.3604271111111111 and parameters: {'max_depth': 10, 'learning_rate': 0.051059032093947576, 'min_child_weight': 0.10994335574766201, 'colsample_bytree': 0.9819459112971965, 'subsample': 0.899465584480253, 'gamma': 0.10616955533913808, 'reg_alpha': 0.035113563139704075, 'reg_lambda': 0.03549878832196503}. Best is trial 0 with value: 0.361878.\n[I 2025-06-20 10:59:39,368] Trial 2 finished with value: 0.37477555555555553 and parameters: {'max_depth': 6, 'learning_rate': 0.03347776308515933, 'min_child_weight': 0.7309539835912913, 'colsample_bytree': 0.5747374841188252, 'subsample': 0.7671117368334277, 'gamma': 0.06974693032602092, 'reg_alpha': 0.07523742884534858, 'reg_lambda': 0.1256277350380703}. Best is trial 2 with value: 0.37477555555555553.\n[I 2025-06-20 11:06:16,456] Trial 3 finished with value: 0.3696902222222222 and parameters: {'max_depth': 8, 'learning_rate': 0.06097839109531514, 'min_child_weight': 0.2508115686045233, 'colsample_bytree': 0.708540663048167, 'subsample': 0.7554487413172255, 'gamma': 0.023225206359998862, 'reg_alpha': 0.6647135865318028, 'reg_lambda': 0.03247673570627449}. Best is trial 2 with value: 0.37477555555555553.\n[I 2025-06-20 11:18:00,046] Trial 4 finished with value: 0.3563906666666667 and parameters: {'max_depth': 3, 'learning_rate': 0.08889667907018929, 'min_child_weight': 8.536189862866832, 'colsample_bytree': 0.8850384088698766, 'subsample': 0.5827682615040224, 'gamma': 0.048836057003191935, 'reg_alpha': 1.1290133559092674, 'reg_lambda': 0.2091498132903561}. Best is trial 2 with value: 0.37477555555555553.\n[I 2025-06-20 11:40:47,740] Trial 5 finished with value: 0.3623255555555555 and parameters: {'max_depth': 4, 'learning_rate': 0.03127353036780371, 'min_child_weight': 0.1171593739230706, 'colsample_bytree': 0.9455922412472693, 'subsample': 0.5552679889600102, 'gamma': 0.331261142176991, 'reg_alpha': 0.08612579192594885, 'reg_lambda': 0.3632486956676606}. Best is trial 2 with value: 0.37477555555555553.\n[I 2025-06-20 12:15:27,206] Trial 6 finished with value: 0.37085288888888884 and parameters: {'max_depth': 10, 'learning_rate': 0.015305744365500184, 'min_child_weight': 8.692991511139551, 'colsample_bytree': 0.8650796940166687, 'subsample': 0.9636993649385135, 'gamma': 0.4474136752138244, 'reg_alpha': 0.6218704727769077, 'reg_lambda': 5.829384542994738}. Best is trial 2 with value: 0.37477555555555553.\n[I 2025-06-20 12:43:07,730] Trial 7 finished with value: 0.36921622222222217 and parameters: {'max_depth': 4, 'learning_rate': 0.015703008378806716, 'min_child_weight': 0.12315571723666023, 'colsample_bytree': 0.5951981984579586, 'subsample': 0.6332063738136893, 'gamma': 0.13567451588694796, 'reg_alpha': 3.063462210622083, 'reg_lambda': 0.11756010900231852}. Best is trial 2 with value: 0.37477555555555553.\n[I 2025-06-20 12:54:16,919] Trial 8 finished with value: 0.3648482222222222 and parameters: {'max_depth': 6, 'learning_rate': 0.03488960745139221, 'min_child_weight': 0.19135880487692303, 'colsample_bytree': 0.8813181884524238, 'subsample': 0.44473038620786254, 'gamma': 0.49344346830025865, 'reg_alpha': 2.0736445177905036, 'reg_lambda': 0.03945908811100001}. Best is trial 2 with value: 0.37477555555555553.\n[I 2025-06-20 13:13:44,253] Trial 9 finished with value: 0.36347688888888885 and parameters: {'max_depth': 3, 'learning_rate': 0.06538248584518043, 'min_child_weight': 2.5924756604751593, 'colsample_bytree': 0.8374043008245924, 'subsample': 0.8627622080115674, 'gamma': 0.03702232586704518, 'reg_alpha': 0.11895896737553548, 'reg_lambda': 0.022264204303769682}. Best is trial 2 with value: 0.37477555555555553.\n[I 2025-06-20 14:41:42,485] Trial 10 finished with value: 0.3788262222222222 and parameters: {'max_depth': 14, 'learning_rate': 0.010139048090380872, 'min_child_weight': 0.546333140348519, 'colsample_bytree': 0.4107771253688918, 'subsample': 0.7415875015913806, 'gamma': 0.22153989840035113, 'reg_alpha': 7.573772188483907, 'reg_lambda': 1.0013244603704614}. Best is trial 10 with value: 0.3788262222222222.\n[I 2025-06-20 16:12:53,461] Trial 11 finished with value: 0.37857466666666667 and parameters: {'max_depth': 15, 'learning_rate': 0.010157503931314201, 'min_child_weight': 0.5514598587093157, 'colsample_bytree': 0.40622987794450144, 'subsample': 0.751681502013535, 'gamma': 0.21717813587820556, 'reg_alpha': 7.481981342010064, 'reg_lambda': 0.9445537988486672}. Best is trial 10 with value: 0.3788262222222222.\n[I 2025-06-20 17:39:17,094] Trial 12 finished with value: 0.37910555555555553 and parameters: {'max_depth': 15, 'learning_rate': 0.010181857193698362, 'min_child_weight': 0.6343101283603367, 'colsample_bytree': 0.40213679386048695, 'subsample': 0.6936417386806593, 'gamma': 0.23727358231785983, 'reg_alpha': 9.642216011708818, 'reg_lambda': 1.1393721704644078}. Best is trial 12 with value: 0.37910555555555553.\n[I 2025-06-20 19:00:28,579] Trial 13 finished with value: 0.3790753333333333 and parameters: {'max_depth': 15, 'learning_rate': 0.010804032574636408, 'min_child_weight': 0.3748978215907942, 'colsample_bytree': 0.4058281352041519, 'subsample': 0.6710395870973215, 'gamma': 0.2574852070104715, 'reg_alpha': 9.956055660659018, 'reg_lambda': 1.4026247400617888}. Best is trial 12 with value: 0.37910555555555553.\n[I 2025-06-20 20:04:03,006] Trial 14 finished with value: 0.377952 and parameters: {'max_depth': 13, 'learning_rate': 0.015748278022995423, 'min_child_weight': 1.6369015404200449, 'colsample_bytree': 0.5042117974591117, 'subsample': 0.6587138211788818, 'gamma': 0.32854430348937447, 'reg_alpha': 9.672991188802765, 'reg_lambda': 1.654838346837673}. Best is trial 12 with value: 0.37910555555555553.\n[I 2025-06-20 20:42:42,803] Trial 15 finished with value: 0.3768935555555555 and parameters: {'max_depth': 12, 'learning_rate': 0.01960946292953642, 'min_child_weight': 0.4073995228620565, 'colsample_bytree': 0.49933376883122665, 'subsample': 0.8344376716296752, 'gamma': 0.3032614058832745, 'reg_alpha': 3.2999552160276795, 'reg_lambda': 2.420672811662282}. Best is trial 12 with value: 0.37910555555555553.\n[I 2025-06-20 21:24:43,485] Trial 16 finished with value: 0.3743553333333333 and parameters: {'max_depth': 12, 'learning_rate': 0.02246578809397596, 'min_child_weight': 1.2531382608527577, 'colsample_bytree': 0.5004751595717465, 'subsample': 0.623770341902924, 'gamma': 0.16732251312135799, 'reg_alpha': 4.147379303554003, 'reg_lambda': 0.5041759642485978}. Best is trial 12 with value: 0.37910555555555553.\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null}]}
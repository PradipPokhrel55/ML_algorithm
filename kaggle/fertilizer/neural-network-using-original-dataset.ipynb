{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91717,"databundleVersionId":12184666,"sourceType":"competition"},{"sourceId":11592231,"sourceType":"datasetVersion","datasetId":7269189},{"sourceId":12251741,"sourceType":"datasetVersion","datasetId":7719660},{"sourceId":12272748,"sourceType":"datasetVersion","datasetId":7734045}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-24T19:34:37.339517Z","iopub.execute_input":"2025-06-24T19:34:37.339882Z","iopub.status.idle":"2025-06-24T19:34:37.348316Z","shell.execute_reply.started":"2025-06-24T19:34:37.339854Z","shell.execute_reply":"2025-06-24T19:34:37.347536Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/datasets2/oof_preds_lgb.npy\n/kaggle/input/datasets2/test_preds_nn_pytorch.npy\n/kaggle/input/datasets2/test_preds_lgb.npy\n/kaggle/input/datasets2/oof_preds_nn_pytorch.npy\n/kaggle/input/fertilizer-prediction/Fertilizer Prediction.csv\n/kaggle/input/firstlayer/oof_preds_hgb_fixed_params.npy\n/kaggle/input/firstlayer/oof_preds_nn_keras.npy\n/kaggle/input/firstlayer/oof_preds_nb_onehot.npy\n/kaggle/input/firstlayer/oof_preds.npy\n/kaggle/input/firstlayer/test_preds.npy\n/kaggle/input/firstlayer/test_preds_nb_onehot.npy\n/kaggle/input/firstlayer/test_preds_nn_keras.npy\n/kaggle/input/firstlayer/test_preds_hgb_fixed_params.npy\n/kaggle/input/playground-series-s5e6/sample_submission.csv\n/kaggle/input/playground-series-s5e6/train.csv\n/kaggle/input/playground-series-s5e6/test.csv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os, gc\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport pytorch_lightning as pl\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\n\npl.seed_everything(42)\n\ndef calculate_map3(y_true, y_pred):\n    top3_preds_indices = np.argsort(y_pred, axis=1)[:, ::-1][:, :3]\n    scores = []\n    for i, true_label in enumerate(y_true):\n        top3 = top3_preds_indices[i]\n        score = 0.0\n        if true_label in top3:\n            rank = np.where(top3 == true_label)[0][0] + 1\n            score = 1.0 / rank\n        scores.append(score)\n    return np.mean(scores)\n\nclass SpatialDropout1D(nn.Dropout2d):\n    def forward(self, x):\n        # x shape: (batch, channels, timesteps)\n        x = x.unsqueeze(3)            # (batch, channels, timesteps, 1)\n        x = super().forward(x)        # apply 2D dropout on (channels, timesteps)\n        return x.squeeze(3)           # back to (batch, channels, timesteps)\n\n\n# -----------------------------\n# Dataset for Embedding\n# -----------------------------\nclass TabularEmbeddingDataset(Dataset):\n    def __init__(self, X, y=None):\n        self.X = torch.tensor(X.values, dtype=torch.long)\n        self.y = torch.tensor(y, dtype=torch.long) if y is not None else None\n\n    def __len__(self): return len(self.X)\n    def __getitem__(self, idx):\n        return (self.X[idx], self.y[idx]) if self.y is not None else self.X[idx]\n\n# -----------------------------\n# CNN Model with Embeddings\n# -----------------------------\nclass SoftOrdering1DCNN(pl.LightningModule):\n    def __init__(self, category_sizes, output_dim, embedding_size=16, lr=2e-4):\n        super().__init__()\n        self.save_hyperparameters()\n        self.lr = lr\n        self.loss_fn = nn.CrossEntropyLoss()\n        self.val_outputs = []\n\n        self.embeddings = nn.ModuleList([\n            nn.Embedding(cat_size, embedding_size) for cat_size in category_sizes\n        ])\n\n        input_channels = len(category_sizes)\n        self.cnn = nn.Sequential(\n            nn.Conv1d(input_channels, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n            nn.ReLU(),\n            SpatialDropout1D(0.2),\n            nn.AdaptiveAvgPool1d(2),\n            nn.Flatten(),\n            nn.Linear(128 * 2, 256),\n            nn.ReLU(),\n            nn.BatchNorm1d(256),\n            nn.Dropout(0.3),\n            nn.Linear(256, output_dim)\n        )\n\n    def forward(self, x):\n        embeds = [emb(x[:, i]) for i, emb in enumerate(self.embeddings)]\n        x = torch.stack(embeds, dim=1)  # shape: [batch_size, channels, embed_dim]\n        return self.cnn(x)\n\n    def training_step(self, batch, _):\n        x, y = batch\n        return self.loss_fn(self(x), y)\n\n    def validation_step(self, batch, _):\n        x, y = batch\n        logits = self(x)\n        probs = torch.softmax(logits, dim=1).detach().cpu().numpy()\n        y_true = y.cpu().numpy()\n        self.val_outputs.append((y_true, probs))\n\n    def on_validation_epoch_end(self):\n        y_true = np.concatenate([out[0] for out in self.val_outputs])\n        y_pred = np.concatenate([out[1] for out in self.val_outputs])\n        map3 = calculate_map3(y_true, y_pred)\n        self.log(\"val_map3\", map3, prog_bar=True)\n        self.val_outputs.clear()\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=self.lr)\n\n# -----------------------------\n# Data Loading & Encoding\n# -----------------------------\ndef load_data():\n    train_synth = pd.read_csv(\"/kaggle/input/playground-series-s5e6/train.csv\").drop(columns='id')\n    train_orig = pd.read_csv(\"/kaggle/input/fertilizer-prediction/Fertilizer Prediction.csv\")\n    test_df = pd.read_csv(\"/kaggle/input/playground-series-s5e6/test.csv\").drop(columns='id')\n    sub_df = pd.read_csv(\"/kaggle/input/playground-series-s5e6/sample_submission.csv\")\n    return train_synth, train_orig, test_df, sub_df\n\ndef prepare_categorical(df):\n    for col in df.columns:\n        df[col] = df[col].astype(\"category\")\n    return df\n\ndef encode_features(train_synth, train_orig, test_df):\n    target_col = 'Fertilizer Name'\n    label_enc = LabelEncoder()\n    label_enc.fit(pd.concat([train_synth[target_col], train_orig[target_col]]))\n    y_synth = label_enc.transform(train_synth[target_col])\n    y_orig = label_enc.transform(train_orig[target_col])\n\n    train_synth = train_synth.drop(columns=[target_col])\n    train_orig = train_orig.drop(columns=[target_col])\n\n    all_data = pd.concat([train_synth, train_orig, test_df], axis=0)\n    all_data = prepare_categorical(all_data)\n\n    for col in all_data.columns:\n        all_data[col] = all_data[col].cat.codes\n\n    n_synth = len(train_synth)\n    n_orig = len(train_orig)\n\n    X_synth = all_data.iloc[:n_synth]\n    X_orig = all_data.iloc[n_synth:n_synth + n_orig]\n    X_test = all_data.iloc[n_synth + n_orig:]\n\n    category_sizes = [all_data[col].nunique() for col in all_data.columns]\n\n    return X_synth, y_synth, X_orig, y_orig, X_test, label_enc, category_sizes\n\n# -----------------------------\n# Training Loop\n# -----------------------------\ndef train_model():\n    train_synth, train_orig, test_df_raw, sub_df = load_data()\n    X_synth, y_synth, X_orig, y_orig, X_test, le, cat_sizes = encode_features(train_synth, train_orig, test_df_raw)\n\n    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    test_preds = np.zeros((len(X_test), len(le.classes_)))\n    oof_preds = np.zeros((len(X_synth), len(le.classes_)))\n\n    for fold, (train_idx, val_idx) in enumerate(skf.split(X_synth, y_synth)):\n        print(f\"--- Fold {fold + 1}/5 ---\")\n        X_train, y_train = X_synth.iloc[train_idx], y_synth[train_idx]\n        X_val, y_val = X_synth.iloc[val_idx], y_synth[val_idx]\n\n        X_train_aug = pd.concat([X_train] + [X_orig] * 5, ignore_index=True)\n        y_train_aug = np.concatenate([y_train] + [y_orig] * 5)\n\n        train_ds = TabularEmbeddingDataset(X_train_aug, y_train_aug)\n        val_ds = TabularEmbeddingDataset(X_val, y_val)\n        test_ds = TabularEmbeddingDataset(X_test)\n\n        train_loader = DataLoader(train_ds, batch_size=2048, shuffle=True)\n        val_loader = DataLoader(val_ds, batch_size=2048)\n        test_loader = DataLoader(test_ds, batch_size=2048)\n\n        model = SoftOrdering1DCNN(category_sizes=cat_sizes, output_dim=len(le.classes_), lr=e-3)\n\n        trainer = pl.Trainer(\n            max_epochs=30,\n            accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n            logger=False,\n            enable_checkpointing=False,\n            deterministic=True,\n            log_every_n_steps=5\n        )\n\n        trainer.fit(model, train_loader, val_loader)\n\n        model.eval()\n        preds = []\n        for batch in val_loader:\n            x, _ = batch\n            with torch.no_grad():\n                preds.append(torch.softmax(model(x.to(model.device)), dim=1).cpu().numpy())\n        oof_preds[val_idx] = np.concatenate(preds)\n\n        preds = []\n        for batch in test_loader:\n            x = batch\n            if isinstance(x, tuple): x = x[0]\n            with torch.no_grad():\n                preds.append(torch.softmax(model(x.to(model.device)), dim=1).cpu().numpy())\n        test_preds += np.concatenate(preds) / 5\n        gc.collect()\n\n    print(f\"Final MAP@3: {calculate_map3(y_synth, oof_preds):.5f}\")\n\n    top3_preds = np.argsort(test_preds, axis=1)[:, ::-1][:, :3]\n    top3_labels = le.inverse_transform(top3_preds.flatten()).reshape(-1, 3)\n    sub_df['Fertilizer Name'] = [' '.join(row) for row in top3_labels]\n    sub_df.to_csv(\"submission_cnn_embedded.csv\", index=False)\n    print(\"Submission saved as 'submission_cnn_embedded.csv'\")\n\n# -----------------------------\nif __name__ == \"__main__\":\n    train_model()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T15:11:07.729877Z","iopub.execute_input":"2025-06-26T15:11:07.730217Z","execution_failed":"2025-06-26T15:43:47.733Z"}},"outputs":[{"name":"stdout","text":"--- Fold 1/5 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f9d14d4a8594c10a2850a05d69f7a1a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"import os\nimport gc\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport pytorch_lightning as pl\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\n\npl.seed_everything(42)\n\n# -----------------------------\n# MAP@3 scorer\n# -----------------------------\ndef calculate_map3(y_true, y_pred):\n    top3_preds_indices = np.argsort(y_pred, axis=1)[:, ::-1][:, :3]\n    scores = []\n    for i, true_label in enumerate(y_true):\n        top3 = top3_preds_indices[i]\n        score = 0.0\n        if true_label in top3:\n            rank = np.where(top3 == true_label)[0][0] + 1\n            score = 1.0 / rank\n        scores.append(score)\n    return np.mean(scores)\n\n# -----------------------------\n# Dataset class\n# -----------------------------\nclass FertilizerDataset(Dataset):\n    def __init__(self, X, y=None):\n        self.X = torch.tensor(X.values, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long) if y is not None else None\n\n    def __len__(self): return len(self.X)\n\n    def __getitem__(self, idx):\n        if self.y is not None:\n            return self.X[idx], self.y[idx]\n        return self.X[idx]\n\n# -----------------------------\n# CNN model\n# -----------------------------\nclass SoftOrdering1DCNN(pl.LightningModule):\n    def __init__(self, input_dim, output_dim, lr=1e-3):\n        super().__init__()\n        self.save_hyperparameters()\n        self.loss_fn = nn.CrossEntropyLoss()\n        self.lr = lr\n        self.val_outputs = []\n\n        self.model = nn.Sequential(\n            nn.BatchNorm1d(input_dim),\n            nn.Linear(input_dim, 256),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Unflatten(1, (16, 16)),         \n            nn.Conv1d(16, 32, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv1d(32, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv1d(64, 128, kernel_size=3, padding=1),  \n            nn.ReLU(),\n            nn.AdaptiveAvgPool1d(2),           \n            nn.Flatten(),                      \n            nn.BatchNorm1d(256),\n            nn.Linear(256, output_dim)\n        )\n\n    def forward(self, x): \n        return self.model(x)\n\n    def training_step(self, batch, _):\n        x, y = batch\n        logits = self(x)\n        loss = self.loss_fn(logits, y)\n        return loss\n\n    def validation_step(self, batch, _):\n        x, y = batch\n        logits = self(x)\n        probs = torch.softmax(logits, dim=1).detach().cpu().numpy()\n        y_true = y.cpu().numpy()\n        self.val_outputs.append((y_true, probs))\n\n    def on_validation_epoch_end(self):\n        y_true = np.concatenate([out[0] for out in self.val_outputs])\n        y_pred = np.concatenate([out[1] for out in self.val_outputs])\n        map3 = calculate_map3(y_true, y_pred)\n        self.log(\"val_map3\", map3, prog_bar=True)\n        self.val_outputs.clear()\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=self.lr)\n\n# -----------------------------\n# Load and preprocess data\n# -----------------------------\ndef load_data():\n    train_synth = pd.read_csv(\"/kaggle/input/playground-series-s5e6/train.csv\").drop(columns='id')\n    train_orig = pd.read_csv(\"/kaggle/input/fertilizer-prediction/Fertilizer Prediction.csv\")\n    test_df = pd.read_csv(\"/kaggle/input/playground-series-s5e6/test.csv\").drop(columns='id')\n    sub_df = pd.read_csv(\"/kaggle/input/playground-series-s5e6/sample_submission.csv\")\n    return train_synth, train_orig, test_df, sub_df\n\ndef feature_eng(df):\n    for col in df.select_dtypes(include=['int64', 'float64']).columns:\n        df[f'{col}_Binned'] = pd.cut(df[col], bins=5, labels=False).astype('category')\n    return df\n\ndef encode_features(train_synth, train_orig, test_df):\n    target_col = 'Fertilizer Name'\n    label_enc = LabelEncoder()\n    label_enc.fit(pd.concat([train_synth[target_col], train_orig[target_col]]))\n    y_synth = label_enc.transform(train_synth[target_col])\n    y_orig = label_enc.transform(train_orig[target_col])\n\n    train_synth = train_synth.drop(columns=[target_col])\n    train_orig = train_orig.drop(columns=[target_col])\n\n    full_df = pd.concat([train_synth, train_orig, test_df], axis=0)\n    for col in full_df.columns:\n        full_df[col] = full_df[col].astype(str).astype(\"category\")\n\n    full_encoded = pd.get_dummies(full_df)\n\n    n_synth = len(train_synth)\n    n_orig = len(train_orig)\n    X_synth = full_encoded.iloc[:n_synth].copy()\n    X_orig = full_encoded.iloc[n_synth:n_synth + n_orig].copy()\n    X_test = full_encoded.iloc[n_synth + n_orig:].copy()\n\n    return X_synth, y_synth, X_orig, y_orig, X_test, label_enc\n\n# -----------------------------\n# Main training loop\n# -----------------------------\ndef train_model():\n    train_synth, train_orig, test_df_raw, sub_df = load_data()\n\n    train_synth = feature_eng(train_synth)\n    train_orig = feature_eng(train_orig)\n    test_df_raw = feature_eng(test_df_raw)\n\n    X_synth, y_synth, X_orig, y_orig, X_test, le = encode_features(train_synth, train_orig, test_df_raw)\n\n    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    test_preds = np.zeros((len(X_test), len(le.classes_)))\n    oof_preds = np.zeros((len(X_synth), len(le.classes_)))\n\n    for fold, (train_idx, val_idx) in enumerate(skf.split(X_synth, y_synth)):\n        print(f\"--- Fold {fold + 1}/5 ---\")\n        X_train, y_train = X_synth.iloc[train_idx], y_synth[train_idx]\n        X_val, y_val = X_synth.iloc[val_idx], y_synth[val_idx]\n\n        X_train_aug = pd.concat([X_train] + [X_orig] * 5, ignore_index=True)\n        y_train_aug = np.concatenate([y_train] + [y_orig] * 5)\n\n        train_ds = FertilizerDataset(X_train_aug, y_train_aug)\n        val_ds = FertilizerDataset(X_val, y_val)\n        test_ds = FertilizerDataset(X_test)\n\n        train_loader = DataLoader(train_ds, batch_size=2048, shuffle=True)\n        val_loader = DataLoader(val_ds, batch_size=2048)\n        test_loader = DataLoader(test_ds, batch_size=2048)\n\n        model = SoftOrdering1DCNN(input_dim=X_train.shape[1], output_dim=len(le.classes_),lr=1e-3)\n\n        trainer = pl.Trainer(\n            max_epochs=30,\n            accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n            logger=False,\n            enable_checkpointing=False,\n            deterministic=True,\n            log_every_n_steps=5\n        )\n\n        trainer.fit(model, train_loader, val_loader)\n\n        model.eval()\n        preds = []\n        for batch in val_loader:\n            x, _ = batch\n            with torch.no_grad():\n                preds.append(torch.softmax(model(x.to(model.device)), dim=1).cpu().numpy())\n        oof_preds[val_idx] = np.concatenate(preds)\n\n        preds = []\n        for batch in test_loader:\n            x = batch\n            if isinstance(x, tuple): x = x[0]\n            with torch.no_grad():\n                preds.append(torch.softmax(model(x.to(model.device)), dim=1).cpu().numpy())\n        test_preds += np.concatenate(preds) / 5\n\n        gc.collect()\n\n    print(f\"Final MAP@3: {calculate_map3(y_synth, oof_preds):.5f}\")\n\n    top3_preds = np.argsort(test_preds, axis=1)[:, ::-1][:, :3]\n    top3_labels = le.inverse_transform(top3_preds.flatten()).reshape(-1, 3)\n    sub_df['Fertilizer Name'] = [' '.join(row) for row in top3_labels]\n    sub_df.to_csv(\"submission_cnn_onehot.csv\", index=False)\n    print(\"Submission saved as 'submission_cnn_onehot.csv'\")\n\n# -----------------------------\nif __name__ == \"__main__\":\n    train_model()##can you do categorical enbedding and hot encoding","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T15:45:08.238779Z","iopub.execute_input":"2025-06-26T15:45:08.239094Z","execution_failed":"2025-06-26T19:19:55.517Z"}},"outputs":[{"name":"stdout","text":"--- Fold 1/5 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7065b84e5bf8468c9be007b1d8bc68f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"--- Fold 2/5 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0771109a924b4a00b022b5cdf12856e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"--- Fold 3/5 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ecc1939428049d5a0ced6329d3bfcb8"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"from pytorch_tabnet.tab_model import TabNetClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nimport pandas as pd\nimport numpy as np\nimport torch\nimport os\nimport gc\n\n# Seed for reproducibility\nnp.random.seed(42)\n\ndef calculate_map3(y_true, y_pred_proba):\n    top3_preds = np.argsort(y_pred_proba, axis=1)[:, ::-1][:, :3]\n    score = 0.0\n    for i in range(len(y_true)):\n        if y_true[i] in top3_preds[i]:\n            idx = np.where(top3_preds[i] == y_true[i])[0][0]\n            score += 1.0 / (idx + 1)\n    return score / len(y_true)\n\ndef load_data():\n    train_synth = pd.read_csv(\"/kaggle/input/playground-series-s5e6/train.csv\").drop(columns='id')\n    train_orig = pd.read_csv(\"/kaggle/input/original/Fertilizer Prediction .csv\")\n    test_df = pd.read_csv(\"/kaggle/input/playground-series-s5e6/test.csv\").drop(columns='id')\n    sub_df = pd.read_csv(\"/kaggle/input/playground-series-s5e6/sample_submission.csv\")\n    return train_synth, train_orig, test_df, sub_df\n\ndef feature_eng(df):\n    for col in df.select_dtypes(include=['int64', 'float64']).columns:\n        df[f'{col}_Binned'] = pd.cut(df[col], bins=5, labels=False).astype('category')\n    return df\n\ndef encode_features(train_synth, train_orig, test_df):\n    target_col = 'Fertilizer Name'\n    le_target = LabelEncoder()\n    le_target.fit(pd.concat([train_synth[target_col], train_orig[target_col]]))\n    y_synth = le_target.transform(train_synth[target_col])\n    y_orig = le_target.transform(train_orig[target_col])\n\n    categorical_cols = ['Soil Type', 'Crop Type'] + [col for col in train_synth.columns if '_Binned' in col]\n    for col in categorical_cols:\n        le_feat = LabelEncoder()\n        full_col = pd.concat([train_synth[col].astype(str), train_orig[col].astype(str), test_df[col].astype(str)])\n        le_feat.fit(full_col)\n        train_synth[col] = le_feat.transform(train_synth[col].astype(str))\n        train_orig[col] = le_feat.transform(train_orig[col].astype(str))\n        test_df[col] = le_feat.transform(test_df[col].astype(str))\n\n    return train_synth[categorical_cols], y_synth, train_orig[categorical_cols], y_orig, test_df[categorical_cols], le_target\n\ndef train_tabnet():\n    train_synth, train_orig, test_df, sub_df = load_data()\n\n    train_synth = feature_eng(train_synth)\n    train_orig = feature_eng(train_orig)\n    test_df = feature_eng(test_df)\n\n    X_synth, y_synth, X_orig, y_orig, X_test, le = encode_features(train_synth, train_orig, test_df)\n\n    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    test_preds = np.zeros((len(X_test), len(le.classes_)))\n    oof_preds = np.zeros((len(X_synth), len(le.classes_)))\n\n    for fold, (train_idx, val_idx) in enumerate(skf.split(X_synth, y_synth)):\n        print(f\"Fold {fold+1}/5\")\n\n        X_train, y_train = X_synth.iloc[train_idx], y_synth[train_idx]\n        X_val, y_val = X_synth.iloc[val_idx], y_synth[val_idx]\n\n        X_train_aug = pd.concat([X_train] + [X_orig] * 5, ignore_index=True)\n        y_train_aug = np.concatenate([y_train] + [y_orig] * 5)\n\n        clf = TabNetClassifier(\n            seed=42,\n            verbose=0,\n            device_name='cuda' if torch.cuda.is_available() else 'cpu'\n        )\n\n        clf.fit(\n            X_train_aug.values, y_train_aug,\n            eval_set=[(X_val.values, y_val)],\n            eval_metric=['accuracy'],\n            max_epochs=200,\n            patience=20,\n            batch_size=1024,\n            virtual_batch_size=128,\n            drop_last=False\n        )\n\n        oof_preds[val_idx] = clf.predict_proba(X_val.values)\n        test_preds += clf.predict_proba(X_test.values) / 5\n        gc.collect()\n\n    score = calculate_map3(y_synth, oof_preds)\n    print(f\"Final CV MAP@3: {score:.5f}\")\n\n    top3 = np.argsort(test_preds, axis=1)[:, ::-1][:, :3]\n    top3_labels = le.inverse_transform(top3.flatten()).reshape(-1, 3)\n    sub_df['Fertilizer Name'] = [' '.join(row) for row in top3_labels]\n    sub_df.to_csv(\"submission_tabnet.csv\", index=False)\n    print(\"Saved submission_tabnet.csv\")\n\ntrain_tabnet()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
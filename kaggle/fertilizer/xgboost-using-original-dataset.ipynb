{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":91717,"databundleVersionId":12184666,"sourceType":"competition"},{"sourceId":11592231,"sourceType":"datasetVersion","datasetId":7269189},{"sourceId":12251741,"sourceType":"datasetVersion","datasetId":7719660}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-23T02:48:47.668457Z","iopub.execute_input":"2025-06-23T02:48:47.668670Z","iopub.status.idle":"2025-06-23T02:48:48.942634Z","shell.execute_reply.started":"2025-06-23T02:48:47.668643Z","shell.execute_reply":"2025-06-23T02:48:48.941959Z"},"editable":false},"outputs":[{"name":"stdout","text":"/kaggle/input/playground-series-s5e6/sample_submission.csv\n/kaggle/input/playground-series-s5e6/train.csv\n/kaggle/input/playground-series-s5e6/test.csv\n/kaggle/input/firstlayer/oof_preds_hgb_fixed_params.npy\n/kaggle/input/firstlayer/oof_preds_nn_keras.npy\n/kaggle/input/firstlayer/oof_preds_nb_onehot.npy\n/kaggle/input/firstlayer/oof_preds.npy\n/kaggle/input/firstlayer/test_preds.npy\n/kaggle/input/firstlayer/test_preds_nb_onehot.npy\n/kaggle/input/firstlayer/test_preds_nn_keras.npy\n/kaggle/input/firstlayer/test_preds_hgb_fixed_params.npy\n/kaggle/input/fertilizer-prediction/Fertilizer Prediction.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n# No xgboost needed\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nimport gc\nimport warnings\nimport os\nimport optuna # Added for hyperparameter optimization\nimport lightgbm as lgb # Changed from xgboost to lightgbm\n\n# Suppress warnings for cleaner output\nwarnings.filterwarnings('ignore')\n\n# --- Reusable Functions ---\ndef calculate_map3(y_true, y_pred_proba):\n    \"\"\"Calculates the Mean Average Precision @ 3 score.\"\"\"\n    # Get the indices of the top 3 predictions for each sample\n    top3_preds_indices = np.argsort(y_pred_proba, axis=1)[:, ::-1][:, :3]\n    \n    scores = []\n    for i, true_label in enumerate(y_true):\n        top3 = top3_preds_indices[i]\n        score = 0.0\n        # Check if the true label is in the top 3 predictions\n        if true_label in top3:\n            # Find the rank (1, 2, or 3)\n            rank = np.where(top3 == true_label)[0][0] + 1\n            if rank == 1:\n                score = 1.0\n            elif rank == 2:\n                score = 0.5\n            elif rank == 3:\n                score = 1/3\n        scores.append(score)\n    return np.mean(scores)\n\n# --- Feature Engineering Function (Converts Numerical to Binned Categorical) ---\ndef feature_eng(df, target_col=None):\n    # Identify numerical columns to be binned\n    numerical_features = [col for col in df.select_dtypes(include=['int64', 'float64']).columns\n                            if col != 'id'] # Exclude 'id'\n    if target_col and target_col in numerical_features: # Exclude target if it's numerical\n        numerical_features.remove(target_col)\n\n    for col in numerical_features:\n        # Create a new column with '_Binned' suffix\n        # Convert numerical values to string, then to 'category' dtype\n        if f'{col}_Binned' not in df.columns: # Avoid recreating if already exists\n            df[f'{col}_Binned'] = df[col].astype(str).astype('category')\n    return df\n\n# --- Optuna Objective Function for LightGBM Hyperparameter Tuning ---\ndef objective(trial, X_synth, y_synth, X_orig, y_orig, test_df_feature_names, num_classes, calculate_map3_func):\n    \"\"\"\n    Objective function for Optuna to optimize LightGBM hyperparameters.\n    \"\"\"\n    # Define hyperparameter search space for LightGBM\n    params = {\n        'objective': 'multiclass', # LightGBM objective for multiclass classification\n        'num_class': num_classes,\n        'metric': 'multi_logloss', # Evaluation metric\n        'boosting_type': 'gbdt',   # Gradient Boosting Decision Tree\n        'n_estimators': 5000,      # Max number of boosting rounds\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n        'num_leaves': trial.suggest_int('num_leaves', 20, 200), # Number of leaves in one tree\n        'max_depth': trial.suggest_int('max_depth', 3, 15),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100), # Min data in a child leaf\n        'subsample': trial.suggest_float('subsample', 0.4, 1.0), # bagging_fraction in LightGBM\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.4, 1.0), # feature_fraction in LightGBM\n        'reg_alpha': trial.suggest_float('reg_alpha', 0.01, 10.0, log=True), # L1 regularization (lambda_l1)\n        'reg_lambda': trial.suggest_float('reg_lambda', 0.01, 10.0, log=True), # L2 regularization (lambda_l2)\n        'random_state': 42, # Keep seed fixed for reproducibility of trials\n        'n_jobs': -1,       # Use all available cores\n        'verbose': -1,      # Suppress verbose output during training\n        'device': 'gpu' if hasattr(lgb, 'GPU_DEVICE') else 'cpu', # Use GPU if available\n    }\n\n    NFOLDS_OPTUNA = 5 # Fixed to 5 folds for Optuna\n    skf_optuna = StratifiedKFold(n_splits=NFOLDS_OPTUNA, shuffle=True, random_state=42)\n\n    oof_preds_trial = np.zeros((len(X_synth), num_classes))\n    \n    # Octuple the original data once for the objective function.\n    X_orig_oct_obj = pd.concat([X_orig] * 5, ignore_index=True)\n    y_orig_oct_obj = np.concatenate([y_orig] * 5)\n\n    for fold_num, (train_idx, val_idx) in enumerate(skf_optuna.split(X_synth, y_synth)):\n        X_synth_train, y_synth_train = X_synth.iloc[train_idx], y_synth[train_idx]\n        X_synth_val, y_synth_val = X_synth.iloc[val_idx], y_synth[val_idx]\n\n        # Augment the training data with the OCTUPLED original dataset\n        X_train_aug = pd.concat([X_synth_train, X_orig_oct_obj], ignore_index=True)\n        y_train_aug = np.concatenate([y_synth_train, y_orig_oct_obj])\n\n        # LightGBM requires Dataset objects\n        # `categorical_feature='auto'` allows LightGBM to detect categorical columns if they are `category` dtype.\n        # Or, pass feature_names as a list of strings: `categorical_feature=[col for col in X_train_aug.columns if X_train_aug[col].dtype.name == 'category']`\n        lgb_train = lgb.Dataset(X_train_aug, y_train_aug, categorical_feature='auto')\n        lgb_eval = lgb.Dataset(X_synth_val, y_synth_val, reference=lgb_train, categorical_feature='auto')\n\n        model = lgb.train(\n            params,\n            lgb_train,\n            valid_sets=[lgb_eval],\n            callbacks=[lgb.early_stopping(50, verbose=False)] # Early stopping\n        )\n\n        oof_preds_trial[val_idx] = model.predict(X_synth_val, num_iteration=model.best_iteration)\n        \n        del model, lgb_train, lgb_eval\n        gc.collect()\n\n    map3_score_trial = calculate_map3_func(y_synth, oof_preds_trial)\n    return map3_score_trial\n\n\n# --- Main Execution ---\nif __name__ == \"__main__\":\n    # Define target column name\n    TARGET_COL = 'Fertilizer Name'\n\n    # --- 1. Load Data ---\n    print(\"Loading datasets...\")\n    train_synthetic_df = pd.read_csv(\"/kaggle/input/playground-series-s5e6/train.csv\")\n    train_original_df = pd.read_csv(\"/kaggle/input/fertilizer-prediction/Fertilizer Prediction.csv\")\n    test_df_raw = pd.read_csv(\"/kaggle/input/playground-series-s5e6/test.csv\")\n    submission_df = pd.read_csv(\"/kaggle/input/playground-series-s5e6/sample_submission.csv\")\n\n    # Drop 'id' columns\n    train_synthetic_df = train_synthetic_df.drop(columns=['id'])\n    if 'id' in test_df_raw.columns:\n        test_df_raw = test_df_raw.drop(columns=['id'])\n\n    # --- Apply Feature Engineering to make all numerical features categorical ---\n    print(\"Applying feature engineering (binning numerical features into categories)...\")\n    train_synthetic_df = feature_eng(train_synthetic_df, target_col=TARGET_COL)\n    train_original_df = feature_eng(train_original_df, target_col=TARGET_COL)\n    test_df_raw = feature_eng(test_df_raw)\n\n\n    # --- 2. Preprocessing and Feature Preparation ---\n    print(\"Applying preprocessing...\")\n    # Target Encoding\n    le_fertilizer = LabelEncoder()\n    # Fit on combined labels to ensure all possible classes are covered\n    le_fertilizer.fit(pd.concat([train_synthetic_df[TARGET_COL], train_original_df[TARGET_COL]]))\n    y_synth = le_fertilizer.transform(train_synthetic_df[TARGET_COL])\n    y_orig = le_fertilizer.transform(train_original_df[TARGET_COL])\n    num_classes = len(le_fertilizer.classes_)\n\n    # Dynamically determine the common feature columns after feature engineering\n    base_categorical_features = ['Soil Type', 'Crop Type']\n    \n    # Collect all binned columns that are present across all three dataframes\n    all_processed_dfs = [train_synthetic_df, train_original_df, test_df_raw]\n    common_binned_cols = set()\n\n    # Get binned columns from the first dataframe\n    if all_processed_dfs:\n        first_df_binned = [col for col in all_processed_dfs[0].columns if col.endswith('_Binned')]\n        common_binned_cols.update(first_df_binned)\n\n        # Intersect with binned columns from subsequent dataframes\n        for df_item in all_processed_dfs[1:]:\n            current_df_binned = [col for col in df_item.columns if col.endswith('_Binned')]\n            common_binned_cols.intersection_update(current_df_binned)\n\n    # The final feature_cols will be the base categorical plus the common binned ones\n    feature_cols = base_categorical_features + sorted(list(common_binned_cols))\n    \n    print(f\"Selected feature columns (all categorical): {feature_cols}\")\n\n    # Feature Sets - now containing only categorical/binned features\n    X_synth = train_synthetic_df[feature_cols].copy()\n    X_orig = train_original_df[feature_cols].copy()\n    test_df = test_df_raw[feature_cols].copy() # Ensure test_df also has only selected features\n\n    # Categorical Feature Encoding (Using LabelEncoder for consistent integer mapping)\n    for col in feature_cols:\n        # Concatenate all categories from all datasets for consistent mapping\n        full_vocab = pd.concat([X_synth[col].astype(str), X_orig[col].astype(str), test_df[col].astype(str)], axis=0).unique()\n        \n        label_enc = LabelEncoder()\n        label_enc.fit(full_vocab.astype(str)) # Fit on the full vocabulary of strings\n        \n        X_synth[col] = label_enc.transform(X_synth[col].astype(str))\n        X_orig[col] = label_enc.transform(X_orig[col].astype(str))\n        test_df[col] = label_enc.transform(test_df[col].astype(str))\n        \n        # Cast to 'category' dtype after encoding, as LightGBM can leverage this\n        X_synth[col] = X_synth[col].astype(\"category\")\n        X_orig[col] = X_orig[col].astype(\"category\")\n        test_df[col] = test_df[col].astype(\"category\")\n\n\n    # --- 3. Optuna Hyperparameter Optimization ---\n    print(\"\\nStarting Optuna hyperparameter optimization (5-Fold CV)...\")\n    # Using TPESampler for more efficient exploration\n    study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))\n    study.optimize(\n        lambda trial: objective(trial, X_synth, y_synth, X_orig, y_orig, X_synth.columns.tolist(), num_classes, calculate_map3),\n        n_trials=50, # Number of trials for Optuna to run. Adjust as needed.\n        show_progress_bar=True\n    )\n\n    print(\"\\nOptuna optimization finished.\")\n    print(f\"Best trial: {study.best_trial.value:.5f} (MAP@3)\")\n    print(\"Best hyperparameters found:\")\n    for key, value in study.best_params.items():\n        print(f\"  {key}: {value}\")\n\n    # --- 4. Define Final Hyperparameters using Optuna's Best ---\n    # Start with fixed parameters, then override with best params from Optuna\n    final_params = {\n        'objective': 'multiclass',\n        'num_class': num_classes,\n        'metric': 'multi_logloss',\n        'boosting_type': 'gbdt',\n        'n_estimators': 5000, # Max number of boosting rounds for final model\n        'random_state': 42,\n        'n_jobs': -1,\n        'verbose': -1, # Suppress verbose output\n        'device': 'gpu' if hasattr(lgb, 'GPU_DEVICE') else 'cpu',\n        # These will be set by Optuna's best params\n        'learning_rate': study.best_params['learning_rate'],\n        'num_leaves': study.best_params['num_leaves'],\n        'max_depth': study.best_params['max_depth'],\n        'min_child_samples': study.best_params['min_child_samples'],\n        'subsample': study.best_params['subsample'],\n        'colsample_bytree': study.best_params['colsample_bytree'],\n        'reg_alpha': study.best_params['reg_alpha'],\n        'reg_lambda': study.best_params['reg_lambda'],\n    }\n\n    # --- 5. Train Final Model with 5-Fold CV & 5x Original Data Augmentation (using Best Params) ---\n    print(\"\\nTraining final LightGBM model with best hyperparameters (5-Fold CV & 5x original data augmentation)...\")\n    NFOLDS = 5 # Fixed to 5 folds for final training and prediction\n    skf = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=42)\n\n    oof_preds = np.zeros((len(X_synth), num_classes))\n    test_preds = np.zeros((len(test_df), num_classes))\n\n    for fold, (train_idx, val_idx) in enumerate(skf.split(X_synth, y_synth)):\n        print(f\"--- Fold {fold+1}/{NFOLDS} ---\")\n        X_synth_train, y_synth_train = X_synth.iloc[train_idx], y_synth[train_idx]\n        X_synth_val, y_synth_val = X_synth.iloc[val_idx], y_synth[val_idx]\n\n        # Octuple the original data to give it more weight (5x)\n        X_orig_oct = pd.concat([X_orig] * 5, ignore_index=True)\n        y_orig_oct = np.concatenate([y_orig] * 5)\n\n        # Augment the training data with the OCTUPLED original dataset\n        X_train_aug = pd.concat([X_synth_train, X_orig_oct], ignore_index=True)\n        y_train_aug = np.concatenate([y_synth_train, y_orig_oct])\n\n        # LightGBM Dataset for training and validation\n        lgb_train = lgb.Dataset(X_train_aug, y_train_aug, categorical_feature='auto')\n        lgb_eval = lgb.Dataset(X_synth_val, y_synth_val, reference=lgb_train, categorical_feature='auto')\n        \n        # Train the LightGBM model\n        model = lgb.train(\n            final_params,\n            lgb_train,\n            valid_sets=[lgb_eval],\n            callbacks=[lgb.early_stopping(50, verbose=False)], # Early stopping\n            verbose_eval=False # Suppress training verbosity\n        )\n\n        oof_preds[val_idx] = model.predict(X_synth_val, num_iteration=model.best_iteration)\n        test_preds += model.predict(test_df, num_iteration=model.best_iteration) / NFOLDS\n        gc.collect()\n\n    # --- 6. Evaluate and Save ---\n    map3_score = calculate_map3(y_synth, oof_preds)\n    print(f\"\\n--- Final Model CV MAP@3 (5x Original Data, Best Params): {map3_score:.5f} ---\\n\")\n\n    # Save OOF and Test Predictions\n    output_dir = './' # Save in current directory\n    os.makedirs(output_dir, exist_ok=True) # Ensure directory exists\n\n    oof_filename = os.path.join(output_dir, 'oof_preds_lgbm.npy') # Changed filename\n    test_filename = os.path.join(output_dir, 'test_preds_lgbm.npy') # Changed filename\n\n    np.save(oof_filename, oof_preds)\n    np.save(test_filename, test_preds)\n    print(f\"OOF predictions saved to: {oof_filename}\")\n    print(f\"Test predictions saved to: {test_filename}\")\n\n    print(\"Generating final submission file...\")\n    top3_preds_indices = np.argsort(test_preds, axis=1)[:, ::-1][:, :3]\n    top3_preds_labels = le_fertilizer.inverse_transform(top3_preds_indices.flatten()).reshape(top3_preds_indices.shape)\n\n    submission_df['Fertilizer Name'] = [' '.join(row) for row in top3_preds_labels]\n    submission_df.to_csv('submission_lgbm.csv', index=False) # Changed submission filename\n\n    print(\"Submission file 'submission_lgbm.csv' created successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T10:00:18.688287Z","iopub.execute_input":"2025-06-23T10:00:18.688982Z"},"editable":false},"outputs":[{"name":"stdout","text":"Loading datasets...\nApplying feature engineering (binning numerical features into categories)...\nApplying preprocessing...\nSelected feature columns (all categorical): ['Soil Type', 'Crop Type', 'Humidity_Binned', 'Moisture_Binned', 'Nitrogen_Binned', 'Phosphorous_Binned', 'Potassium_Binned', 'Temparature_Binned']\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-06-23 10:00:28,067] A new study created in memory with name: no-name-b6f2e14a-356e-4fb4-b065-60a74d288197\n","output_type":"stream"},{"name":"stdout","text":"\nStarting Optuna hyperparameter optimization (5-Fold CV)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/50 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[I 2025-06-23 10:54:28,535] Trial 0 finished with value: 0.36666422222222217 and parameters: {'learning_rate': 0.023688639503640783, 'num_leaves': 192, 'max_depth': 12, 'min_child_samples': 62, 'subsample': 0.4936111842654619, 'colsample_bytree': 0.49359671220172163, 'reg_alpha': 0.014936568554617643, 'reg_lambda': 3.9676050770529883}. Best is trial 0 with value: 0.36666422222222217.\n[I 2025-06-23 12:40:24,512] Trial 1 finished with value: 0.3648224444444444 and parameters: {'learning_rate': 0.039913058785616795, 'num_leaves': 148, 'max_depth': 3, 'min_child_samples': 98, 'subsample': 0.899465584480253, 'colsample_bytree': 0.5274034664069657, 'reg_alpha': 0.035113563139704075, 'reg_lambda': 0.03549878832196503}. Best is trial 0 with value: 0.36666422222222217.\n[I 2025-06-23 13:54:26,194] Trial 2 finished with value: 0.3676902222222222 and parameters: {'learning_rate': 0.02014847788415866, 'num_leaves': 114, 'max_depth': 8, 'min_child_samples': 32, 'subsample': 0.7671117368334277, 'colsample_bytree': 0.4836963163912251, 'reg_alpha': 0.07523742884534858, 'reg_lambda': 0.1256277350380703}. Best is trial 2 with value: 0.3676902222222222.\n[I 2025-06-23 15:28:25,084] Trial 3 finished with value: 0.3724864444444444 and parameters: {'learning_rate': 0.028580510658069373, 'num_leaves': 162, 'max_depth': 5, 'min_child_samples': 54, 'subsample': 0.7554487413172255, 'colsample_bytree': 0.42787024763199866, 'reg_alpha': 0.6647135865318028, 'reg_lambda': 0.03247673570627449}. Best is trial 3 with value: 0.3724864444444444.\n[I 2025-06-23 17:24:02,722] Trial 4 finished with value: 0.3679468888888888 and parameters: {'learning_rate': 0.011615865989246453, 'num_leaves': 191, 'max_depth': 15, 'min_child_samples': 82, 'subsample': 0.5827682615040224, 'colsample_bytree': 0.45860326840383037, 'reg_alpha': 1.1290133559092674, 'reg_lambda': 0.2091498132903561}. Best is trial 3 with value: 0.3724864444444444.\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null}]}
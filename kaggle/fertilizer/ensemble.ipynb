{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91717,"databundleVersionId":12184666,"sourceType":"competition"},{"sourceId":12064209,"sourceType":"datasetVersion","datasetId":7593561},{"sourceId":12160557,"sourceType":"datasetVersion","datasetId":7658717},{"sourceId":12165301,"sourceType":"datasetVersion","datasetId":7661947},{"sourceId":12176527,"sourceType":"datasetVersion","datasetId":7668855},{"sourceId":12194745,"sourceType":"datasetVersion","datasetId":7681490}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-19T03:37:53.633631Z","iopub.execute_input":"2025-06-19T03:37:53.633969Z","iopub.status.idle":"2025-06-19T03:37:55.863604Z","shell.execute_reply.started":"2025-06-19T03:37:53.633935Z","shell.execute_reply":"2025-06-19T03:37:55.862627Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/ensemble3/oof_preds_xgb_custom.npy\n/kaggle/input/ensemble3/test_preds_xgb_optimized.npy\n/kaggle/input/ensemble3/oof_preds_xgb_optimized.npy\n/kaggle/input/ensemble3/test_preds_xgb_custom.npy\n/kaggle/input/ensemble6/pred_test_log_proba.csv\n/kaggle/input/ensemble6/xgb_repeat_train_oof.npy\n/kaggle/input/ensemble6/pred_oof_log_proba.csv\n/kaggle/input/ensemble6/xgb_repeat_test_oof.npy\n/kaggle/input/ensemble4/test_preds_gaussian_nb.npy\n/kaggle/input/ensemble4/test_preds_lda_base.npy\n/kaggle/input/ensemble4/oof_preds_gaussian_nb.npy\n/kaggle/input/ensemble4/oof_preds_lda_base.npy\n/kaggle/input/ensemble5/test_preds_hgb.npy\n/kaggle/input/ensemble5/oof_preds_hgb.npy\n/kaggle/input/ensemble5/oof_preds_ydf.npy\n/kaggle/input/ensemble5/test_preds_ydf.npy\n/kaggle/input/boosting-output/test_preds_xgb_sklearn.npy\n/kaggle/input/boosting-output/oof_preds_xgb_sklearn.npy\n/kaggle/input/boosting-output/test_preds_lgbm.npy\n/kaggle/input/boosting-output/oof_preds_xgb_train_api2.npy\n/kaggle/input/boosting-output/oof_preds_lgbm.npy\n/kaggle/input/boosting-output/oof_preds_xgb_train_api1.npy\n/kaggle/input/boosting-output/test_preds_lgbm_goss.npy\n/kaggle/input/boosting-output/oof_preds_lgbm_goss.npy\n/kaggle/input/boosting-output/test_preds_xgb_train_api2.npy\n/kaggle/input/boosting-output/test_preds_xgb_train_api1.npy\n/kaggle/input/original/Fertilizer Prediction .csv\n/kaggle/input/playground-series-s5e6/sample_submission.csv\n/kaggle/input/playground-series-s5e6/train.csv\n/kaggle/input/playground-series-s5e6/test.csv\n/kaggle/input/ensemble2/oof_preds_lr.npy\n/kaggle/input/ensemble2/oof_preds_voting.npy\n/kaggle/input/ensemble2/oof_preds_lgbm_optimized.npy\n/kaggle/input/ensemble2/test_preds_voting.npy\n/kaggle/input/ensemble2/test_preds_lr.npy\n/kaggle/input/ensemble2/test_preds_lgbm_optimized.npy\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nfrom datetime import datetime # Import datetime for timestamps\nimport optuna # Still imported but not used directly in the provided snippets\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.preprocessing import OrdinalEncoder, LabelEncoder, StandardScaler\nfrom sklearn.metrics import log_loss\nimport warnings\nimport gc\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.naive_bayes import GaussianNB # Import Gaussian Naive Bayes\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis # Import Linear Discriminant Analysis\nfrom sklearn.ensemble import HistGradientBoostingClassifier # Import HistGradientBoostingClassifier\nimport ydf # NEW: Import Yggdrasil Decision Forests\n\n\n# Suppress warnings for cleaner output\nwarnings.filterwarnings('ignore')\n\n# --- 1. Configuration and Global Random State ---\nclass CFG:\n    seed = 42\n    target = 'Fertilizer Name'\n    n_splits = 5 # Number of folds for cross-validation\n    learning_rate = 0.03\n    num_boost_round = 5000\n    early_stopping_rounds = 50\n    verbose_eval = 200\n\n    # --- Output Directory for current session (will be cleared on session end) ---\n    OUTPUT_DIR = '/kaggle/working/outputs/' \n    \n    # --- Directory for User Uploaded Input Files (e.g., from a Kaggle Dataset) ---\n    # Set to /kaggle/input/ensemble2/ as the primary uploaded input dir for new models\n    UPLOADED_INPUT_DIR = '/kaggle/input/ensemble2/' \n\n    # Filenames for base model predictions (ONLY INCLUDING ACTIVE MODELS)\n    FNAME_XGB_API1_OOF = 'oof_preds_xgb_train_api1.npy'\n    FNAME_XGB_API1_TEST = 'test_preds_xgb_train_api1.npy'\n    \n    FNAME_XGB_SKLEARN_OOF = 'oof_preds_xgb_sklearn.npy'\n    FNAME_XGB_SKLEARN_TEST = 'test_preds_xgb_sklearn.npy'\n    \n    FNAME_LGBM_OOF = 'oof_preds_lgbm.npy'\n    FNAME_LGBM_TEST = 'test_preds_lgbm.npy'\n    \n    FNAME_LR_OOF = 'oof_preds_lr.npy'\n    FNAME_LR_TEST = 'test_preds_lr.npy'\n\n    FNAME_LGBM_OPTIMIZED_OOF = 'oof_preds_lgbm_optimized.npy'\n    FNAME_LGBM_OPTIMIZED_TEST = 'test_preds_lgbm_optimized.npy'\n\n    FNAME_VOTING_OOF = 'oof_preds_voting.npy'\n    FNAME_VOTING_TEST = 'test_preds_voting.npy'\n\n    FNAME_GAUSSIAN_NB_OOF = 'oof_preds_gaussian_nb.npy'\n    FNAME_GAUSSIAN_NB_TEST = 'test_preds_gaussian_nb.npy'\n\n    FNAME_HGB_OOF = 'oof_preds_hgb.npy'\n    FNAME_HGB_TEST = 'test_preds_hgb.npy'\n\n    FNAME_YDF_OOF = 'oof_preds_ydf.npy'\n    FNAME_YDF_TEST = 'test_preds_ydf.npy'\n\n    # NEW: Filenames for the new custom XGB model\n    FNAME_XGB_CUSTOM_NEW_OOF = 'oof_preds_xgb_custom_new.npy'\n    FNAME_XGB_CUSTOM_NEW_TEST = 'test_preds_xgb_custom_new.npy'\n\n\n# Create the output directory if it doesn't exist. This is essential for saving.\nos.makedirs(CFG.OUTPUT_DIR, exist_ok=True)\n\n# Define FOLDS based on CFG for consistency\nFOLDS = CFG.n_splits\nGLOBAL_RANDOM_STATE = CFG.seed\nnp.random.seed(GLOBAL_RANDOM_STATE)\n\n# --- Helper function to check and load predictions from either source ---\ndef load_predictions_if_exist(oof_filename, test_filename, X_shape, X_test_shape, num_classes):\n    oof_path_output = os.path.join(CFG.OUTPUT_DIR, oof_filename)\n    test_path_output = os.path.join(CFG.OUTPUT_DIR, test_filename)\n    \n    uploaded_input_dir_exists = hasattr(CFG, 'UPLOADED_INPUT_DIR') and CFG.UPLOADED_INPUT_DIR\n    \n    oof_path_primary_input = None\n    test_path_primary_input = None\n    if uploaded_input_dir_exists:\n        oof_path_primary_input = os.path.join(CFG.UPLOADED_INPUT_DIR, oof_filename) # /kaggle/input/ensemble2/\n        test_path_primary_input = os.path.join(CFG.UPLOADED_INPUT_DIR, test_filename)\n\n    # Specific path for /kaggle/input/ensemble3/\n    oof_path_ensemble3_input = os.path.join('/kaggle/input/ensemble3/', oof_filename)\n    test_path_ensemble3_input = os.path.join('/kaggle/input/ensemble3/', test_filename)\n\n    # Specific path for /kaggle/input/ensemble4/\n    oof_path_ensemble4_input = os.path.join('/kaggle/input/ensemble4/', oof_filename)\n    test_path_ensemble4_input = os.path.join('/kaggle/input/ensemble4/', test_filename)\n\n    # NEW: Specific path for /kaggle/input/ensemble5/ (for HGB and YDF typically)\n    oof_path_ensemble5_input = os.path.join('/kaggle/input/ensemble5/', oof_filename)\n    test_path_ensemble5_input = os.path.join('/kaggle/input/ensemble5/', test_filename)\n\n    # Original secondary input directory for boosting-output files\n    oof_path_boosting_output_input = os.path.join('/kaggle/input/boosting-output/', oof_filename)\n    test_path_boosting_output_input = os.path.join('/kaggle/input/boosting-output/', test_filename)\n\n\n    # Priority 1: Check in the current session's output directory\n    if os.path.exists(oof_path_output) and os.path.exists(test_path_output):\n        print(f\"Loading predictions from current session's OUTPUT_DIR: {oof_filename}, {test_filename}\")\n        return np.load(oof_path_output), np.load(test_path_output), True\n    # Priority 2: Check in the primary user-uploaded input directory (e.g., /kaggle/input/ensemble2/)\n    elif uploaded_input_dir_exists and os.path.exists(oof_path_primary_input) and os.path.exists(test_path_primary_input):\n        print(f\"Loading predictions from primary UPLOADED_INPUT_DIR ({CFG.UPLOADED_INPUT_DIR}): {oof_filename}, {test_filename}\")\n        return np.load(oof_path_primary_input), np.load(test_path_primary_input), True\n    # Priority 3: Check in the /kaggle/input/ensemble3/ directory\n    elif os.path.exists(oof_path_ensemble3_input) and os.path.exists(test_path_ensemble3_input):\n        print(f\"Loading predictions from /kaggle/input/ensemble3/: {oof_filename}, {test_filename}\")\n        return np.load(oof_path_ensemble3_input), np.load(test_path_ensemble3_input), True\n    # Priority 4: Check in the /kaggle/input/ensemble4/ directory\n    elif os.path.exists(oof_path_ensemble4_input) and os.path.exists(test_path_ensemble4_input):\n        print(f\"Loading predictions from /kaggle/input/ensemble4/: {oof_filename}, {test_filename}\")\n        return np.load(oof_path_ensemble4_input), np.load(test_path_ensemble4_input), True\n    # NEW Priority 5: Check in the /kaggle/input/ensemble5/ directory\n    elif os.path.exists(oof_path_ensemble5_input) and os.path.exists(test_path_ensemble5_input):\n        print(f\"Loading predictions from /kaggle/input/ensemble5/: {oof_filename}, {test_filename}\")\n        return np.load(oof_path_ensemble5_input), np.load(test_path_ensemble5_input), True\n    # Priority 6: Check in the secondary user-uploaded input directory (e.g., /kaggle/input/boosting-output/)\n    elif os.path.exists(oof_path_boosting_output_input) and os.path.exists(test_path_boosting_output_input):\n        print(f\"Loading predictions from secondary /kaggle/input/boosting-output/: {oof_filename}, {test_filename}\")\n        return np.load(oof_path_boosting_output_input), np.load(test_path_boosting_output_input), True\n    else:\n        # If files are not found, return zero-initialized arrays\n        print(f\"No existing predictions found for {oof_filename} or {test_filename}. Will initialize as zeros.\")\n        return np.zeros((X_shape[0], num_classes)), np.zeros((X_test_shape[0], num_classes)), False\n\n\n# --- 2. Data Loading and Initial Preprocessing ---\nprint(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] --- Starting Data Loading and Initial Preprocessing ---\")\nstart_time_data_load = datetime.now()\n\n# Ensure these paths are correct for your environment\ndf_train = pd.read_csv('/kaggle/input/playground-series-s5e6/train.csv')\ndf_test = pd.read_csv('/kaggle/input/playground-series-s5e6/test.csv')\ndf_sub = pd.read_csv('/kaggle/input/playground-series-s5e6/sample_submission.csv')\ndf_original = pd.read_csv('/kaggle/input/original/Fertilizer Prediction .csv')\n\n# Drop 'id' columns if they exist in train/test sets as per original notebook\ndf_train = df_train.drop(columns=['id'])\nif 'id' in df_test.columns:\n    df_test = df_test.drop(columns=['id'])\n\n# Concatenate original dataset to the training data\ndf_train = pd.concat([df_train, df_original], axis=0, ignore_index=True)\n\n# --- 3. Ordinal and Label Encoding ---\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Starting Ordinal and Label Encoding ---\")\ncat_cols_for_ordinal = df_train.select_dtypes(include='object').columns.tolist()\nif 'Fertilizer Name' in cat_cols_for_ordinal:\n    cat_cols_for_ordinal.remove('Fertilizer Name')\n\nordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\ndf_train[cat_cols_for_ordinal] = ordinal_encoder.fit_transform(df_train[cat_cols_for_ordinal].astype(str)).astype(int)\n\ncat_cols_for_test = [col for col in cat_cols_for_ordinal if col in df_test.columns]\ndf_test[cat_cols_for_test] = ordinal_encoder.transform(df_test[cat_cols_for_test].astype(str)).astype(int)\n\nle = LabelEncoder()\ndf_train['Fertilizer Name'] = le.fit_transform(df_train['Fertilizer Name'])\nnum_classes = len(np.unique(df_train['Fertilizer Name']))\n\ny_encoded = df_train['Fertilizer Name'] # Target for training\nX = df_train.drop(columns=['Fertilizer Name']) # Features for training\nX_test = df_test # Features for final test prediction\n\n# Define numerical columns for scaling\nnumerical_cols = X.select_dtypes(include=np.number).columns.tolist()\n# Filter out any columns that might have been ordinal encoded but are numeric-like\nnumerical_cols = [col for col in numerical_cols if col not in cat_cols_for_ordinal]\n\n# Get indices of categorical features for HistGradientBoostingClassifier\n# Since we ordinal encoded them to integers, we need to explicitly tell HGB which ones are categorical\ncategorical_feature_indices_hgb = [X.columns.get_loc(col) for col in cat_cols_for_ordinal]\nprint(f\"HistGradientBoostingClassifier will treat columns at indices {categorical_feature_indices_hgb} as categorical.\")\n\nend_time_data_load = datetime.now()\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Data Loading and Preprocessing Finished. Elapsed: {end_time_data_load - start_time_data_load} ---\\n\")\n\n# Define mapk function\ndef mapk(actual, predicted, k=3):\n    def apk(a, p, k):\n        p = p[:k]\n        score = 0.0\n        hits = 0\n        seen = set()\n        for i, pred in enumerate(p):\n            if pred in a and pred not in seen:\n                hits += 1\n                score += hits / (i + 1.0)\n                seen.add(pred)\n            if hits == k: # Optimized: if we have found 'k' items, no need to continue\n                break\n        return score / min(len(a), k) if min(len(a), k) > 0 else 0.0 # Return 0.0 if actual has 0 elements to avoid division by zero\n\n    if not isinstance(actual[0], (list, np.ndarray)):\n        actual = [[a] for a in actual]\n\n    return np.mean([apk(a, p, k) for a, p in zip(actual, predicted)])\n\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Stacking Ensemble Setup Started ---\")\n\n# --- Initialize OOF and Test Prediction Arrays for Each Active Base Model (10 Models Now) ---\noof_preds_xgb_train_api1 = np.empty(0)\ntest_preds_xgb_train_api1 = np.empty(0)\n\noof_preds_xgb_sklearn = np.empty(0)\ntest_preds_xgb_sklearn = np.empty(0)\n\noof_preds_lgbm = np.empty(0)\ntest_preds_lgbm = np.empty(0)\n\noof_preds_lr = np.empty(0)\ntest_preds_lr = np.empty(0)\n\noof_preds_lgbm_optimized = np.empty(0) # Model 7\ntest_preds_lgbm_optimized = np.empty(0) # Model 7\n\noof_preds_voting = np.empty(0)          # Model 8\ntest_preds_voting = np.empty(0)         # Model 8\n\noof_preds_gaussian_nb = np.empty(0) # Model 11 (Gaussian Naive Bayes)\ntest_preds_gaussian_nb = np.empty(0) # Model 11 (Gaussian Naive Bayes)\n\noof_preds_hgb = np.empty(0) # Model 13 (HistGradientBoostingClassifier)\ntest_preds_hgb = np.empty(0) # Model 13 (HistGradientBoostingClassifier)\n\noof_preds_ydf = np.empty(0) # NEW Model 14 (YDF)\ntest_preds_ydf = np.empty(0) # NEW Model 14 (YDF)\n\noof_preds_xgb_custom_new = np.empty(0) # NEW Model (custom XGB)\ntest_preds_xgb_custom_new = np.empty(0) # NEW Model (custom XGB)\n\n\n# --- Base Model 1: XGBoost (using xgb.train API - original block 1) ---\nprint(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] --- Handling Base Model 1: XGBoost (xgb.train API - Original Block 1) ---\")\nstart_time_model1 = datetime.now()\noof_preds_xgb_train_api1, test_preds_xgb_train_api1, loaded_from_disk = load_predictions_if_exist(\n    CFG.FNAME_XGB_API1_OOF, CFG.FNAME_XGB_API1_TEST, X.shape, X_test.shape, num_classes\n)\nif loaded_from_disk:\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Loaded existing predictions for Model 1.\")\nelse:\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] No existing predictions found for Model 1. This model will be zero-initialized.\")\nend_time_model1 = datetime.now()\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Model 1 Handling Finished. Elapsed: {end_time_model1 - start_time_model1} ---\\n\")\n\n\n# --- Base Model 2: XGBoost (using XGBClassifier API) ---\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Handling Base Model 2: XGBoost (XGBClassifier API) ---\")\nstart_time_model2 = datetime.now()\noof_preds_xgb_sklearn, test_preds_xgb_sklearn, loaded_from_disk = load_predictions_if_exist(\n    CFG.FNAME_XGB_SKLEARN_OOF, CFG.FNAME_XGB_SKLEARN_TEST, X.shape, X_test.shape, num_classes\n)\nif loaded_from_disk:\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Loaded existing predictions for Model 2.\")\nelse:\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] No existing predictions found for Model 2. This model will be zero-initialized.\")\nend_time_model2 = datetime.now()\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Model 2 Handling Finished. Elapsed: {end_time_model2 - start_time_model2} ---\\n\")\n\n\n# --- Base Model 3: LightGBM Model ---\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Handling Base Model 3: LightGBM ---\")\nstart_time_model3 = datetime.now()\noof_preds_lgbm, test_preds_lgbm, loaded_from_disk = load_predictions_if_exist(\n    CFG.FNAME_LGBM_OOF, CFG.FNAME_LGBM_TEST, X.shape, X_test.shape, num_classes\n)\nif loaded_from_disk:\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Loaded existing predictions for Model 3.\")\nelse:\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] No existing predictions found for Model 3. This model will be zero-initialized.\")\nend_time_model3 = datetime.now()\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Model 3 Handling Finished. Elapsed: {end_time_model3 - start_time_model3} ---\\n\")\n\n\n# --- Base Model 6: Logistic Regression ---\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Handling Base Model 6: Logistic Regression ---\")\nstart_time_model6 = datetime.now()\noof_preds_lr, test_preds_lr, loaded_from_disk = load_predictions_if_exist(\n    CFG.FNAME_LR_OOF, CFG.FNAME_LR_TEST, X.shape, X_test.shape, num_classes\n)\nif loaded_from_disk:\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Loaded existing predictions for Model 6.\")\nelse:\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] No existing predictions found for Model 6. This model will be zero-initialized.\")\nend_time_model6 = datetime.now()\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Model 6 Handling Finished. Elapsed: {end_time_model6 - start_time_model6} ---\\n\")\n\n\n# --- Base Model 7: LightGBM (Optimized Hyperparameters) ---\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Handling Base Model 7: LightGBM (Optimized Hyperparameters) ---\")\nstart_time_model7 = datetime.now()\n\noof_preds_lgbm_optimized, test_preds_lgbm_optimized, loaded_from_disk = load_predictions_if_exist(\n    CFG.FNAME_LGBM_OPTIMIZED_OOF, CFG.FNAME_LGBM_OPTIMIZED_TEST, X.shape, X_test.shape, num_classes\n)\nif loaded_from_disk:\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Loaded existing predictions for Model 7.\")\nelse:\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] No existing predictions found for Model 7. This model will be zero-initialized.\")\nend_time_model7 = datetime.now()\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Model 7 Handling Finished. Elapsed: {end_time_model7 - start_time_model7} ---\\n\")\n\n\n# --- Base Model 8: VotingClassifier ---\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Handling Base Model 8: VotingClassifier ---\")\nstart_time_model8 = datetime.now()\n\noof_preds_voting, test_preds_voting, loaded_from_disk = load_predictions_if_exist(\n    CFG.FNAME_VOTING_OOF, CFG.FNAME_VOTING_TEST, X.shape, X_test.shape, num_classes\n)\nif loaded_from_disk:\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Loaded existing predictions for Model 8.\")\nelse:\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] No existing predictions found for Model 8. This model will be zero-initialized.\")\nend_time_model8 = datetime.now()\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Model 8 Handling Finished. Elapsed: {end_time_model8 - start_time_model8} ---\\n\")\n    \n# --- Base Model 11: Gaussian Naive Bayes ---\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Handling Base Model 11: Gaussian Naive Bayes ---\")\nstart_time_model11 = datetime.now()\n\noof_preds_gaussian_nb, test_preds_gaussian_nb, loaded_from_disk = load_predictions_if_exist(\n    CFG.FNAME_GAUSSIAN_NB_OOF, CFG.FNAME_GAUSSIAN_NB_TEST, X.shape, X_test.shape, num_classes\n)\nif loaded_from_disk:\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Loaded existing predictions for Model 11.\")\nelse:\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] No existing predictions found for Model 11. This model will be zero-initialized.\")\nend_time_model11 = datetime.now()\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Model 11 Handling Finished. Elapsed: {end_time_model11 - start_time_model11} ---\\n\")\n\n\n# --- Base Model 13: HistGradientBoostingClassifier ---\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Handling Base Model 13: HistGradientBoostingClassifier ---\")\nstart_time_model13 = datetime.now()\n\noof_preds_hgb, test_preds_hgb, loaded_from_disk = load_predictions_if_exist(\n    CFG.FNAME_HGB_OOF, CFG.FNAME_HGB_TEST, X.shape, X_test.shape, num_classes\n)\nif loaded_from_disk:\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Loaded existing predictions for Model 13 (HGB).\")\nelse:\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] No existing predictions found for Model 13 (HGB). This model will be zero-initialized.\")\nend_time_model13 = datetime.now()\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Model 13 Handling Finished. Elapsed: {end_time_model13 - start_time_model13} ---\\n\")\n\n# --- Base Model 14: Yggdrasil Decision Forests (RandomForest) ---\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Handling Base Model 14: Yggdrasil Decision Forests (RandomForest) ---\")\nstart_time_model14 = datetime.now()\n\noof_preds_ydf, test_preds_ydf, loaded_from_disk = load_predictions_if_exist(\n    CFG.FNAME_YDF_OOF, CFG.FNAME_YDF_TEST, X.shape, X_test.shape, num_classes\n)\nif loaded_from_disk:\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Loaded existing predictions for Model 14 (YDF).\")\nelse:\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] No existing predictions found for Model 14 (YDF). This model will be zero-initialized.\")\nend_time_model14 = datetime.now()\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Model 14 Handling Finished. Elapsed: {end_time_model14 - start_time_model14} ---\\n\")\n\n# --- NEW Base Model: Custom XGBoost ---\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Handling NEW Base Model: Custom XGBoost ---\")\nstart_time_model_new_xgb = datetime.now()\n\noof_preds_xgb_custom_new, test_preds_xgb_custom_new, loaded_from_disk = load_predictions_if_exist(\n    CFG.FNAME_XGB_CUSTOM_NEW_OOF, CFG.FNAME_XGB_CUSTOM_NEW_TEST, X.shape, X_test.shape, num_classes\n)\n\nif loaded_from_disk:\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Loaded existing predictions for NEW Custom XGBoost Model.\")\nelse:\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Training NEW Custom XGBoost Model...\")\n    xgb_custom_new_params = {\n        'objective': 'multi:softprob', \n        'num_class': num_classes, \n        'max_depth': 7,\n        'learning_rate': 0.03,\n        'subsample': 0.8,\n        'max_bin': 128,\n        'colsample_bytree': 0.3, \n        'colsample_bylevel': 1, \n        'colsample_bynode': 1, \n        'tree_method': 'hist', \n        'random_state': 42,\n        'eval_metric': 'mlogloss',\n        'enable_categorical':True,\n        'n_estimators': 10000, # Use CFG value for consistency\n        'early_stopping_rounds': 50, # Use CFG value for consistency\n    }\n\n    kf_xgb_new = KFold(n_splits=FOLDS, shuffle=True, random_state=GLOBAL_RANDOM_STATE)\n    model_new_xgb_logloss_scores = []\n    model_new_xgb_map3_scores = [] # NEW: To store MAP@3 scores per fold\n    model_new_xgb_test_pred_sum = np.zeros((len(X_test), num_classes))\n\n    for i, (train_idx, valid_idx) in enumerate(kf_xgb_new.split(X, y_encoded)):\n        fold_start_time = datetime.now()\n        print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] {'#'*10} Training Fold {i+1}/{FOLDS} (NEW Custom XGBoost) {'#'*10}\")\n\n        x_train_fold, y_train_fold = X.iloc[train_idx].copy(), y_encoded.iloc[train_idx]\n        x_valid_fold, y_valid_fold = X.iloc[valid_idx].copy(), y_encoded.iloc[valid_idx]\n\n        dtrain = xgb.DMatrix(x_train_fold, label=y_train_fold, enable_categorical=True)\n        dvalid = xgb.DMatrix(x_valid_fold, label=y_valid_fold, enable_categorical=True)\n        dtest = xgb.DMatrix(X_test, enable_categorical=True)\n\n        ES_callback = xgb.callback.EarlyStopping(\n            rounds=xgb_custom_new_params['early_stopping_rounds'],\n            maximize=False,\n            save_best=True,\n        )\n\n        model_new_xgb_instance = xgb.train(\n            xgb_custom_new_params,\n            dtrain,\n            num_boost_round=xgb_custom_new_params['n_estimators'],\n            evals=[(dvalid, 'validation')],\n            callbacks=[ES_callback],\n            verbose_eval=CFG.verbose_eval\n        )\n\n        oof_preds_xgb_custom_new[valid_idx] = model_new_xgb_instance.predict(dvalid, iteration_range=(0, model_new_xgb_instance.best_iteration + 1))\n        model_new_xgb_test_pred_sum += model_new_xgb_instance.predict(dtest, iteration_range=(0, model_new_xgb_instance.best_iteration + 1))\n\n        log_loss_value = log_loss(y_valid_fold, oof_preds_xgb_custom_new[valid_idx])\n        model_new_xgb_logloss_scores.append(log_loss_value)\n\n        # NEW: Calculate MAP@3 for the current fold\n        top_3_oof_preds_fold = np.argsort(oof_preds_xgb_custom_new[valid_idx], axis=1)[:, -3:][:, ::-1]\n        map3_score_fold = mapk(y_valid_fold.values, top_3_oof_preds_fold)\n        model_new_xgb_map3_scores.append(map3_score_fold)\n\n        print(f\"[{datetime.now().strftime('%H:%M:%S')}] Fold {i+1} log_loss: {log_loss_value:.4f}, MAP@3: {map3_score_fold:.5f}. Elapsed for fold: {datetime.now() - fold_start_time}\")\n        \n        del model_new_xgb_instance, dtrain, dvalid, dtest\n        gc.collect()\n\n    test_preds_xgb_custom_new = model_new_xgb_test_pred_sum / FOLDS\n    avg_log_loss_xgb_new = np.mean(model_new_xgb_logloss_scores)\n    avg_map3_xgb_new = np.mean(model_new_xgb_map3_scores) # NEW: Average MAP@3\n    print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] NEW Custom XGBoost Model Final CV log_loss: {avg_log_loss_xgb_new:.4f}, Avg MAP@3: {avg_map3_xgb_new:.5f}\") # NEW: Print Avg MAP@3\n\n    np.save(os.path.join(CFG.OUTPUT_DIR, CFG.FNAME_XGB_CUSTOM_NEW_OOF), oof_preds_xgb_custom_new)\n    np.save(os.path.join(CFG.OUTPUT_DIR, CFG.FNAME_XGB_CUSTOM_NEW_TEST), test_preds_xgb_custom_new)\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Saved NEW Custom XGBoost Model predictions to {CFG.OUTPUT_DIR}\")\n\nend_time_model_new_xgb = datetime.now()\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- NEW Custom XGBoost Model Handling Finished. Elapsed: {end_time_model_new_xgb - start_time_model_new_xgb} ---\\n\")\n\n\n# --- Prepare Meta-Features for the Single-Layer Logistic Regression (10 Active Base Models) ---\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Preparing Meta-Features for Single-Layer Logistic Regression (10 Active Base Models) ---\")\nstart_time_meta_features = datetime.now()\n\nX_meta_train = np.hstack([\n    oof_preds_xgb_train_api1, # Model 1\n    oof_preds_xgb_sklearn, # Model 2\n    oof_preds_lgbm, # Model 3\n    oof_preds_lr, # Model 6\n    oof_preds_lgbm_optimized, # Model 7\n    oof_preds_voting, # Model 8\n    oof_preds_gaussian_nb, # Model 11\n    oof_preds_hgb, # Model 13\n    oof_preds_ydf, # Model 14\n    oof_preds_xgb_custom_new # NEW Custom XGBoost Model\n])\nX_meta_test = np.hstack([\n    test_preds_xgb_train_api1, # Model 1\n    test_preds_xgb_sklearn, # Model 2\n    test_preds_lgbm, # Model 3\n    test_preds_lr, # Model 6\n    test_preds_lgbm_optimized, # Model 7\n    test_preds_voting, # Model 8\n    test_preds_gaussian_nb, # Model 11\n    test_preds_hgb, # Model 13\n    test_preds_ydf, # Model 14\n    test_preds_xgb_custom_new # NEW Custom XGBoost Model\n])\n\n# Scale inputs for the final Logistic Regression meta-model\nscaler_final_meta = StandardScaler()\nX_meta_train_scaled = scaler_final_meta.fit_transform(X_meta_train)\nX_meta_test_scaled = scaler_final_meta.transform(X_meta_test)\n\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] Meta-training set shape for Logistic Regression: {X_meta_train_scaled.shape}\")\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] Meta-test set shape for Logistic Regression: {X_meta_test_scaled.shape}\")\n\nend_time_meta_features = datetime.now()\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Meta-Features for Logistic Regression Prepared. Elapsed: {end_time_meta_features - start_time_meta_features} ---\\n\")\n\n# --- NO Hill Climbing Optimization for Logistic Regression C parameter ---\n# Using a fixed C value as requested.\nFIXED_C_VALUE = 0.1\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Using a fixed C value for Logistic Regression: C={FIXED_C_VALUE} ---\")\n\n\n# --- 8. Train the Final Meta-Model (Logistic Regression) ---\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Training Final Logistic Regression Meta-Model (Single Layer, 10 Active Base Models) ---\")\nstart_time_final_model = datetime.now()\n\nfinal_meta_model = LogisticRegression(\n    solver='liblinear',\n    C=FIXED_C_VALUE, # Using the fixed C value\n    random_state=GLOBAL_RANDOM_STATE,\n    n_jobs=-1,\n    multi_class='ovr'\n)\nfinal_meta_model.fit(X_meta_train_scaled, y_encoded)\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] Final Logistic Regression Meta-Model training complete with fixed C={FIXED_C_VALUE:.6f}.\")\n\n# --- Evaluate Meta-Model's OOF MAP@3 Score ---\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Evaluating Final Meta-Model's OOF MAP@3 Score ---\")\n# Get OOF predictions from the trained meta-model\nmeta_oof_preds = final_meta_model.predict_proba(X_meta_train_scaled)\n\n# Get the top 3 predicted class indices for the OOF predictions\ntop_3_meta_oof_preds = np.argsort(meta_oof_preds, axis=1)[:, -3:][:, ::-1]\n\n# Calculate MAP@3 score using the original encoded labels\nmeta_map3_score = mapk(y_encoded.values, top_3_meta_oof_preds)\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] Final Meta-Model OOF MAP@3 Score: {meta_map3_score:.5f}\")\n\n\nend_time_final_model = datetime.now()\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Final Logistic Regression Meta-Model Training Finished. Elapsed: {end_time_final_model - start_time_final_model} ---\\n\")\n\n\n# --- 9. Generate Final Ensemble Predictions and Submission ---\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Generating Final Stacked Ensemble Predictions (Single Layer, 10 Active Base Models) ---\")\nstart_time_submission = datetime.now()\n\nfinal_ensemble_test_probs = final_meta_model.predict_proba(X_meta_test_scaled)\ntop_3_preds_ensemble = np.argsort(final_ensemble_test_probs, axis=1)[:, -3:][:, ::-1]\n\n# Inverse transform to get original fertilizer names (strings)\ntop_3_labels_ensemble = le.inverse_transform(top_3_preds_ensemble.ravel()).reshape(top_3_preds_ensemble.shape)\n\n# Create submission DataFrame\nsubmission_filename = \"submission_stacked_ensemble_10_models.csv\" # Updated filename\nsubmission_ensemble = pd.DataFrame({\n    \"id\": df_sub[\"id\"],\n    \"Fertilizer Name\": [' '.join(label for label in row) for row in top_3_labels_ensemble]\n})\n\nsubmission_ensemble.to_csv(submission_filename, index=False)\n\nprint(f\"üìÅ Final single-layer stacked ensemble submission saved to '{submission_filename}'\")\n\n# --- Display the head of the submission file (for verification) ---\nprint(\"\\nFirst 5 rows of the final submission DataFrame (for display):\")\nwith pd.option_context('display.max_colwidth', None, 'display.width', 1000):\n    print(submission_ensemble.head())\n\nend_time_submission = datetime.now()\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Final Submission Generation Finished. Elapsed: {end_time_submission - start_time_submission} ---\\n\")\n\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Single-Layer Stacked Ensemble Process Finished ---\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T03:38:42.479159Z","iopub.execute_input":"2025-06-19T03:38:42.479457Z","execution_failed":"2025-06-19T05:44:32.145Z"}},"outputs":[{"name":"stdout","text":"\n[03:38:50] --- Starting Data Loading and Initial Preprocessing ---\n[03:38:52] --- Starting Ordinal and Label Encoding ---\nHistGradientBoostingClassifier will treat columns at indices [3, 4] as categorical.\n[03:38:53] --- Data Loading and Preprocessing Finished. Elapsed: 0:00:02.718159 ---\n\n[03:38:53] --- Stacking Ensemble Setup Started ---\n\n[03:38:53] --- Handling Base Model 1: XGBoost (xgb.train API - Original Block 1) ---\nLoading predictions from secondary /kaggle/input/boosting-output/: oof_preds_xgb_train_api1.npy, test_preds_xgb_train_api1.npy\n[03:38:54] Loaded existing predictions for Model 1.\n[03:38:54] --- Model 1 Handling Finished. Elapsed: 0:00:00.353391 ---\n\n[03:38:54] --- Handling Base Model 2: XGBoost (XGBClassifier API) ---\nLoading predictions from secondary /kaggle/input/boosting-output/: oof_preds_xgb_sklearn.npy, test_preds_xgb_sklearn.npy\n[03:38:54] Loaded existing predictions for Model 2.\n[03:38:54] --- Model 2 Handling Finished. Elapsed: 0:00:00.332317 ---\n\n[03:38:54] --- Handling Base Model 3: LightGBM ---\nLoading predictions from secondary /kaggle/input/boosting-output/: oof_preds_lgbm.npy, test_preds_lgbm.npy\n[03:38:54] Loaded existing predictions for Model 3.\n[03:38:54] --- Model 3 Handling Finished. Elapsed: 0:00:00.289514 ---\n\n[03:38:54] --- Handling Base Model 6: Logistic Regression ---\nLoading predictions from primary UPLOADED_INPUT_DIR (/kaggle/input/ensemble2/): oof_preds_lr.npy, test_preds_lr.npy\n[03:38:54] Loaded existing predictions for Model 6.\n[03:38:54] --- Model 6 Handling Finished. Elapsed: 0:00:00.274119 ---\n\n[03:38:54] --- Handling Base Model 7: LightGBM (Optimized Hyperparameters) ---\nLoading predictions from primary UPLOADED_INPUT_DIR (/kaggle/input/ensemble2/): oof_preds_lgbm_optimized.npy, test_preds_lgbm_optimized.npy\n[03:38:55] Loaded existing predictions for Model 7.\n[03:38:55] --- Model 7 Handling Finished. Elapsed: 0:00:00.282784 ---\n\n[03:38:55] --- Handling Base Model 8: VotingClassifier ---\nLoading predictions from primary UPLOADED_INPUT_DIR (/kaggle/input/ensemble2/): oof_preds_voting.npy, test_preds_voting.npy\n[03:38:55] Loaded existing predictions for Model 8.\n[03:38:55] --- Model 8 Handling Finished. Elapsed: 0:00:00.287050 ---\n\n[03:38:55] --- Handling Base Model 11: Gaussian Naive Bayes ---\nLoading predictions from /kaggle/input/ensemble4/: oof_preds_gaussian_nb.npy, test_preds_gaussian_nb.npy\n[03:38:55] Loaded existing predictions for Model 11.\n[03:38:55] --- Model 11 Handling Finished. Elapsed: 0:00:00.366574 ---\n\n[03:38:55] --- Handling Base Model 13: HistGradientBoostingClassifier ---\nLoading predictions from /kaggle/input/ensemble5/: oof_preds_hgb.npy, test_preds_hgb.npy\n[03:38:56] Loaded existing predictions for Model 13 (HGB).\n[03:38:56] --- Model 13 Handling Finished. Elapsed: 0:00:00.308743 ---\n\n[03:38:56] --- Handling Base Model 14: Yggdrasil Decision Forests (RandomForest) ---\nLoading predictions from /kaggle/input/ensemble5/: oof_preds_ydf.npy, test_preds_ydf.npy\n[03:38:56] Loaded existing predictions for Model 14 (YDF).\n[03:38:56] --- Model 14 Handling Finished. Elapsed: 0:00:00.360505 ---\n\n[03:38:56] --- Handling NEW Base Model: Custom XGBoost ---\nNo existing predictions found for oof_preds_xgb_custom_new.npy or test_preds_xgb_custom_new.npy. Will initialize as zeros.\n[03:38:56] Training NEW Custom XGBoost Model...\n\n[03:38:56] ########## Training Fold 1/5 (NEW Custom XGBoost) ##########\n[0]\tvalidation-mlogloss:1.94572\n[200]\tvalidation-mlogloss:1.92987\n[400]\tvalidation-mlogloss:1.92318\n[600]\tvalidation-mlogloss:1.91857\n[1200]\tvalidation-mlogloss:1.90943\n[1400]\tvalidation-mlogloss:1.90723\n[1600]\tvalidation-mlogloss:1.90534\n[1800]\tvalidation-mlogloss:1.90375\n[2000]\tvalidation-mlogloss:1.90229\n[2200]\tvalidation-mlogloss:1.90108\n[2400]\tvalidation-mlogloss:1.90002\n[2600]\tvalidation-mlogloss:1.89913\n[2800]\tvalidation-mlogloss:1.89831\n[3000]\tvalidation-mlogloss:1.89755\n[3200]\tvalidation-mlogloss:1.89692\n[3400]\tvalidation-mlogloss:1.89637\n[3600]\tvalidation-mlogloss:1.89589\n[3800]\tvalidation-mlogloss:1.89547\n[4000]\tvalidation-mlogloss:1.89510\n[4200]\tvalidation-mlogloss:1.89480\n[4400]\tvalidation-mlogloss:1.89451\n[4600]\tvalidation-mlogloss:1.89428\n[4800]\tvalidation-mlogloss:1.89410\n[5000]\tvalidation-mlogloss:1.89393\n[5131]\tvalidation-mlogloss:1.89386\n[04:10:48] Fold 1 log_loss: 1.8939, MAP@3: 0.36256. Elapsed for fold: 0:31:51.764832\n\n[04:10:48] ########## Training Fold 2/5 (NEW Custom XGBoost) ##########\n[0]\tvalidation-mlogloss:1.94572\n[200]\tvalidation-mlogloss:1.92979\n[400]\tvalidation-mlogloss:1.92321\n[600]\tvalidation-mlogloss:1.91873\n[800]\tvalidation-mlogloss:1.91511\n[1000]\tvalidation-mlogloss:1.91222\n[1200]\tvalidation-mlogloss:1.90968\n[1400]\tvalidation-mlogloss:1.90756\n[1600]\tvalidation-mlogloss:1.90565\n[1800]\tvalidation-mlogloss:1.90405\n[2000]\tvalidation-mlogloss:1.90266\n[2200]\tvalidation-mlogloss:1.90145\n[2400]\tvalidation-mlogloss:1.90038\n[2600]\tvalidation-mlogloss:1.89950\n[2800]\tvalidation-mlogloss:1.89867\n[3000]\tvalidation-mlogloss:1.89796\n[3200]\tvalidation-mlogloss:1.89740\n[3400]\tvalidation-mlogloss:1.89687\n[3600]\tvalidation-mlogloss:1.89640\n[3800]\tvalidation-mlogloss:1.89599\n[4000]\tvalidation-mlogloss:1.89564\n[4200]\tvalidation-mlogloss:1.89535\n[4400]\tvalidation-mlogloss:1.89511\n[4600]\tvalidation-mlogloss:1.89489\n[4800]\tvalidation-mlogloss:1.89468\n[5000]\tvalidation-mlogloss:1.89453\n[5200]\tvalidation-mlogloss:1.89441\n[5400]\tvalidation-mlogloss:1.89430\n[5600]\tvalidation-mlogloss:1.89425\n[5791]\tvalidation-mlogloss:1.89419\n[04:46:34] Fold 2 log_loss: 1.8942, MAP@3: 0.36311. Elapsed for fold: 0:35:46.363821\n\n[04:46:35] ########## Training Fold 3/5 (NEW Custom XGBoost) ##########\n[0]\tvalidation-mlogloss:1.94571\n[200]\tvalidation-mlogloss:1.92974\n[400]\tvalidation-mlogloss:1.92309\n[600]\tvalidation-mlogloss:1.91855\n[800]\tvalidation-mlogloss:1.91499\n[1000]\tvalidation-mlogloss:1.91207\n[1200]\tvalidation-mlogloss:1.90947\n[1400]\tvalidation-mlogloss:1.90726\n[1600]\tvalidation-mlogloss:1.90537\n[1800]\tvalidation-mlogloss:1.90375\n[2000]\tvalidation-mlogloss:1.90230\n[2200]\tvalidation-mlogloss:1.90103\n[2400]\tvalidation-mlogloss:1.89992\n[2600]\tvalidation-mlogloss:1.89894\n[2800]\tvalidation-mlogloss:1.89812\n[3000]\tvalidation-mlogloss:1.89738\n[3600]\tvalidation-mlogloss:1.89557\n[3800]\tvalidation-mlogloss:1.89509\n[4000]\tvalidation-mlogloss:1.89470\n[4200]\tvalidation-mlogloss:1.89435\n[4400]\tvalidation-mlogloss:1.89406\n[4600]\tvalidation-mlogloss:1.89380\n[4800]\tvalidation-mlogloss:1.89358\n[5000]\tvalidation-mlogloss:1.89337\n[5200]\tvalidation-mlogloss:1.89320\n[5400]\tvalidation-mlogloss:1.89306\n[5600]\tvalidation-mlogloss:1.89293\n[5800]\tvalidation-mlogloss:1.89283\n[6000]\tvalidation-mlogloss:1.89273\n[6192]\tvalidation-mlogloss:1.89271\n[05:24:45] Fold 3 log_loss: 1.8927, MAP@3: 0.36367. Elapsed for fold: 0:38:10.666736\n\n[05:24:46] ########## Training Fold 4/5 (NEW Custom XGBoost) ##########\n[0]\tvalidation-mlogloss:1.94572\n[200]\tvalidation-mlogloss:1.92975\n[400]\tvalidation-mlogloss:1.92315\n[600]\tvalidation-mlogloss:1.91867\n[800]\tvalidation-mlogloss:1.91510\n[1000]\tvalidation-mlogloss:1.91217\n[1200]\tvalidation-mlogloss:1.90965\n[1400]\tvalidation-mlogloss:1.90750\n[1600]\tvalidation-mlogloss:1.90567\n[1800]\tvalidation-mlogloss:1.90405\n[2000]\tvalidation-mlogloss:1.90264\n[2200]\tvalidation-mlogloss:1.90144\n[2400]\tvalidation-mlogloss:1.90037\n[2600]\tvalidation-mlogloss:1.89954\n[2800]\tvalidation-mlogloss:1.89875\n[3000]\tvalidation-mlogloss:1.89804\n[3200]\tvalidation-mlogloss:1.89742\n[3400]\tvalidation-mlogloss:1.89684\n[3600]\tvalidation-mlogloss:1.89637\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nfrom datetime import datetime\nfrom sklearn.preprocessing import LabelEncoder, OrdinalEncoder # Import for data preprocessing\n\n# --- Configuration and Global Random State ---\nclass CFG:\n    seed = 42\n    target = 'Fertilizer Name'\n    n_splits = 5 # Number of folds for cross-validation\n    \n    # Output Directory for current session\n    OUTPUT_DIR = '/kaggle/working/outputs/' \n    \n    # --- Filenames for base model predictions ---\n    # Boosting-output files\n    FNAME_XGB_API1_OOF = 'oof_preds_xgb_train_api1.npy'\n    FNAME_XGB_API1_TEST = 'test_preds_xgb_train_api1.npy'\n    \n    FNAME_XGB_SKLEARN_OOF = 'oof_preds_xgb_sklearn.npy'\n    FNAME_XGB_SKLEARN_TEST = 'test_preds_xgb_sklearn.npy'\n    \n    FNAME_LGBM_OOF = 'oof_preds_lgbm.npy'\n    FNAME_LGBM_TEST = 'test_preds_lgbm.npy'\n    \n    FNAME_XGB_API2_OOF = 'oof_preds_xgb_train_api2.npy'\n    FNAME_XGB_API2_TEST = 'test_preds_xgb_train_api2.npy'\n    \n    FNAME_LGBM_GOSS_OOF = 'oof_preds_lgbm_goss.npy'\n    FNAME_LGBM_GOSS_TEST = 'test_preds_lgbm_goss.npy'\n\n    # Ensemble2 files\n    FNAME_LR_OOF = 'oof_preds_lr.npy'\n    FNAME_LR_TEST = 'test_preds_lr.npy'\n\n    FNAME_LGBM_OPTIMIZED_OOF = 'oof_preds_lgbm_optimized.npy'\n    FNAME_LGBM_OPTIMIZED_TEST = 'test_preds_lgbm_optimized.npy'\n\n    FNAME_VOTING_OOF = 'oof_preds_voting.npy'\n    FNAME_VOTING_TEST = 'test_preds_voting.npy'\n\n    # Ensemble3 files\n    FNAME_XGB_OPTIMIZED_ENSEMB3_OOF = 'oof_preds_xgb_optimized.npy' \n    FNAME_XGB_OPTIMIZED_ENSEMB3_TEST = 'test_preds_xgb_optimized.npy'\n\n    FNAME_XGB_CUSTOM_OOF = 'oof_preds_xgb_custom.npy'\n    FNAME_XGB_CUSTOM_TEST = 'test_preds_xgb_custom.npy'\n\n    # Ensemble4 files\n    FNAME_GAUSSIAN_NB_OOF = 'oof_preds_gaussian_nb.npy'\n    FNAME_GAUSSIAN_NB_TEST = 'test_preds_gaussian_nb.npy'\n\n    FNAME_LDA_OOF = 'oof_preds_lda_base.npy'\n    FNAME_LDA_TEST = 'test_preds_lda_base.npy'\n\n    # Ensemble5 files\n    FNAME_HGB_OOF = 'oof_preds_hgb.npy'\n    FNAME_HGB_TEST = 'test_preds_hgb.npy'\n\n    FNAME_YDF_OOF = 'oof_preds_ydf.npy'\n    FNAME_YDF_TEST = 'test_preds_ydf.npy'\n\n    # Ensemble6 files (New models)\n    FNAME_XGB_REPEAT_OOF = 'xgb_repeat_train_oof.npy' # Renamed from xgb_repeat_train_oof.npy for clarity\n    FNAME_XGB_REPEAT_TEST = 'xgb_repeat_test_oof.npy' # Renamed from xgb_repeat_test_oof.npy for clarity\n    FNAME_LOG_PROBA_OOF = 'pred_oof_log_proba.csv'\n    FNAME_LOG_PROBA_TEST = 'pred_test_log_proba.csv'\n\n\n# Create the output directory if it doesn't exist.\nos.makedirs(CFG.OUTPUT_DIR, exist_ok=True)\n\n# --- 2. Data Loading and Initial Preprocessing to get num_classes and X_shape ---\nprint(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] --- Starting Data Loading for shape and class info ---\")\ndf_train_raw = pd.read_csv('/kaggle/input/playground-series-s5e6/train.csv')\ndf_original_raw = pd.read_csv('/kaggle/input/original/Fertilizer Prediction .csv')\ndf_test_raw = pd.read_csv('/kaggle/input/playground-series-s5e6/test.csv')\n\n# Concatenate original dataset to the training data\ndf_train_full = pd.concat([df_train_raw.drop(columns=['id']), df_original_raw], axis=0, ignore_index=True)\nif 'id' in df_test_raw.columns:\n    df_test_raw = df_test_raw.drop(columns=['id'])\n\n# Encoding for class count\nle = LabelEncoder()\nle.fit(df_train_full[CFG.target])\nnum_classes = len(le.classes_)\nX_train_len = len(df_train_full)\nX_test_len = len(df_test_raw)\n\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] Detected number of classes: {num_classes}\")\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] Training data length: {X_train_len}\")\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] Test data length: {X_test_len}\\n\")\n\n\n# --- Helper function to check and load predictions from various sources ---\ndef load_predictions_if_exist(oof_filename, test_filename, x_len, x_test_len, n_classes):\n    \"\"\"\n    Checks if prediction files exist in the current session's OUTPUT_DIR or\n    various specified Kaggle input directories, and loads them.\n    Handles both .npy and .csv files.\n\n    Args:\n        oof_filename (str): The filename for the OOF predictions.\n        test_filename (str): The filename for the test predictions.\n        x_len (int): Expected number of rows for OOF predictions (training data size).\n        x_test_len (int): Expected number of rows for test predictions (test data size).\n        n_classes (int): Expected number of columns (classes) for predictions.\n\n    Returns:\n        tuple: (oof_preds_array, test_preds_array, loaded_from_disk_flag).\n               Returns (zero-initialized arrays, False) if files are not found.\n    \"\"\"\n    # Define potential directories in order of priority (newer/specific first)\n    search_dirs = [\n        CFG.OUTPUT_DIR,\n        '/kaggle/input/ensemble6/',\n        '/kaggle/input/ensemble5/',\n        '/kaggle/input/ensemble4/',\n        '/kaggle/input/ensemble3/',\n        '/kaggle/input/ensemble2/',\n        '/kaggle/input/boosting-output/',\n    ]\n    \n    oof_path = None\n    test_path = None\n    \n    for d in search_dirs:\n        current_oof_path = os.path.join(d, oof_filename)\n        current_test_path = os.path.join(d, test_filename)\n        if os.path.exists(current_oof_path) and os.path.exists(current_test_path):\n            oof_path = current_oof_path\n            test_path = current_test_path\n            print(f\"Found predictions in: {d} for {oof_filename}, {test_filename}\")\n            break # Found files, stop searching\n\n    if oof_path and test_path:\n        try:\n            # Handle .npy files\n            if oof_path.endswith('.npy'):\n                oof_preds = np.load(oof_path)\n            elif oof_path.endswith('.csv'):\n                oof_preds = pd.read_csv(oof_path).values # Assuming CSV is already probabilities/scores\n            else:\n                raise ValueError(f\"Unsupported OOF file format for {oof_filename}\")\n\n            if test_path.endswith('.npy'):\n                test_preds = np.load(test_path)\n            elif test_path.endswith('.csv'):\n                test_preds = pd.read_csv(test_path).values\n            else:\n                raise ValueError(f\"Unsupported test file format for {test_filename}\")\n\n            # Ensure the loaded arrays have the expected number of classes\n            if oof_preds.shape[1] != n_classes or test_preds.shape[1] != n_classes:\n                print(f\"Warning: Loaded predictions for {oof_filename} have {oof_preds.shape[1]} classes, expected {n_classes}. Reshaping or zero-initializing.\")\n                # If shapes don't match, it's safer to zero-initialize than force-reshape\n                # unless a specific logic for different number of classes is known.\n                # For this correlation task, it's better to align or skip.\n                return np.zeros((x_len, n_classes)), np.zeros((x_test_len, n_classes)), False\n\n            # Ensure the loaded arrays have the expected number of samples\n            if oof_preds.shape[0] != x_len or test_preds.shape[0] != x_test_len:\n                print(f\"Warning: Loaded predictions for {oof_filename} have {oof_preds.shape[0]} samples, expected {x_len}. Reshaping or zero-initializing.\")\n                return np.zeros((x_len, n_classes)), np.zeros((x_test_len, n_classes)), False\n\n\n            return oof_preds, test_preds, True\n        except Exception as e:\n            print(f\"Error loading {oof_filename} or {test_filename}: {e}. Initializing as zeros.\")\n            return np.zeros((x_len, n_classes)), np.zeros((x_test_len, n_classes)), False\n    else:\n        print(f\"No existing predictions found for {oof_filename} or {test_filename}. Will initialize as zeros.\")\n        return np.zeros((x_len, n_classes)), np.zeros((x_test_len, n_classes)), False\n\n\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Starting Base Model OOF Prediction Loading for Correlation ---\")\n\n# List of (OOF_filename, Test_filename, Model_Name) for all base models\nmodel_files = [\n    (CFG.FNAME_XGB_API1_OOF, CFG.FNAME_XGB_API1_TEST, 'XGB_API1'),\n    (CFG.FNAME_XGB_SKLEARN_OOF, CFG.FNAME_XGB_SKLEARN_TEST, 'XGB_SKLEARN'),\n    (CFG.FNAME_LGBM_OOF, CFG.FNAME_LGBM_TEST, 'LGBM'),\n    (CFG.FNAME_XGB_API2_OOF, CFG.FNAME_XGB_API2_TEST, 'XGB_API2'),\n    (CFG.FNAME_LGBM_GOSS_OOF, CFG.FNAME_LGBM_GOSS_TEST, 'LGBM_GOSS'),\n    (CFG.FNAME_LR_OOF, CFG.FNAME_LR_TEST, 'LR'),\n    (CFG.FNAME_LGBM_OPTIMIZED_OOF, CFG.FNAME_LGBM_OPTIMIZED_TEST, 'LGBM_OPT'),\n    (CFG.FNAME_VOTING_OOF, CFG.FNAME_VOTING_TEST, 'VOTING'),\n    (CFG.FNAME_XGB_OPTIMIZED_ENSEMB3_OOF, CFG.FNAME_XGB_OPTIMIZED_ENSEMB3_TEST, 'XGB_OPT_E3'),\n    (CFG.FNAME_XGB_CUSTOM_OOF, CFG.FNAME_XGB_CUSTOM_TEST, 'XGB_CUSTOM_E3'),\n    (CFG.FNAME_GAUSSIAN_NB_OOF, CFG.FNAME_GAUSSIAN_NB_TEST, 'GAUSSIAN_NB'),\n    (CFG.FNAME_LDA_OOF, CFG.FNAME_LDA_TEST, 'LDA'),\n    (CFG.FNAME_HGB_OOF, CFG.FNAME_HGB_TEST, 'HGB'),\n    (CFG.FNAME_YDF_OOF, CFG.FNAME_YDF_TEST, 'YDF'),\n    (CFG.FNAME_XGB_REPEAT_OOF, CFG.FNAME_XGB_REPEAT_TEST, 'XGB_REPEAT'), # New model from ensemble6\n    (CFG.FNAME_LOG_PROBA_OOF, CFG.FNAME_LOG_PROBA_TEST, 'LOG_PROBA') # New model from ensemble6\n]\n\noof_predictions_flattened = {}\n\nfor oof_fname, test_fname, model_name in model_files:\n    oof_preds, _, loaded_flag = load_predictions_if_exist(\n        oof_fname, test_fname, X_train_len, X_test_len, num_classes\n    )\n    \n    # Only proceed if predictions were actually loaded and are not entirely zero (unless they genuinely are all zeros)\n    # Check if the loaded array is mostly non-zero. A simple sum check can suffice for this purpose.\n    if loaded_flag and np.sum(oof_preds) > 1e-9: # Check for non-trivial loaded data\n        # Flatten the OOF predictions (N, num_classes) to (N * num_classes,)\n        oof_predictions_flattened[model_name] = oof_preds.flatten()\n    else:\n        print(f\"[{datetime.now().strftime('%H:%M:%S')}] WARNING: Skipping {model_name} for correlation as its OOF predictions were not found, loaded as zeros, or contain only zeros.\")\n\nif not oof_predictions_flattened:\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] No base model OOF predictions were successfully loaded or contained meaningful data. Cannot calculate correlation.\")\nelse:\n    # Ensure all arrays have the same length for correlation calculation.\n    # We take the minimum length to avoid issues if files have slight discrepancies.\n    min_len = min(len(arr) for arr in oof_predictions_flattened.values())\n    \n    correlation_df = pd.DataFrame({\n        model_name: preds[:min_len] for model_name, preds in oof_predictions_flattened.items()\n    })\n\n    # Calculate the correlation matrix\n    correlation_matrix = correlation_df.corr()\n\n    print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] --- Correlation Matrix of Base Model OOF Predictions ---\")\n    # Display the correlation matrix with 3 decimal places for readability\n    print(correlation_matrix.round(3))\n\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Correlation Analysis Finished ---\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T15:34:07.223057Z","iopub.execute_input":"2025-06-18T15:34:07.223408Z","iopub.status.idle":"2025-06-18T15:34:29.015498Z","shell.execute_reply.started":"2025-06-18T15:34:07.223382Z","shell.execute_reply":"2025-06-18T15:34:29.014591Z"}},"outputs":[{"name":"stdout","text":"\n[15:34:07] --- Starting Data Loading for shape and class info ---\n[15:34:09] Detected number of classes: 7\n[15:34:09] Training data length: 850000\n[15:34:09] Test data length: 250000\n\n[15:34:09] --- Starting Base Model OOF Prediction Loading for Correlation ---\nFound predictions in: /kaggle/input/boosting-output/ for oof_preds_xgb_train_api1.npy, test_preds_xgb_train_api1.npy\nFound predictions in: /kaggle/input/boosting-output/ for oof_preds_xgb_sklearn.npy, test_preds_xgb_sklearn.npy\nFound predictions in: /kaggle/input/boosting-output/ for oof_preds_lgbm.npy, test_preds_lgbm.npy\nFound predictions in: /kaggle/input/boosting-output/ for oof_preds_xgb_train_api2.npy, test_preds_xgb_train_api2.npy\nFound predictions in: /kaggle/input/boosting-output/ for oof_preds_lgbm_goss.npy, test_preds_lgbm_goss.npy\nFound predictions in: /kaggle/input/ensemble2/ for oof_preds_lr.npy, test_preds_lr.npy\nFound predictions in: /kaggle/input/ensemble2/ for oof_preds_lgbm_optimized.npy, test_preds_lgbm_optimized.npy\nFound predictions in: /kaggle/input/ensemble2/ for oof_preds_voting.npy, test_preds_voting.npy\nFound predictions in: /kaggle/input/ensemble3/ for oof_preds_xgb_optimized.npy, test_preds_xgb_optimized.npy\nFound predictions in: /kaggle/input/ensemble3/ for oof_preds_xgb_custom.npy, test_preds_xgb_custom.npy\nFound predictions in: /kaggle/input/ensemble4/ for oof_preds_gaussian_nb.npy, test_preds_gaussian_nb.npy\nFound predictions in: /kaggle/input/ensemble4/ for oof_preds_lda_base.npy, test_preds_lda_base.npy\nFound predictions in: /kaggle/input/ensemble5/ for oof_preds_hgb.npy, test_preds_hgb.npy\nFound predictions in: /kaggle/input/ensemble5/ for oof_preds_ydf.npy, test_preds_ydf.npy\nFound predictions in: /kaggle/input/ensemble6/ for xgb_repeat_train_oof.npy, xgb_repeat_test_oof.npy\nWarning: Loaded predictions for xgb_repeat_train_oof.npy have 750000 samples, expected 850000. Reshaping or zero-initializing.\n[15:34:12] WARNING: Skipping XGB_REPEAT for correlation as its OOF predictions were not found, loaded as zeros, or contain only zeros.\nFound predictions in: /kaggle/input/ensemble6/ for pred_oof_log_proba.csv, pred_test_log_proba.csv\nWarning: Loaded predictions for pred_oof_log_proba.csv have 7 classes, expected 7. Reshaping or zero-initializing.\n[15:34:25] WARNING: Skipping LOG_PROBA for correlation as its OOF predictions were not found, loaded as zeros, or contain only zeros.\n\n[15:34:29] --- Correlation Matrix of Base Model OOF Predictions ---\n               XGB_API1  XGB_SKLEARN   LGBM  XGB_API2  LGBM_GOSS     LR  \\\nXGB_API1          1.000        0.818  0.676     0.921      0.774  0.230   \nXGB_SKLEARN       0.818        1.000  0.848     0.893      0.911  0.231   \nLGBM              0.676        0.848  1.000     0.749      0.875  0.255   \nXGB_API2          0.921        0.893  0.749     1.000      0.863  0.231   \nLGBM_GOSS         0.774        0.911  0.875     0.863      1.000  0.239   \nLR                0.230        0.231  0.255     0.231      0.239  1.000   \nLGBM_OPT          0.734        0.875  0.825     0.805      0.866  0.245   \nVOTING            0.626        0.741  0.835     0.676      0.773  0.496   \nXGB_OPT_E3        0.736        0.933  0.903     0.810      0.910  0.242   \nXGB_CUSTOM_E3     0.828        0.977  0.819     0.899      0.888  0.233   \nGAUSSIAN_NB       0.247        0.248  0.275     0.248      0.257  0.964   \nLDA               0.236        0.236  0.262     0.236      0.245  0.976   \nHGB               0.669        0.805  0.825     0.738      0.823  0.245   \nYDF               0.222        0.233  0.261     0.229      0.248  0.555   \n\n               LGBM_OPT  VOTING  XGB_OPT_E3  XGB_CUSTOM_E3  GAUSSIAN_NB  \\\nXGB_API1          0.734   0.626       0.736          0.828        0.247   \nXGB_SKLEARN       0.875   0.741       0.933          0.977        0.248   \nLGBM              0.825   0.835       0.903          0.819        0.275   \nXGB_API2          0.805   0.676       0.810          0.899        0.248   \nLGBM_GOSS         0.866   0.773       0.910          0.888        0.257   \nLR                0.245   0.496       0.242          0.233        0.964   \nLGBM_OPT          1.000   0.771       0.869          0.849        0.264   \nVOTING            0.771   1.000       0.796          0.712        0.526   \nXGB_OPT_E3        0.869   0.796       1.000          0.910        0.260   \nXGB_CUSTOM_E3     0.849   0.712       0.910          1.000        0.250   \nGAUSSIAN_NB       0.264   0.526       0.260          0.250        1.000   \nLDA               0.251   0.508       0.248          0.238        0.988   \nHGB               0.800   0.780       0.835          0.773        0.264   \nYDF               0.250   0.427       0.248          0.230        0.588   \n\n                 LDA    HGB    YDF  \nXGB_API1       0.236  0.669  0.222  \nXGB_SKLEARN    0.236  0.805  0.233  \nLGBM           0.262  0.825  0.261  \nXGB_API2       0.236  0.738  0.229  \nLGBM_GOSS      0.245  0.823  0.248  \nLR             0.976  0.245  0.555  \nLGBM_OPT       0.251  0.800  0.250  \nVOTING         0.508  0.780  0.427  \nXGB_OPT_E3     0.248  0.835  0.248  \nXGB_CUSTOM_E3  0.238  0.773  0.230  \nGAUSSIAN_NB    0.988  0.264  0.588  \nLDA            1.000  0.251  0.566  \nHGB            0.251  1.000  0.255  \nYDF            0.566  0.255  1.000  \n[15:34:29] --- Correlation Analysis Finished ---\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nfrom datetime import datetime # Import datetime for timestamps\nimport optuna\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.preprocessing import OrdinalEncoder, LabelEncoder, StandardScaler\nfrom sklearn.metrics import log_loss\nimport warnings\nimport gc\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.naive_bayes import GaussianNB # Import Gaussian Naive Bayes\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis # Import Linear Discriminant Analysis\n# Removed: from catboost import CatBoostClassifier\n# Removed: import torch # No longer needed without CatBoost's GPU check\nfrom sklearn.ensemble import HistGradientBoostingClassifier # Import HistGradientBoostingClassifier\nimport ydf # NEW: Import Yggdrasil Decision Forests\n\n\n# Suppress warnings for cleaner output\nwarnings.filterwarnings('ignore')\n\n# --- 1. Configuration and Global Random State ---\nclass CFG:\n    seed = 42\n    target = 'Fertilizer Name'\n    n_splits = 5 # Number of folds for cross-validation\n    learning_rate = 0.03\n    num_boost_round = 5000\n    early_stopping_rounds = 50\n    verbose_eval = 200\n\n    # --- Output Directory for current session (will be cleared on session end) ---\n    OUTPUT_DIR = '/kaggle/working/outputs/'\n    \n    # --- Directory for User Uploaded Input Files (e.g., from a Kaggle Dataset) ---\n    # Set to /kaggle/input/ensemble2/ as the primary uploaded input dir for new models\n    UPLOADED_INPUT_DIR = '/kaggle/input/ensemble2/' \n\n    # Filenames for base model predictions\n    FNAME_XGB_API1_OOF = 'oof_preds_xgb_train_api1.npy'\n    FNAME_XGB_API1_TEST = 'test_preds_xgb_train_api1.npy'\n    \n    FNAME_XGB_SKLEARN_OOF = 'oof_preds_xgb_sklearn.npy'\n    FNAME_XGB_SKLEARN_TEST = 'test_preds_xgb_sklearn.npy'\n    \n    FNAME_LGBM_OOF = 'oof_preds_lgbm.npy'\n    FNAME_LGBM_TEST = 'test_preds_lgbm.npy'\n    \n    # Updated filenames for Model 4\n    FNAME_XGB_API2_OOF = 'oof_preds_xgb_train_api2.npy'\n    FNAME_XGB_API2_TEST = 'test_preds_xgb_train_api2.npy'\n    \n    FNAME_LGBM_GOSS_OOF = 'oof_preds_lgbm_goss.npy'\n    FNAME_LGBM_GOSS_TEST = 'test_preds_lgbm_goss.npy'\n\n    FNAME_LR_OOF = 'oof_preds_lr.npy'\n    FNAME_LR_TEST = 'test_preds_lr.npy'\n\n    FNAME_LGBM_OPTIMIZED_OOF = 'oof_preds_lgbm_optimized.npy'\n    FNAME_LGBM_OPTIMIZED_TEST = 'test_preds_lgbm_optimized.npy'\n\n    FNAME_VOTING_OOF = 'oof_preds_voting.npy'\n    FNAME_VOTING_TEST = 'test_preds_voting.npy'\n\n    # --- Filenames for the XGBoost models from /kaggle/input/ensemble3/ ---\n    FNAME_XGB_OPTIMIZED_ENSEMBLE3_OOF = 'oof_preds_xgb_optimized.npy' \n    FNAME_XGB_OPTIMIZED_ENSEMBLE3_TEST = 'test_preds_xgb_optimized.npy'\n\n    FNAME_XGB_CUSTOM_OOF = 'oof_preds_xgb_custom.npy'\n    FNAME_XGB_CUSTOM_TEST = 'test_preds_xgb_custom.npy'\n\n    # --- Filenames for the Gaussian Naive Bayes Model ---\n    FNAME_GAUSSIAN_NB_OOF = 'oof_preds_gaussian_nb.npy'\n    FNAME_GAUSSIAN_NB_TEST = 'test_preds_gaussian_nb.npy'\n\n    # --- Filenames for the LDA Base Model ---\n    FNAME_LDA_OOF = 'oof_preds_lda_base.npy'\n    FNAME_LDA_TEST = 'test_preds_lda_base.npy'\n\n    # NEW: Filenames for HistGradientBoostingClassifier Model\n    FNAME_HGB_OOF = 'oof_preds_hgb.npy'\n    FNAME_HGB_TEST = 'test_preds_hgb.npy'\n\n    # NEW: Filenames for YDF Model (Model 14)\n    FNAME_YDF_OOF = 'oof_preds_ydf.npy'\n    FNAME_YDF_TEST = 'test_preds_ydf.npy'\n\n\n# Create the output directory if it doesn't exist. This is essential for saving.\nos.makedirs(CFG.OUTPUT_DIR, exist_ok=True)\n\n# Define FOLDS based on CFG for consistency\nFOLDS = CFG.n_splits\nGLOBAL_RANDOM_STATE = CFG.seed\nnp.random.seed(GLOBAL_RANDOM_STATE)\n\n# --- Helper function to check and load predictions from either source ---\ndef load_predictions_if_exist(oof_filename, test_filename, X_shape, X_test_shape, num_classes):\n    \"\"\"\n    Checks if prediction files exist in the current session's OUTPUT_DIR,\n    the primary UPLOADED_INPUT_DIR (CFG.UPLOADED_INPUT_DIR),\n    the /kaggle/input/ensemble3/ directory,\n    the /kaggle/input/ensemble4/ directory (NEW), or\n    the secondary /kaggle/input/boosting-output/ directory, and loads them.\n    If files are not found, it returns zero-initialized NumPy arrays of the correct shape.\n\n    Args:\n        oof_filename (str): The filename for the OOF predictions.\n        test_filename (str): The filename for the test predictions.\n        X_shape (tuple): Shape of the full training features (e.g., (num_samples, num_features)).\n        X_test_shape (tuple): Shape of the full test features (e.g., (num_test_samples, num_features)).\n        num_classes (int): Number of target classes.\n\n    Returns:\n        tuple: (oof_preds_array, test_preds_array, loaded_from_disk_flag).\n               Returns (zero_initialized_oof_array, zero_initialized_test_array, False)\n               if files are not found.\n    \"\"\"\n    oof_path_output = os.path.join(CFG.OUTPUT_DIR, oof_filename)\n    test_path_output = os.path.join(CFG.OUTPUT_DIR, test_filename)\n    \n    uploaded_input_dir_exists = hasattr(CFG, 'UPLOADED_INPUT_DIR') and CFG.UPLOADED_INPUT_DIR\n    \n    oof_path_primary_input = None\n    test_path_primary_input = None\n    if uploaded_input_dir_exists:\n        oof_path_primary_input = os.path.join(CFG.UPLOADED_INPUT_DIR, oof_filename) # /kaggle/input/ensemble2/\n        test_path_primary_input = os.path.join(CFG.UPLOADED_INPUT_DIR, test_filename)\n\n    # Specific path for /kaggle/input/ensemble3/\n    oof_path_ensemble3_input = os.path.join('/kaggle/input/ensemble3/', oof_filename)\n    test_path_ensemble3_input = os.path.join('/kaggle/input/ensemble3/', test_filename)\n\n    # NEW: Specific path for /kaggle/input/ensemble4/\n    oof_path_ensemble4_input = os.path.join('/kaggle/input/ensemble4/', oof_filename)\n    test_path_ensemble4_input = os.path.join('/kaggle/input/ensemble4/', test_filename)\n\n    # Original secondary input directory for boosting-output files\n    oof_path_boosting_output_input = os.path.join('/kaggle/input/boosting-output/', oof_filename)\n    test_path_boosting_output_input = os.path.join('/kaggle/input/boosting-output/', test_filename)\n\n\n    # Priority 1: Check in the current session's output directory\n    if os.path.exists(oof_path_output) and os.path.exists(test_path_output):\n        print(f\"Loading predictions from current session's OUTPUT_DIR: {oof_filename}, {test_filename}\")\n        return np.load(oof_path_output), np.load(test_path_output), True\n    # Priority 2: Check in the primary user-uploaded input directory (e.g., /kaggle/input/ensemble2/)\n    elif uploaded_input_dir_exists and os.path.exists(oof_path_primary_input) and os.path.exists(test_path_primary_input):\n        print(f\"Loading predictions from primary UPLOADED_INPUT_DIR ({CFG.UPLOADED_INPUT_DIR}): {oof_filename}, {test_filename}\")\n        return np.load(oof_path_primary_input), np.load(test_path_primary_input), True\n    # Priority 3: Check in the /kaggle/input/ensemble3/ directory\n    elif os.path.exists(oof_path_ensemble3_input) and os.path.exists(test_path_ensemble3_input):\n        print(f\"Loading predictions from /kaggle/input/ensemble3/: {oof_filename}, {test_filename}\")\n        return np.load(oof_path_ensemble3_input), np.load(test_path_ensemble3_input), True\n    # NEW Priority 4: Check in the /kaggle/input/ensemble4/ directory\n    elif os.path.exists(oof_path_ensemble4_input) and os.path.exists(test_path_ensemble4_input):\n        print(f\"Loading predictions from /kaggle/input/ensemble4/: {oof_filename}, {test_filename}\")\n        return np.load(oof_path_ensemble4_input), np.load(test_path_ensemble4_input), True\n    # Priority 5: Check in the secondary user-uploaded input directory (e.g., /kaggle/input/boosting-output/)\n    elif os.path.exists(oof_path_boosting_output_input) and os.path.exists(test_path_boosting_output_input):\n        print(f\"Loading predictions from secondary /kaggle/input/boosting-output/: {oof_filename}, {test_filename}\")\n        return np.load(oof_path_boosting_output_input), np.load(test_path_boosting_output_input), True\n    else:\n        # If files are not found, return zero-initialized arrays\n        print(f\"No existing predictions found for {oof_filename} or {test_filename}. Will initialize as zeros.\")\n        return np.zeros((X_shape[0], num_classes)), np.zeros((X_test_shape[0], num_classes)), False\n\n\n# --- 2. Data Loading and Initial Preprocessing ---\nprint(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] --- Starting Data Loading and Initial Preprocessing ---\")\nstart_time_data_load = datetime.now()\n\n# Ensure these paths are correct for your environment\ndf_train = pd.read_csv('/kaggle/input/playground-series-s5e6/train.csv')\ndf_test = pd.read_csv('/kaggle/input/playground-series-s5e6/test.csv')\ndf_sub = pd.read_csv('/kaggle/input/playground-series-s5e6/sample_submission.csv')\ndf_original = pd.read_csv('/kaggle/input/original/Fertilizer Prediction .csv')\n\n# Drop 'id' columns if they exist in train/test sets as per original notebook\ndf_train = df_train.drop(columns=['id'])\nif 'id' in df_test.columns:\n    df_test = df_test.drop(columns=['id'])\n\n# Concatenate original dataset to the training data\ndf_train = pd.concat([df_train, df_original], axis=0, ignore_index=True)\n\n# --- 3. Ordinal and Label Encoding ---\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Starting Ordinal and Label Encoding ---\")\ncat_cols_for_ordinal = df_train.select_dtypes(include='object').columns.tolist()\nif 'Fertilizer Name' in cat_cols_for_ordinal:\n    cat_cols_for_ordinal.remove('Fertilizer Name')\n\nordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\ndf_train[cat_cols_for_ordinal] = ordinal_encoder.fit_transform(df_train[cat_cols_for_ordinal].astype(str)).astype(int)\n\ncat_cols_for_test = [col for col in cat_cols_for_ordinal if col in df_test.columns]\ndf_test[cat_cols_for_test] = ordinal_encoder.transform(df_test[cat_cols_for_test].astype(str)).astype(int)\n\nle = LabelEncoder()\ndf_train['Fertilizer Name'] = le.fit_transform(df_train['Fertilizer Name'])\nnum_classes = len(np.unique(df_train['Fertilizer Name']))\n\ny_encoded = df_train['Fertilizer Name'] # Target for training\nX = df_train.drop(columns=['Fertilizer Name']) # Features for training\nX_test = df_test # Features for final test prediction\n\n# Define numerical columns for scaling\n# Moved this definition here to ensure it's available globally before any model training or helper functions use it.\nnumerical_cols = X.select_dtypes(include=np.number).columns.tolist()\n# Filter out any columns that might have been ordinal encoded but are numeric-like\nnumerical_cols = [col for col in numerical_cols if col not in cat_cols_for_ordinal]\n\n# Get indices of categorical features for HistGradientBoostingClassifier\n# Since we ordinal encoded them to integers, we need to explicitly tell HGB which ones are categorical\ncategorical_feature_indices_hgb = [X.columns.get_loc(col) for col in cat_cols_for_ordinal]\nprint(f\"HistGradientBoostingClassifier will treat columns at indices {categorical_feature_indices_hgb} as categorical.\")\n\nend_time_data_load = datetime.now()\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Data Loading and Preprocessing Finished. Elapsed: {end_time_data_load - start_time_data_load} ---\\n\")\n\n# Define mapk function\ndef mapk(actual, predicted, k=3):\n    def apk(a, p, k):\n        p = p[:k]\n        score = 0.0\n        hits = 0\n        seen = set()\n        for i, pred in enumerate(p):\n            if pred in a and pred not in seen:\n                hits += 1\n                score += hits / (i + 1.0)\n                seen.add(pred)\n            if hits == k: # Optimized: if we have found 'k' items, no need to continue\n                break\n        return score / min(len(a), k) if min(len(a), k) > 0 else 0.0 # Return 0.0 if actual has 0 elements to avoid division by zero\n\n    if not isinstance(actual[0], (list, np.ndarray)):\n        actual = [[a] for a in actual]\n\n    return np.mean([apk(a, p, k) for a, p in zip(actual, predicted)])\n\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Stacking Ensemble Setup Started ---\")\n\n# --- Initialize OOF and Test Prediction Arrays for Each Base Model (14 Models Now) ---\n# These variables will be assigned directly by load_predictions_if_exist,\n# which now guarantees returning NumPy arrays.\noof_preds_xgb_train_api1 = np.empty(0)\ntest_preds_xgb_train_api1 = np.empty(0)\n\noof_preds_xgb_sklearn = np.empty(0)\ntest_preds_xgb_sklearn = np.empty(0)\n\noof_preds_lgbm = np.empty(0)\ntest_preds_lgbm = np.empty(0)\n\noof_preds_xgb_api2 = np.empty(0)\ntest_preds_xgb_api2 = np.empty(0)\n\noof_preds_lgbm_goss = np.empty(0)\ntest_preds_lgbm_goss = np.empty(0)\n\noof_preds_lr = np.empty(0)\ntest_preds_lr = np.empty(0)\n\noof_preds_lgbm_optimized = np.empty(0) # Model 7\ntest_preds_lgbm_optimized = np.empty(0) # Model 7\n\noof_preds_voting = np.empty(0)          # Model 8\ntest_preds_voting = np.empty(0)         # Model 8\n\noof_preds_xgb_optimized_ensemble3 = np.empty(0) # Model 9 (from ensemble3 optimized)\ntest_preds_xgb_optimized_ensemble3 = np.empty(0) # Model 9 (from ensemble3 optimized)\n\noof_preds_xgb_custom = np.empty(0) # Model 10 (from ensemble3 custom)\ntest_preds_xgb_custom = np.empty(0) # Model 10 (from ensemble3 custom)\n\noof_preds_gaussian_nb = np.empty(0) # Model 11 (Gaussian Naive Bayes)\ntest_preds_gaussian_nb = np.empty(0) # Model 11 (Gaussian Naive Bayes)\n\noof_preds_lda_base = np.empty(0) # Model 12 (LDA)\ntest_preds_lda_base = np.empty(0) # Model 12 (LDA)\n\noof_preds_hgb = np.empty(0) # Model 13 (HistGradientBoostingClassifier)\ntest_preds_hgb = np.empty(0) # Model 13 (HistGradientBoostingClassifier)\n\noof_preds_ydf = np.empty(0) # NEW Model 14 (YDF)\ntest_preds_ydf = np.empty(0) # NEW Model 14 (YDF)\n\n\n# --- Base Model 1: XGBoost (using xgb.train API - original block 1) ---\nprint(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] --- Training Base Model 1: XGBoost (xgb.train API - Original Block 1) ---\")\nstart_time_model1 = datetime.now()\noof_preds_xgb_train_api1, test_preds_xgb_train_api1, loaded_from_disk = load_predictions_if_exist(\n    CFG.FNAME_XGB_API1_OOF, CFG.FNAME_XGB_API1_TEST, X.shape, X_test.shape, num_classes\n)\nif loaded_from_disk:\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Skipping training for Model 1.\")\nelse:\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] No existing predictions found for Model 1. Starting training.\")\n    # oof_preds_xgb_train_api1 and test_preds_xgb_train_api1 are already zero-initialized from load_predictions_if_exist\n    kf_model1 = KFold(n_splits=FOLDS, shuffle=True, random_state=42)\n    model1_logloss_scores = []\n    model1_test_pred_sum = np.zeros((len(X_test), num_classes))\n\n    for i, (train_idx, valid_idx) in enumerate(kf_model1.split(X, y_encoded)):\n        print(f\"\\n{'#'*10} Fold {i+1} (Model 1) {'#'*10}\")\n\n        x_train_fold, y_train_fold = X.iloc[train_idx].copy(), y_encoded.iloc[train_idx]\n        x_valid_fold, y_valid_fold = X.iloc[valid_idx].copy(), y_encoded.iloc[valid_idx]\n\n        dtrain = xgb.DMatrix(x_train_fold, label=y_train_fold)\n        dvalid = xgb.DMatrix(x_valid_fold, label=y_valid_fold)\n        dtest = xgb.DMatrix(X_test)\n\n        params_model1 = {\n            'objective': 'multi:softprob',\n            'num_class': num_classes,\n            'max_depth': 16,\n            'learning_rate': 0.03,\n            'min_child_weight': 2,\n            'alpha': 0.8,\n            'reg_lambda': 4.0,\n            'colsample_bytree': 0.3,\n            'subsample': 0.8,\n            'max_bin': 128,\n            'colsample_bylevel': 1,\n            'colsample_bynode': 1,\n            'tree_method': 'hist',\n            'random_state': 42,\n            'eval_metric': 'mlogloss'\n        }\n\n        model1_instance = xgb.train(\n            params_model1,\n            dtrain,\n            num_boost_round=CFG.num_boost_round, # Use CFG value\n            evals=[(dvalid, 'valid')],\n            early_stopping_rounds=CFG.early_stopping_rounds, # Use CFG value\n            verbose_eval=CFG.verbose_eval\n        )\n\n        oof_preds_xgb_train_api1[valid_idx] = model1_instance.predict(dvalid, iteration_range=(0, model1_instance.best_iteration + 1))\n        model1_test_pred_sum += model1_instance.predict(dtest, iteration_range=(0, model1_instance.best_iteration + 1))\n\n        log_loss_value = log_loss(y_valid_fold, oof_preds_xgb_train_api1[valid_idx])\n        print(f\"Fold {i+1} log_loss: {log_loss_value:.4f}\")\n        model1_logloss_scores.append(log_loss_value)\n\n        del model1_instance, dtrain, dvalid, dtest\n        gc.collect()\n\n    test_preds_xgb_train_api1 = model1_test_pred_sum / FOLDS\n    avg_log_loss_model1 = np.mean(model1_logloss_scores)\n    print(f\"\\nModel 1 (xgb.train API 1) Final CV log_loss: {avg_log_loss_model1:.4f}\")\n    top_3_oof_preds_model1 = np.argsort(oof_preds_xgb_train_api1, axis=1)[:, -3:][:, ::-1]\n    map3_score_model1 = mapk(y_encoded.values, top_3_oof_preds_model1)\n    print(f\"Model 1 (xgb.train API 1) OOF MAP@3 Score: {map3_score_model1:.5f}\")\n\n    np.save(os.path.join(CFG.OUTPUT_DIR, CFG.FNAME_XGB_API1_OOF), oof_preds_xgb_train_api1)\n    np.save(os.path.join(CFG.OUTPUT_DIR, CFG.FNAME_XGB_API1_TEST), test_preds_xgb_train_api1)\n    print(f\"Saved Model 1 predictions to {CFG.OUTPUT_DIR}\")\n\n\n# --- Base Model 2: XGBoost (using XGBClassifier API) ---\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Training Base Model 2: XGBoost (XGBClassifier API) ---\")\nstart_time_model2 = datetime.now()\noof_preds_xgb_sklearn, test_preds_xgb_sklearn, loaded_from_disk = load_predictions_if_exist(\n    CFG.FNAME_XGB_SKLEARN_OOF, CFG.FNAME_XGB_SKLEARN_TEST, X.shape, X_test.shape, num_classes\n)\nif loaded_from_disk:\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Skipping training for Model 2.\")\nelse:\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] No existing predictions found for Model 2. Starting training.\")\n    # oof_preds_xgb_sklearn and test_preds_xgb_sklearn are already zero-initialized from load_predictions_if_exist\n    fixed_xgb_params_model2 = {\n        'max_depth': 12,\n        'colsample_bytree': 0.467,\n        'subsample': 0.86,\n        'n_estimators': 4000,\n        'learning_rate': 0.03,\n        'gamma': 0.26,\n        'max_delta_step': 4,\n        'reg_alpha': 2.7,\n        'reg_lambda': 1.4,\n        'objective': 'multi:softprob',\n        'random_state': 13,\n        'enable_categorical': True,\n        'tree_method': 'hist',\n        'early_stopping_rounds': 100,\n        'eval_metric': 'mlogloss',\n        'num_class': num_classes,\n        'n_jobs': -1\n    }\n    n_splits_cv_model2 = FOLDS # Use CFG.n_splits\n    model2_fold_scores = []\n    model2_test_pred_sum = np.zeros((len(X_test), num_classes))\n    kf_model2 = StratifiedKFold(n_splits=n_splits_cv_model2, shuffle=True, random_state=GLOBAL_RANDOM_STATE)\n\n    for fold, (train_idx, valid_idx) in enumerate(kf_model2.split(X, y_encoded)):\n        print(f\"--- Training Fold {fold+1} (Model 2) ---\")\n        fold_train_X, fold_valid_X = X.iloc[train_idx], X.iloc[valid_idx]\n        fold_train_y, fold_valid_y = y_encoded.iloc[train_idx], y_encoded.iloc[valid_idx]\n\n        model2_instance = XGBClassifier(**fixed_xgb_params_model2)\n        model2_instance.fit(fold_train_X, fold_train_y,\n                            eval_set=[(fold_valid_X, fold_valid_y)],\n                            verbose=False)\n\n        best_iteration = model2_instance.best_iteration if hasattr(model2_instance, 'best_iteration') else fixed_xgb_params_model2['n_estimators']\n        print(f\"Fold {fold+1} best iteration: {best_iteration}\")\n\n        oof_preds_xgb_sklearn[valid_idx] = model2_instance.predict_proba(fold_valid_X, iteration_range=(0, best_iteration))\n        model2_test_pred_sum += model2_instance.predict_proba(X_test, iteration_range=(0, best_iteration))\n\n        top_3_preds = np.argsort(oof_preds_xgb_sklearn[valid_idx], axis=1)[:, -3:][:, ::-1]\n        fold_score = mapk(fold_valid_y.values, top_3_preds)\n        model2_fold_scores.append(fold_score)\n        print(f\"Fold {fold+1} MAP@3: {fold_score:.4f}\")\n\n        del model2_instance\n        gc.collect()\n\n    test_preds_xgb_sklearn = model2_test_pred_sum / n_splits_cv_model2\n    avg_cv_score_model2 = np.mean(model2_fold_scores)\n    print(f\"\\nModel 2 (XGBClassifier) Average MAP@3 across {n_splits_cv_model2} folds: {avg_cv_score_model2:.4f}\")\n\n    np.save(os.path.join(CFG.OUTPUT_DIR, CFG.FNAME_XGB_SKLEARN_OOF), oof_preds_xgb_sklearn)\n    np.save(os.path.join(CFG.OUTPUT_DIR, CFG.FNAME_XGB_SKLEARN_TEST), test_preds_xgb_sklearn)\n    print(f\"Saved Model 2 predictions to {CFG.OUTPUT_DIR}\")\n\n# --- Base Model 3: LightGBM Model ---\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Training Base Model 3: LightGBM ---\")\nstart_time_model3 = datetime.now()\noof_preds_lgbm, test_preds_lgbm, loaded_from_disk = load_predictions_if_exist(\n    CFG.FNAME_LGBM_OOF, CFG.FNAME_LGBM_TEST, X.shape, X_test.shape, num_classes\n)\nif loaded_from_disk:\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Skipping training for Model 3.\")\nelse:\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] No existing predictions found for Model 3. Starting training.\")\n    # oof_preds_lgbm and test_preds_lgbm are already zero-initialized from load_predictions_if_exist\n    fixed_lgbm_params = {\n        \"objective\": \"multiclass\",\n        \"num_class\": num_classes,\n        \"metric\": \"multi_logloss\",\n        \"boosting_type\": \"gbdt\",\n        \"n_estimators\": 1214,\n        \"learning_rate\": 0.064080948,\n        \"num_leaves\": 169,\n        \"max_depth\": 10,\n        \"subsample\": 0.642,\n        \"min_child_samples\": 19,\n        \"colsample_bytree\": 0.7,\n        \"reg_alpha\": 6.2941,\n        \"reg_lambda\": 5.556,\n        \"random_state\": GLOBAL_RANDOM_STATE,\n        \"n_jobs\": -1,\n        \"verbose\": -1,\n    }\n    n_splits_cv_model3 = FOLDS # Use CFG.n_splits\n    early_stopping_rounds_cv_model3 = 100\n    model3_fold_scores = []\n    model3_test_pred_sum = np.zeros((len(X_test), num_classes))\n    kf_model3 = StratifiedKFold(n_splits=n_splits_cv_model3, shuffle=True, random_state=GLOBAL_RANDOM_STATE)\n\n    for fold, (train_idx, valid_idx) in enumerate(kf_model3.split(X, y_encoded)):\n        print(f\"--- Training Fold {fold+1} (Model 3) ---\")\n        fold_train_X, fold_valid_X = X.iloc[train_idx], X.iloc[valid_idx]\n        fold_train_y, fold_valid_y = y_encoded.iloc[train_idx], y_encoded.iloc[valid_idx]\n\n        model3_instance = lgb.LGBMClassifier(**fixed_lgbm_params)\n\n        model3_instance.fit(fold_train_X, fold_train_y,\n                            eval_set=[(fold_valid_X, fold_valid_y)],\n                            callbacks=[lgb.early_stopping(early_stopping_rounds_cv_model3, verbose=False)])\n\n        best_iteration = model3_instance.best_iteration_ if hasattr(model3_instance, 'best_iteration_') else fixed_lgbm_params['n_estimators']\n        print(f\"Fold {fold+1} best iteration: {best_iteration}\")\n\n        oof_preds_lgbm[valid_idx] = model3_instance.predict_proba(fold_valid_X)\n        model3_test_pred_sum += model3_instance.predict_proba(X_test)\n\n        top_3_preds = np.argsort(oof_preds_lgbm[valid_idx], axis=1)[:, -3:][:, ::-1]\n        fold_score = mapk(fold_valid_y.values, top_3_preds)\n        model3_fold_scores.append(fold_score)\n        print(f\"Fold {fold+1} MAP@3: {fold_score:.4f}\")\n\n        del model3_instance\n        gc.collect()\n\n    test_preds_lgbm = model3_test_pred_sum / n_splits_cv_model3\n    avg_cv_score_model3 = np.mean(model3_fold_scores)\n    print(f\"\\nModel 3 (LightGBM) Average MAP@3 across {n_splits_cv_model3} folds: {avg_cv_score_model3:.4f}\")\n\n    np.save(os.path.join(CFG.OUTPUT_DIR, CFG.FNAME_LGBM_OOF), oof_preds_lgbm)\n    np.save(os.path.join(CFG.OUTPUT_DIR, CFG.FNAME_LGBM_TEST), test_preds_lgbm)\n    print(f\"Saved Model 3 predictions to {CFG.OUTPUT_DIR}\")\n\n# --- Base Model 4: XGBoost (using xgb.train API with CFG parameters) ---\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Training Base Model 4: XGBoost (xgb.train API with CFG parameters) ---\")\nstart_time_model4 = datetime.now()\noof_preds_xgb_api2, test_preds_xgb_api2, loaded_from_disk = load_predictions_if_exist(\n    CFG.FNAME_XGB_API2_OOF, CFG.FNAME_XGB_API2_TEST, X.shape, X_test.shape, num_classes\n)\nif loaded_from_disk:\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Skipping training for Model 4.\")\nelse:\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] No existing predictions found for Model 4. Starting training.\")\n    # oof_preds_xgb_api2 and test_preds_xgb_api2 are already zero-initialized from load_predictions_if_exist\n    params_model4 = {\n        'objective': 'multi:softprob',\n        'num_class': num_classes,\n        'seed': CFG.seed,\n        'max_depth': 32,\n        'learning_rate': CFG.learning_rate,\n        'min_child_weight': 2,\n        'alpha': 5.6,\n        'reg_lambda': 0.06,\n        'subsample': 0.8,\n        'colsample_bytree': 0.3,\n        'colsample_bylevel': 1,\n        'colsample_bynode': 1,\n        'tree_method': 'hist',\n        'device': \"cuda\" if os.environ.get('ACCELERATOR_TYPE') == 'GPU' else 'cpu'\n    }\n    model4_fold_scores = []\n    model4_test_pred_sum = np.zeros((len(X_test), num_classes))\n    kf_model4 = StratifiedKFold(CFG.n_splits, shuffle=True, random_state=CFG.seed)\n\n    for fold, (trn_idx, val_idx) in enumerate(kf_model4.split(X, y_encoded)):\n        X_train_fold = X.iloc[trn_idx]\n        y_train_fold = y_encoded.iloc[trn_idx]\n        X_valid_fold = X.iloc[val_idx]\n        y_valid_fold = y_encoded.iloc[val_idx]\n\n        dtrain = xgb.DMatrix(X_train_fold, label=y_train_fold, enable_categorical=True)\n        dvalid = xgb.DMatrix(X_valid_fold, label=y_valid_fold, enable_categorical=True)\n        dtest = xgb.DMatrix(X_test, enable_categorical=True)\n\n        ES = xgb.callback.EarlyStopping(\n            rounds=CFG.early_stopping_rounds,\n            maximize=False,\n            save_best=True,\n        )\n\n        model4_instance = xgb.train(\n            params_model4,\n            dtrain,\n            num_boost_round=CFG.num_boost_round,\n            evals=[(dtrain, 'train'), (dvalid, 'validation')],\n            verbose_eval=CFG.verbose_eval,\n            callbacks=[ES]\n        )\n\n        oof_preds_xgb_api2[val_idx] = model4_instance.predict(dvalid, iteration_range=(0, model4_instance.best_iteration + 1))\n        model4_test_pred_sum += model4_instance.predict(dtest, iteration_range=(0, model4_instance.best_iteration + 1))\n\n        top3_preds = np.argsort(oof_preds_xgb_api2[val_idx], axis=1)[:, -3:][:, ::-1]\n        actual = [[label] for label in y_valid_fold.values]\n        map3_score_fold = mapk(actual, top3_preds)\n        model4_fold_scores.append(map3_score_fold)\n        print(\"----------------------------------------------------------------\")\n        print(f\"fold: {fold:02d}, map@3: {map3_score_fold:.6f}, best iteration: {model4_instance.best_iteration}, best score: {model4_instance.best_score: .6f}\\n\")\n\n        del model4_instance, dtrain, dvalid, dtest\n        gc.collect()\n\n    test_preds_xgb_api2 = model4_test_pred_sum / CFG.n_splits\n    avg_map3_score_model4 = np.mean(model4_fold_scores)\n    print(\"----------------------------------------------------------------\")\n    print(f\"Model 4 (xgb.train CFG) Average MAP@3: {avg_map3_score_model4:.6f}\")\n\n    np.save(os.path.join(CFG.OUTPUT_DIR, CFG.FNAME_XGB_API2_OOF), oof_preds_xgb_api2)\n    np.save(os.path.join(CFG.OUTPUT_DIR, CFG.FNAME_XGB_API2_TEST), test_preds_xgb_api2)\n    print(f\"Saved Model 4 predictions to {CFG.OUTPUT_DIR}\")\n\n# --- Base Model 5: LightGBM (GOSS Boosting Type) ---\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Training Base Model 5: LightGBM (GOSS) ---\")\nstart_time_model5 = datetime.now()\noof_preds_lgbm_goss, test_preds_lgbm_goss, loaded_from_disk = load_predictions_if_exist(\n    CFG.FNAME_LGBM_GOSS_OOF, CFG.FNAME_LGBM_GOSS_TEST, X.shape, X_test.shape, num_classes\n)\nif loaded_from_disk:\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Skipping training for Model 5.\")\nelse:\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] No existing predictions found for Model 5. Starting training.\")\n    # oof_preds_lgbm_goss and test_preds_lgbm_goss are already zero-initialized from load_predictions_if_exist\n    lgbm_goss_params = {\n        \"objective\": \"multiclass\",\n        \"num_class\": num_classes,\n        \"metric\": \"multi_logloss\",\n        \"boosting_type\": \"goss\", # GOSS boosting type\n        \"colsample_bytree\": 0.39736332491996407,\n        \"learning_rate\": 0.008033740989500222,\n        \"min_child_samples\": 29,\n        \"min_child_weight\": 0.6732469853333759,\n        \"n_estimators\": 10000,\n        \"n_jobs\": -1,\n        \"num_leaves\": 89,\n        \"random_state\": GLOBAL_RANDOM_STATE,\n        \"reg_alpha\": 15.595856670965969,\n        \"reg_lambda\": 51.43625034648377,\n        \"subsample\": 0.07846482736630467,\n        \"verbose\": -1,\n    }\n    n_splits_cv_model5 = FOLDS # Use CFG.n_splits\n    early_stopping_rounds_cv_model5 = 100\n    model5_fold_scores = []\n    model5_test_pred_sum = np.zeros((len(X_test), num_classes))\n    kf_model5 = StratifiedKFold(n_splits=n_splits_cv_model5, shuffle=True, random_state=GLOBAL_RANDOM_STATE)\n\n    for fold, (train_idx, valid_idx) in enumerate(kf_model5.split(X, y_encoded)):\n        print(f\"--- Training Fold {fold+1} (Model 5) ---\")\n        fold_train_X, fold_valid_X = X.iloc[train_idx], X.iloc[valid_idx]\n        fold_train_y, fold_valid_y = y_encoded.iloc[train_idx], y_encoded.iloc[valid_idx]\n\n        model5_instance = lgb.LGBMClassifier(**lgbm_goss_params)\n\n        model5_instance.fit(fold_train_X, fold_train_y,\n                            eval_set=[(fold_valid_X, fold_valid_y)],\n                            callbacks=[lgb.early_stopping(early_stopping_rounds_cv_model5, verbose=False)])\n\n        best_iteration = model5_instance.best_iteration_ if hasattr(model5_instance, 'best_iteration_') else lgbm_goss_params['n_estimators']\n        print(f\"Fold {fold+1} best iteration: {best_iteration}\")\n\n        oof_preds_lgbm_goss[valid_idx] = model5_instance.predict_proba(fold_valid_X)\n        model5_test_pred_sum += model5_instance.predict_proba(X_test)\n\n        top_3_preds = np.argsort(oof_preds_lgbm_goss[valid_idx], axis=1)[:, -3:][:, ::-1]\n        fold_score = mapk(fold_valid_y.values, top_3_preds)\n        model5_fold_scores.append(fold_score)\n        print(f\"Fold {fold+1} MAP@3: {fold_score:.4f}\")\n\n        del model5_instance\n        gc.collect()\n\n    test_preds_lgbm_goss = model5_test_pred_sum / n_splits_cv_model5\n    avg_cv_score_model5 = np.mean(model5_fold_scores)\n    print(f\"\\nModel 5 (LightGBM GOSS) Average MAP@3 across {n_splits_cv_model5} folds: {avg_cv_score_model5:.4f}\")\n\n    np.save(os.path.join(CFG.OUTPUT_DIR, CFG.FNAME_LGBM_GOSS_OOF), oof_preds_lgbm_goss)\n    np.save(os.path.join(CFG.OUTPUT_DIR, CFG.FNAME_LGBM_TEST), test_preds_lgbm_goss)\n    print(f\"Saved Model 5 predictions to {CFG.OUTPUT_DIR}\")\n\n# --- Base Model 6: Logistic Regression ---\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Training Base Model 6: Logistic Regression ---\")\nstart_time_model6 = datetime.now()\noof_preds_lr, test_preds_lr, loaded_from_disk = load_predictions_if_exist(\n    CFG.FNAME_LR_OOF, CFG.FNAME_LR_TEST, X.shape, X_test.shape, num_classes\n)\nif loaded_from_disk:\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Skipping training for Model 6.\")\nelse:\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] No existing predictions found for Model 6. Starting training.\")\n    # oof_preds_lr and test_preds_lr are already zero-initialized from load_predictions_if_exist\n    lr_params = {\n        'solver': 'liblinear',\n        'C': 0.1,\n        'random_state': GLOBAL_RANDOM_STATE,\n        'n_jobs': -1,\n        'multi_class': 'ovr'\n    }\n    n_splits_cv_model6 = FOLDS\n    model6_fold_scores = []\n    model6_test_pred_sum = np.zeros((len(X_test), num_classes))\n    kf_model6 = StratifiedKFold(n_splits=n_splits_cv_model6, shuffle=True, random_state=GLOBAL_RANDOM_STATE)\n\n    for fold, (train_idx, valid_idx) in enumerate(kf_model6.split(X, y_encoded)):\n        print(f\"--- Training Fold {fold+1} (Model 6) ---\")\n        fold_train_X, fold_valid_X = X.iloc[train_idx], X.iloc[valid_idx]\n        fold_train_y, fold_valid_y = y_encoded.iloc[train_idx], y_encoded.iloc[valid_idx]\n\n        # Scale numerical features for Logistic Regression\n        scaler_lr = StandardScaler()\n        X_train_scaled_lr = fold_train_X.copy()\n        X_valid_scaled_lr = fold_valid_X.copy()\n        X_test_scaled_lr = X_test.copy()\n\n        X_train_scaled_lr[numerical_cols] = scaler_lr.fit_transform(X_train_scaled_lr[numerical_cols])\n        X_valid_scaled_lr[numerical_cols] = scaler_lr.transform(X_valid_scaled_lr[numerical_cols])\n        X_test_scaled_lr[numerical_cols] = scaler_lr.transform(X_test_scaled_lr[numerical_cols])\n\n\n        model6_instance = LogisticRegression(**lr_params)\n        model6_instance.fit(X_train_scaled_lr, y_train_fold)\n\n        oof_preds_lr[valid_idx] = model6_instance.predict_proba(X_valid_scaled_lr)\n        model6_test_pred_sum += model6_instance.predict_proba(X_test_scaled_lr)\n\n        top_3_preds = np.argsort(oof_preds_lr[valid_idx], axis=1)[:, -3:][:, ::-1]\n        fold_score = mapk(fold_valid_y.values, top_3_preds)\n        model6_fold_scores.append(fold_score)\n        print(f\"Fold {fold+1} MAP@3: {fold_score:.4f}\")\n\n        del model6_instance, scaler_lr\n        gc.collect()\n\n    test_preds_lr = model6_test_pred_sum / n_splits_cv_model6\n    avg_cv_score_model6 = np.mean(model6_fold_scores)\n    print(f\"\\nModel 6 (Logistic Regression) Average MAP@3 across {n_splits_cv_model6} folds: {avg_cv_score_model6:.4f}\")\n\n    np.save(os.path.join(CFG.OUTPUT_DIR, CFG.FNAME_LR_OOF), oof_preds_lr)\n    np.save(os.path.join(CFG.OUTPUT_DIR, CFG.FNAME_LR_TEST), test_preds_lr)\n    print(f\"Saved Model 6 predictions to {CFG.OUTPUT_DIR}\")\n\n# --- NEW Base Model 7: LightGBM (Optimized Hyperparameters) ---\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Training Base Model 7: LightGBM (Optimized Hyperparameters) ---\")\nstart_time_model7 = datetime.now()\n\noof_preds_lgbm_optimized, test_preds_lgbm_optimized, loaded_from_disk = load_predictions_if_exist(\n    CFG.FNAME_LGBM_OPTIMIZED_OOF, CFG.FNAME_LGBM_OPTIMIZED_TEST, X.shape, X_test.shape, num_classes\n)\nif loaded_from_disk:\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Skipping training for Model 7.\")\nelse:\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] No existing predictions found for Model 7. Starting training.\")\n    # oof_preds_lgbm_optimized and test_preds_lgbm_optimized are already zero-initialized from load_predictions_if_exist\n    lgbm_optimized_params = {\n        \"objective\": \"multiclass\",\n        \"num_class\": num_classes,\n        \"metric\": \"multi_logloss\",\n        \"boosting_type\": \"gbdt\",\n        \"n_estimators\": 2000, # Increased n_estimators\n        \"learning_rate\": 0.01, # Adjusted learning rate\n        \"num_leaves\": 60,\n        \"max_depth\": 12,\n        \"subsample\": 0.7,\n        \"min_child_samples\": 25,\n        \"colsample_bytree\": 0.6,\n        \"reg_alpha\": 0.5,\n        \"reg_lambda\": 0.5,\n        \"random_state\": GLOBAL_RANDOM_STATE,\n        \"n_jobs\": -1,\n        \"verbose\": -1,\n    }\n    n_splits_cv_model7 = FOLDS\n    early_stopping_rounds_cv_model7 = 150 # Adjusted early stopping\n    model7_fold_scores = []\n    model7_test_pred_sum = np.zeros((len(X_test), num_classes))\n    kf_model7 = StratifiedKFold(n_splits=n_splits_cv_model7, shuffle=True, random_state=GLOBAL_RANDOM_STATE)\n\n    for fold, (train_idx, valid_idx) in enumerate(kf_model7.split(X, y_encoded)):\n        print(f\"--- Training Fold {fold+1} (Model 7) ---\")\n        fold_train_X, fold_valid_X = X.iloc[train_idx], X.iloc[valid_idx]\n        fold_train_y, fold_valid_y = y_encoded.iloc[train_idx], y_encoded.iloc[valid_idx]\n\n        model7_instance = lgb.LGBMClassifier(**lgbm_optimized_params)\n\n        model7_instance.fit(fold_train_X, fold_train_y,\n                            eval_set=[(fold_valid_X, fold_valid_y)],\n                            callbacks=[lgb.early_stopping(early_stopping_rounds_cv_model7, verbose=False)])\n\n        best_iteration = model7_instance.best_iteration_ if hasattr(model7_instance, 'best_iteration_') else lgbm_optimized_params['n_estimators']\n        print(f\"Fold {fold+1} best iteration: {best_iteration}\")\n\n        oof_preds_lgbm_optimized[valid_idx] = model7_instance.predict_proba(fold_valid_X)\n        model7_test_pred_sum += model7_instance.predict_proba(X_test)\n\n        top_3_preds = np.argsort(oof_preds_lgbm_optimized[valid_idx], axis=1)[:, -3:][:, ::-1]\n        fold_score = mapk(fold_valid_y.values, top_3_preds)\n        model7_fold_scores.append(fold_score)\n        print(f\"Fold {fold+1} MAP@3: {fold_score:.4f}\")\n\n        del model7_instance\n        gc.collect()\n\n    test_preds_lgbm_optimized = model7_test_pred_sum / n_splits_cv_model7\n    avg_cv_score_model7 = np.mean(model7_fold_scores)\n    print(f\"\\nModel 7 (LightGBM Optimized) Average MAP@3 across {n_splits_cv_model7} folds: {avg_cv_score_model7:.4f}\")\n\n    np.save(os.path.join(CFG.OUTPUT_DIR, CFG.FNAME_LGBM_OPTIMIZED_OOF), oof_preds_lgbm_optimized)\n    np.save(os.path.join(CFG.OUTPUT_DIR, CFG.FNAME_LGBM_OPTIMIZED_TEST), test_preds_lgbm_optimized)\n    print(f\"Saved Model 7 predictions to {CFG.OUTPUT_DIR}\")\n\n\n# --- NEW Base Model 8: VotingClassifier ---\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Training Base Model 8: VotingClassifier ---\")\nstart_time_model8 = datetime.now()\n\noof_preds_voting, test_preds_voting, loaded_from_disk = load_predictions_if_exist(\n    CFG.FNAME_VOTING_OOF, CFG.FNAME_VOTING_TEST, X.shape, X_test.shape, num_classes\n)\nif loaded_from_disk:\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Skipping training for Model 8.\")\nelse:\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] No existing predictions found for Model 8. Starting training.\")\n    # oof_preds_voting and test_preds_voting are already zero-initialized from load_predictions_if_exist\n\n    # Create dummy classifiers for the VotingClassifier.\n    # These are placeholders and should ideally be trained or loaded actual models.\n    # For a real VotingClassifier, you'd integrate the actual best-performing base models.\n    clf1 = LogisticRegression(random_state=GLOBAL_RANDOM_STATE, solver='liblinear')\n    clf2 = lgb.LGBMClassifier(random_state=GLOBAL_RANDOM_STATE, verbose=-1)\n    clf3 = XGBClassifier(random_state=GLOBAL_RANDOM_STATE, use_label_encoder=False, eval_metric='mlogloss', n_jobs=-1)\n\n    voting_classifier = VotingClassifier(\n        estimators=[('lr', clf1), ('lgbm', clf2), ('xgb', clf3)],\n        voting='soft', # 'soft' voting uses predicted probabilities\n        weights=[0.1, 0.45, 0.45], # Example weights, should be tuned\n        n_jobs=-1 # Use all available cores\n    )\n\n    n_splits_cv_model8 = FOLDS\n    model8_fold_scores = []\n    model8_test_pred_sum = np.zeros((len(X_test), num_classes))\n    kf_model8 = StratifiedKFold(n_splits=n_splits_cv_model8, shuffle=True, random_state=GLOBAL_RANDOM_STATE)\n\n    for fold, (train_idx, valid_idx) in enumerate(kf_model8.split(X, y_encoded)):\n        print(f\"--- Training Fold {fold+1} (Model 8: VotingClassifier) ---\")\n        from sklearn.base import clone\n        current_voting_classifier = clone(voting_classifier)\n\n        current_voting_classifier.fit(X.iloc[train_idx], y_encoded.iloc[train_idx]) # Use X.iloc here\n        \n        oof_preds_voting[valid_idx] = current_voting_classifier.predict_proba(X.iloc[valid_idx]) # Use X.iloc here\n        model8_test_pred_sum += current_voting_classifier.predict_proba(X_test)\n\n        top_3_preds = np.argsort(oof_preds_voting[valid_idx], axis=1)[:, -3:][:, ::-1]\n        fold_score = mapk(y_encoded.iloc[valid_idx].values, top_3_preds) # Use y_encoded.iloc here\n        model8_fold_scores.append(fold_score)\n        print(f\"Fold {fold+1} MAP@3: {fold_score:.4f}\")\n\n        del current_voting_classifier\n        gc.collect()\n\n    test_preds_voting = model8_test_pred_sum / n_splits_cv_model8\n    avg_cv_score_model8 = np.mean(model8_fold_scores)\n    print(f\"\\nModel 8 (VotingClassifier) Average MAP@3 across {n_splits_cv_model8} folds: {avg_cv_score_model8:.4f}\")\n\n    np.save(os.path.join(CFG.OUTPUT_DIR, CFG.FNAME_VOTING_OOF), oof_preds_voting)\n    np.save(os.path.join(CFG.OUTPUT_DIR, CFG.FNAME_VOTING_TEST), test_preds_voting)\n    print(f\"Saved Model 8 predictions to {CFG.OUTPUT_DIR}\")\n    \n# --- NEW Base Model 9: XGBoost Optimized (from /kaggle/input/ensemble3/) ---\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Training Base Model 9: XGBoost Optimized (from /kaggle/input/ensemble3/) ---\")\nstart_time_model9 = datetime.now()\noof_preds_xgb_optimized_ensemble3, test_preds_xgb_optimized_ensemble3, loaded_from_disk = load_predictions_if_exist(\n    CFG.FNAME_XGB_OPTIMIZED_ENSEMBLE3_OOF, CFG.FNAME_XGB_OPTIMIZED_ENSEMBLE3_TEST, X.shape, X_test.shape, num_classes\n)\nif loaded_from_disk:\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Loaded existing predictions for Model 9. Skipping training.\")\nelse:\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] No existing predictions found for Model 9. This model will be zero-initialized.\")\n    # Add training logic for Model 9 here if it's not expected to be loaded.\n    # For now, it will remain zero-initialized if not found from the provided paths.\n\n\n# --- NEW Base Model 10: XGBoost Custom (from /kaggle/input/ensemble3/) ---\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Training Base Model 10: XGBoost Custom (from /kaggle/input/ensemble3/) ---\")\nstart_time_model10 = datetime.now()\noof_preds_xgb_custom, test_preds_xgb_custom, loaded_from_disk = load_predictions_if_exist(\n    CFG.FNAME_XGB_CUSTOM_OOF, CFG.FNAME_XGB_CUSTOM_TEST, X.shape, X_test.shape, num_classes\n)\nif loaded_from_disk:\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Loaded existing predictions for Model 10. Skipping training.\")\nelse:\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] No existing predictions found for Model 10. This model will be zero-initialized.\")\n    # Add training logic for Model 10 here if it's not expected to be loaded.\n    # For now, it will remain zero-initialized if not found from the provided paths.\n\n\n# --- NEW Base Model 11: Gaussian Naive Bayes ---\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Training Base Model 11: Gaussian Naive Bayes ---\")\nstart_time_model11 = datetime.now()\n\noof_preds_gaussian_nb, test_preds_gaussian_nb, loaded_from_disk = load_predictions_if_exist(\n    CFG.FNAME_GAUSSIAN_NB_OOF, CFG.FNAME_GAUSSIAN_NB_TEST, X.shape, X_test.shape, num_classes\n)\nif loaded_from_disk:\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Skipping training for Model 11.\")\nelse:\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] No existing predictions found for Model 11. Starting training.\")\n    \n    n_splits_cv_model11 = FOLDS\n    model11_fold_scores = []\n    model11_test_pred_sum = np.zeros((len(X_test), num_classes))\n    kf_model11 = StratifiedKFold(n_splits=n_splits_cv_model11, shuffle=True, random_state=GLOBAL_RANDOM_STATE)\n\n    for fold, (train_idx, valid_idx) in enumerate(kf_model11.split(X, y_encoded)):\n        print(f\"--- Training Fold {fold+1} (Model 11: Gaussian Naive Bayes) ---\")\n        fold_train_X, fold_valid_X = X.iloc[train_idx], X.iloc[valid_idx]\n        fold_train_y, fold_valid_y = y_encoded.iloc[train_idx], y_encoded.iloc[valid_idx]\n\n        model11_instance = GaussianNB()\n        model11_instance.fit(fold_train_X, fold_train_y)\n\n        # Naive Bayes predict_proba outputs probabilities\n        oof_preds_gaussian_nb[valid_idx] = model11_instance.predict_proba(fold_valid_X)\n        model11_test_pred_sum += model11_instance.predict_proba(X_test)\n\n        top_3_preds = np.argsort(oof_preds_gaussian_nb[valid_idx], axis=1)[:, -3:][:, ::-1]\n        fold_score = mapk(fold_valid_y.values, top_3_preds)\n        model11_fold_scores.append(fold_score)\n        print(f\"Fold {fold+1} MAP@3: {fold_score:.4f}\")\n\n        del model11_instance\n        gc.collect()\n\n    test_preds_gaussian_nb = model11_test_pred_sum / n_splits_cv_model11\n    avg_cv_score_model11 = np.mean(model11_fold_scores)\n    print(f\"\\nModel 11 (Gaussian Naive Bayes) Average MAP@3 across {n_splits_cv_model11} folds: {avg_cv_score_model11:.4f}\")\n\n    np.save(os.path.join(CFG.OUTPUT_DIR, CFG.FNAME_GAUSSIAN_NB_OOF), oof_preds_gaussian_nb)\n    np.save(os.path.join(CFG.OUTPUT_DIR, CFG.FNAME_GAUSSIAN_NB_TEST), test_preds_gaussian_nb)\n    print(f\"Saved Model 11 predictions to {CFG.OUTPUT_DIR}\")\n\n\n# --- NEW Base Model 12: Linear Discriminant Analysis (LDA) ---\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Training Base Model 12: Linear Discriminant Analysis (LDA) ---\")\nstart_time_model12 = datetime.now()\n\noof_preds_lda_base, test_preds_lda_base, loaded_from_disk = load_predictions_if_exist(\n    CFG.FNAME_LDA_OOF, CFG.FNAME_LDA_TEST, X.shape, X_test.shape, num_classes\n)\nif loaded_from_disk:\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Skipping training for Model 12 (LDA).\")\nelse:\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] No existing predictions found for Model 12 (LDA). Starting training.\")\n    \n    n_splits_cv_model12 = FOLDS\n    model12_fold_scores = []\n    model12_test_pred_sum = np.zeros((len(X_test), num_classes))\n    kf_model12 = StratifiedKFold(n_splits=n_splits_cv_model12, shuffle=True, random_state=GLOBAL_RANDOM_STATE)\n\n    for fold, (train_idx, valid_idx) in enumerate(kf_model12.split(X, y_encoded)):\n        print(f\"--- Training Fold {fold+1} (Model 12: LDA) ---\")\n        \n        X_train_fold, y_train_fold = X.iloc[train_idx], y_encoded.iloc[train_idx]\n        X_valid_fold, y_valid_fold = X.iloc[valid_idx], y_encoded.iloc[valid_idx]\n        \n        # Apply StandardScaler specifically to numerical columns for LDA\n        scaler_lda = StandardScaler()\n        X_train_scaled_lda = X_train_fold.copy()\n        X_valid_scaled_lda = X_valid_fold.copy()\n        X_test_scaled_lda = X_test.copy()\n\n        # Scale only the numerical columns. LDA is sensitive to scaling.\n        X_train_scaled_lda[numerical_cols] = scaler_lda.fit_transform(X_train_scaled_lda[numerical_cols])\n        X_valid_scaled_lda[numerical_cols] = scaler_lda.transform(X_valid_scaled_lda[numerical_cols])\n        X_test_scaled_lda[numerical_cols] = scaler_lda.transform(X_test_scaled_lda[numerical_cols])\n\n        # Initialize LDA model\n        # 'lsqr' solver is generally good for large datasets, 'eigen' for more complex covariance.\n        # 'svd' is default and robust.\n        # shrinkage can be used for regularization, but start without it or use 'auto'.\n        lda_model = LinearDiscriminantAnalysis(solver='svd', n_components=min(num_classes - 1, X_train_scaled_lda.shape[1])) # n_components <= num_classes - 1\n        \n        lda_model.fit(X_train_scaled_lda, y_train_fold)\n        \n        # Predict probabilities\n        oof_preds_lda_base[valid_idx] = lda_model.predict_proba(X_valid_scaled_lda)\n        model12_test_pred_sum += lda_model.predict_proba(X_test_scaled_lda)\n\n        top_3_preds = np.argsort(oof_preds_lda_base[valid_idx], axis=1)[:, -3:][:, ::-1]\n        fold_score = mapk(y_valid_fold.values, top_3_preds)\n        model12_fold_scores.append(fold_score)\n        print(f\"Fold {fold+1} MAP@3: {fold_score:.4f}\")\n\n        del scaler_lda, lda_model\n        gc.collect()\n\n    test_preds_lda_base = model12_test_pred_sum / n_splits_cv_model12\n    avg_cv_score_model12 = np.mean(model12_fold_scores)\n    print(f\"\\nModel 12 (LDA Base) Average MAP@3 across {n_splits_cv_model12} folds: {avg_cv_score_model12:.4f}\")\n\n    np.save(os.path.join(CFG.OUTPUT_DIR, CFG.FNAME_LDA_OOF), oof_preds_lda_base)\n    np.save(os.path.join(CFG.OUTPUT_DIR, CFG.FNAME_LDA_TEST), test_preds_lda_base)\n    print(f\"Saved Model 12 predictions to {CFG.OUTPUT_DIR}\")\n\n\n# --- NEW Base Model 13: HistGradientBoostingClassifier ---\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Training Base Model 13: HistGradientBoostingClassifier ---\")\nstart_time_model13 = datetime.now()\n\noof_preds_hgb, test_preds_hgb, loaded_from_disk = load_predictions_if_exist(\n    CFG.FNAME_HGB_OOF, CFG.FNAME_HGB_TEST, X.shape, X_test.shape, num_classes\n)\nif loaded_from_disk:\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Skipping training for Model 13 (HGB).\")\nelse:\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] No existing predictions found for Model 13 (HGB). Starting training.\")\n    \n    # HistGradientBoostingClassifier parameters provided by the user\n    hgb_params = {\n        'max_iter': 5000,\n        'random_state': GLOBAL_RANDOM_STATE, # Using GLOBAL_RANDOM_STATE for consistency\n        'early_stopping': True,\n        'learning_rate': 0.1,\n        'loss': 'log_loss',\n        'l2_regularization': 2.786601939402124e-08,\n        'max_depth': 5,\n        'max_leaf_nodes': 37,\n        'min_samples_leaf': 75,\n        'n_iter_no_change': CFG.early_stopping_rounds # Using CFG for early stopping rounds\n    }\n\n    n_splits_cv_model13 = FOLDS\n    model13_fold_scores = []\n    model13_test_pred_sum = np.zeros((len(X_test), num_classes))\n    kf_model13 = StratifiedKFold(n_splits=n_splits_cv_model13, shuffle=True, random_state=GLOBAL_RANDOM_STATE)\n\n    for fold, (train_idx, valid_idx) in enumerate(kf_model13.split(X, y_encoded)):\n        print(f\"--- Training Fold {fold+1} (Model 13: HistGradientBoostingClassifier) ---\")\n        \n        X_train_fold, y_train_fold = X.iloc[train_idx], y_encoded.iloc[train_idx]\n        X_valid_fold, y_valid_fold = X.iloc[valid_idx], y_encoded.iloc[valid_idx]\n        \n        hgb_model = HistGradientBoostingClassifier(\n            **hgb_params,\n            # Pass the indices of categorical features.\n            # \"from_dtype\" would not work correctly as OrdinalEncoder converts them to int.\n            categorical_features=categorical_feature_indices_hgb \n        )\n        \n        hgb_model.fit(X_train_fold, y_train_fold)\n        \n        oof_preds_hgb[valid_idx] = hgb_model.predict_proba(X_valid_fold)\n        model13_test_pred_sum += hgb_model.predict_proba(X_test)\n\n        top_3_preds = np.argsort(oof_preds_hgb[valid_idx], axis=1)[:, -3:][:, ::-1]\n        fold_score = mapk(y_valid_fold.values, top_3_preds)\n        model13_fold_scores.append(fold_score)\n        print(f\"Fold {fold+1} MAP@3: {fold_score:.4f}\")\n\n        del hgb_model\n        gc.collect()\n\n    test_preds_hgb = model13_test_pred_sum / n_splits_cv_model13\n    avg_cv_score_model13 = np.mean(model13_fold_scores)\n    print(f\"\\nModel 13 (HistGradientBoostingClassifier) Average MAP@3 across {n_splits_cv_model13} folds: {avg_cv_score_model13:.4f}\")\n\n    np.save(os.path.join(CFG.OUTPUT_DIR, CFG.FNAME_HGB_OOF), oof_preds_hgb)\n    np.save(os.path.join(CFG.OUTPUT_DIR, CFG.FNAME_HGB_TEST), test_preds_hgb)\n    print(f\"Saved Model 13 predictions to {CFG.OUTPUT_DIR}\")\n\n# --- NEW Base Model 14: Yggdrasil Decision Forests (RandomForest) ---\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Training Base Model 14: Yggdrasil Decision Forests (RandomForest) ---\")\nstart_time_model14 = datetime.now()\n\noof_preds_ydf, test_preds_ydf, loaded_from_disk = load_predictions_if_exist(\n    CFG.FNAME_YDF_OOF, CFG.FNAME_YDF_TEST, X.shape, X_test.shape, num_classes\n)\nif loaded_from_disk:\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Skipping training for Model 14 (YDF).\")\nelse:\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] No existing predictions found for Model 14 (YDF). Starting training.\")\n    \n    # YDF hyperparameters for training\n    ydf_training_params = {\n        'num_trees': 200,\n        'max_depth': 15,\n        'random_seed': GLOBAL_RANDOM_STATE, # Map Config.state to GLOBAL_RANDOM_STATE\n        'growing_strategy': 'BEST_FIRST_GLOBAL',\n    }\n\n    n_splits_cv_model14 = FOLDS\n    model14_fold_scores = []\n    model14_test_pred_sum = np.zeros((len(X_test), num_classes))\n    kf_model14 = StratifiedKFold(n_splits=n_splits_cv_model14, shuffle=True, random_state=GLOBAL_RANDOM_STATE)\n\n    for fold, (train_idx, valid_idx) in enumerate(kf_model14.split(X, y_encoded)):\n        print(f\"--- Training Fold {fold+1} (Model 14: YDF) ---\")\n        \n        # Create a combined DataFrame for YDF with a 'label' column\n        X_train_fold_ydf = X.iloc[train_idx].copy()\n        X_valid_fold_ydf = X.iloc[valid_idx].copy()\n\n        # YDF requires the label to be named 'label' in the DataFrame passed to train_model\n        train_df_ydf = X_train_fold_ydf.copy()\n        train_df_ydf['label'] = y_encoded.iloc[train_idx].values\n\n        # For validation and test, no 'label' column is needed for predict_proba\n        valid_df_ydf = X_valid_fold_ydf.copy()\n        test_df_ydf = X_test.copy()\n\n        # Initialize YDF learner for training, passing label and hyperparameters to the constructor\n        ydf_learner = ydf.RandomForestLearner(label=\"label\", **ydf_training_params)\n        \n        # Train YDF model\n        ydf_trained_model = ydf_learner.train(train_df_ydf)\n        \n        # Predict probabilities (using .predict() as it returns probabilities for classification)\n        oof_preds_ydf[valid_idx] = ydf_trained_model.predict(valid_df_ydf)\n        model14_test_pred_sum += ydf_trained_model.predict(test_df_ydf)\n\n        top_3_preds = np.argsort(oof_preds_ydf[valid_idx], axis=1)[:, -3:][:, ::-1]\n        fold_score = mapk(y_encoded.iloc[valid_idx].values, top_3_preds)\n        model14_fold_scores.append(fold_score)\n        print(f\"Fold {fold+1} MAP@3: {fold_score:.4f}\")\n\n        del ydf_learner, ydf_trained_model, train_df_ydf, valid_df_ydf\n        gc.collect()\n\n    test_preds_ydf = model14_test_pred_sum / n_splits_cv_model14\n    avg_cv_score_model14 = np.mean(model14_fold_scores)\n    print(f\"\\nModel 14 (YDF) Average MAP@3 across {n_splits_cv_model14} folds: {avg_cv_score_model14:.4f}\")\n\n    np.save(os.path.join(CFG.OUTPUT_DIR, CFG.FNAME_YDF_OOF), oof_preds_ydf)\n    np.save(os.path.join(CFG.OUTPUT_DIR, CFG.FNAME_YDF_TEST), test_preds_ydf)\n    print(f\"Saved Model 14 predictions to {CFG.OUTPUT_DIR}\")\n\n\n# --- Prepare Meta-Features for the Single-Layer Logistic Regression ---\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Preparing Meta-Features for Single-Layer Logistic Regression (14 Base Models) ---\")\nstart_time_meta_features = datetime.now()\n\n# At this point, oof_preds_* and test_preds_* variables should contain either\n# loaded data or newly computed data (zero-initialized if not found and not computed).\n# The load_predictions_if_exist function ensures they are proper NumPy arrays.\n\n# X_meta_train for the Logistic Regression is directly the concatenated OOF predictions\nX_meta_train = np.hstack([\n    oof_preds_xgb_train_api1,\n    oof_preds_xgb_sklearn,\n    oof_preds_lgbm,\n    oof_preds_xgb_api2,\n    oof_preds_lgbm_goss,\n    oof_preds_lr,\n    oof_preds_lgbm_optimized,\n    oof_preds_voting,\n    oof_preds_xgb_optimized_ensemble3, # Model 9\n    oof_preds_xgb_custom, # Model 10\n    oof_preds_gaussian_nb, # Model 11\n    oof_preds_lda_base, # Model 12 (LDA)\n    oof_preds_hgb, # Model 13 (HistGradientBoostingClassifier)\n    oof_preds_ydf # NEW Model 14 (YDF)\n])\n# X_meta_test for the Logistic Regression is directly the concatenated test predictions\nX_meta_test = np.hstack([\n    test_preds_xgb_train_api1,\n    test_preds_xgb_sklearn,\n    test_preds_lgbm,\n    test_preds_xgb_api2,\n    test_preds_lgbm_goss,\n    test_preds_lr,\n    test_preds_lgbm_optimized,\n    test_preds_voting,\n    test_preds_xgb_optimized_ensemble3, # Model 9\n    test_preds_xgb_custom, # Model 10\n    test_preds_gaussian_nb, # Model 11\n    test_preds_lda_base, # Model 12 (LDA)\n    test_preds_hgb, # Model 13 (HistGradientBoostingClassifier)\n    test_preds_ydf # NEW Model 14 (YDF)\n])\n\n# Scale inputs for the final Logistic Regression meta-model\nscaler_final_meta = StandardScaler()\nX_meta_train_scaled = scaler_final_meta.fit_transform(X_meta_train)\nX_meta_test_scaled = scaler_final_meta.transform(X_meta_test)\n\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] Meta-training set shape for Logistic Regression: {X_meta_train_scaled.shape}\")\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] Meta-test set shape for Logistic Regression: {X_meta_test_scaled.shape}\")\n\nend_time_meta_features = datetime.now()\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Meta-Features for Logistic Regression Prepared. Elapsed: {end_time_meta_features - start_time_meta_features} ---\\n\")\n\n\n# --- 8. Train the Final Meta-Model (Logistic Regression) ---\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Training Final Logistic Regression Meta-Model (Single Layer, 14 Base Models) ---\")\nstart_time_final_model = datetime.now()\n\nfinal_meta_model = LogisticRegression(\n    solver='liblinear',\n    C=0.1,\n    random_state=GLOBAL_RANDOM_STATE,\n    n_jobs=-1,\n    multi_class='ovr'\n)\nfinal_meta_model.fit(X_meta_train_scaled, y_encoded)\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] Final Logistic Regression Meta-Model training complete.\")\n\nend_time_final_model = datetime.now()\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Final Logistic Regression Meta-Model Training Finished. Elapsed: {end_time_final_model - start_time_final_model} ---\\n\")\n\n\n# --- 9. Generate Final Ensemble Predictions and Submission ---\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Generating Final Stacked Ensemble Predictions (Single Layer, 14 Base Models) ---\")\nstart_time_submission = datetime.now()\n\nfinal_ensemble_test_probs = final_meta_model.predict_proba(X_meta_test_scaled)\ntop_3_preds_ensemble = np.argsort(final_ensemble_test_probs, axis=1)[:, -3:][:, ::-1]\n\n# Inverse transform to get original fertilizer names (strings)\ntop_3_labels_ensemble = le.inverse_transform(top_3_preds_ensemble.ravel()).reshape(top_3_preds_ensemble.shape)\n\n# Create submission DataFrame\nsubmission_ensemble = pd.DataFrame({\n    \"id\": df_sub[\"id\"],\n    \"Fertilizer Name\": [' '.join(label for label in row) for row in top_3_labels_ensemble]\n})\n\nsubmission_filename = \"submission_stacked_ensemble_14_models.csv\"\nsubmission_ensemble.to_csv(submission_filename, index=False)\n\nprint(f\"üìÅ Final single-layer stacked ensemble submission saved to '{submission_filename}'\")\n\n# --- Display the head of the submission file (for verification) ---\nprint(\"\\nFirst 5 rows of the final submission DataFrame (for display):\")\nwith pd.option_context('display.max_colwidth', None, 'display.width', 1000):\n    print(submission_ensemble.head())\n\nend_time_submission = datetime.now()\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Final Submission Generation Finished. Elapsed: {end_time_submission - start_time_submission} ---\\n\")\n\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Single-Layer Stacked Ensemble Process Finished ---\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T16:58:10.641381Z","iopub.execute_input":"2025-06-17T16:58:10.641740Z","iopub.status.idle":"2025-06-17T17:12:19.892533Z","shell.execute_reply.started":"2025-06-17T16:58:10.641716Z","shell.execute_reply":"2025-06-17T17:12:19.891674Z"}},"outputs":[{"name":"stdout","text":"\n[16:58:10] --- Starting Data Loading and Initial Preprocessing ---\n[16:58:11] --- Starting Ordinal and Label Encoding ---\nHistGradientBoostingClassifier will treat columns at indices [3, 4] as categorical.\n[16:58:12] --- Data Loading and Preprocessing Finished. Elapsed: 0:00:01.791497 ---\n\n[16:58:12] --- Stacking Ensemble Setup Started ---\n\n[16:58:12] --- Training Base Model 1: XGBoost (xgb.train API - Original Block 1) ---\nLoading predictions from secondary /kaggle/input/boosting-output/: oof_preds_xgb_train_api1.npy, test_preds_xgb_train_api1.npy\n[16:58:12] Skipping training for Model 1.\n[16:58:12] --- Training Base Model 2: XGBoost (XGBClassifier API) ---\nLoading predictions from secondary /kaggle/input/boosting-output/: oof_preds_xgb_sklearn.npy, test_preds_xgb_sklearn.npy\n[16:58:12] Skipping training for Model 2.\n[16:58:12] --- Training Base Model 3: LightGBM ---\nLoading predictions from secondary /kaggle/input/boosting-output/: oof_preds_lgbm.npy, test_preds_lgbm.npy\n[16:58:12] Skipping training for Model 3.\n[16:58:12] --- Training Base Model 4: XGBoost (xgb.train API with CFG parameters) ---\nLoading predictions from secondary /kaggle/input/boosting-output/: oof_preds_xgb_train_api2.npy, test_preds_xgb_train_api2.npy\n[16:58:12] Skipping training for Model 4.\n[16:58:12] --- Training Base Model 5: LightGBM (GOSS) ---\nLoading predictions from secondary /kaggle/input/boosting-output/: oof_preds_lgbm_goss.npy, test_preds_lgbm_goss.npy\n[16:58:12] Skipping training for Model 5.\n[16:58:12] --- Training Base Model 6: Logistic Regression ---\nLoading predictions from primary UPLOADED_INPUT_DIR (/kaggle/input/ensemble2/): oof_preds_lr.npy, test_preds_lr.npy\n[16:58:12] Skipping training for Model 6.\n[16:58:12] --- Training Base Model 7: LightGBM (Optimized Hyperparameters) ---\nLoading predictions from primary UPLOADED_INPUT_DIR (/kaggle/input/ensemble2/): oof_preds_lgbm_optimized.npy, test_preds_lgbm_optimized.npy\n[16:58:12] Skipping training for Model 7.\n[16:58:12] --- Training Base Model 8: VotingClassifier ---\nLoading predictions from primary UPLOADED_INPUT_DIR (/kaggle/input/ensemble2/): oof_preds_voting.npy, test_preds_voting.npy\n[16:58:12] Skipping training for Model 8.\n[16:58:12] --- Training Base Model 9: XGBoost Optimized (from /kaggle/input/ensemble3/) ---\nLoading predictions from /kaggle/input/ensemble3/: oof_preds_xgb_optimized.npy, test_preds_xgb_optimized.npy\n[16:58:12] Loaded existing predictions for Model 9. Skipping training.\n[16:58:12] --- Training Base Model 10: XGBoost Custom (from /kaggle/input/ensemble3/) ---\nLoading predictions from /kaggle/input/ensemble3/: oof_preds_xgb_custom.npy, test_preds_xgb_custom.npy\n[16:58:12] Loaded existing predictions for Model 10. Skipping training.\n[16:58:12] --- Training Base Model 11: Gaussian Naive Bayes ---\nLoading predictions from current session's OUTPUT_DIR: oof_preds_gaussian_nb.npy, test_preds_gaussian_nb.npy\n[16:58:12] Skipping training for Model 11.\n[16:58:12] --- Training Base Model 12: Linear Discriminant Analysis (LDA) ---\nLoading predictions from current session's OUTPUT_DIR: oof_preds_lda_base.npy, test_preds_lda_base.npy\n[16:58:12] Skipping training for Model 12 (LDA).\n[16:58:12] --- Training Base Model 13: HistGradientBoostingClassifier ---\nLoading predictions from current session's OUTPUT_DIR: oof_preds_hgb.npy, test_preds_hgb.npy\n[16:58:12] Skipping training for Model 13 (HGB).\n[16:58:12] --- Training Base Model 14: Yggdrasil Decision Forests (RandomForest) ---\nNo existing predictions found for oof_preds_ydf.npy or test_preds_ydf.npy. Will initialize as zeros.\n[16:58:12] No existing predictions found for Model 14 (YDF). Starting training.\n--- Training Fold 1 (Model 14: YDF) ---\nTrain model on 680000 examples\nModel trained in 0:00:48.635317\nFold 1 MAP@3: 0.2892\n--- Training Fold 2 (Model 14: YDF) ---\nTrain model on 680000 examples\nModel trained in 0:00:48.401064\nFold 2 MAP@3: 0.2897\n--- Training Fold 3 (Model 14: YDF) ---\nTrain model on 680000 examples\nModel trained in 0:00:49.288760\nFold 3 MAP@3: 0.2871\n--- Training Fold 4 (Model 14: YDF) ---\nTrain model on 680000 examples\nModel trained in 0:00:48.532898\nFold 4 MAP@3: 0.2886\n--- Training Fold 5 (Model 14: YDF) ---\nTrain model on 680000 examples\nModel trained in 0:00:49.359752\nFold 5 MAP@3: 0.2891\n\nModel 14 (YDF) Average MAP@3 across 5 folds: 0.2888\nSaved Model 14 predictions to /kaggle/working/outputs/\n[17:02:36] --- Preparing Meta-Features for Single-Layer Logistic Regression (14 Base Models) ---\n[17:02:39] Meta-training set shape for Logistic Regression: (850000, 98)\n[17:02:39] Meta-test set shape for Logistic Regression: (250000, 98)\n[17:02:39] --- Meta-Features for Logistic Regression Prepared. Elapsed: 0:00:02.386316 ---\n\n[17:02:39] --- Training Final Logistic Regression Meta-Model (Single Layer, 14 Base Models) ---\n[17:12:18] Final Logistic Regression Meta-Model training complete.\n[17:12:18] --- Final Logistic Regression Meta-Model Training Finished. Elapsed: 0:09:39.732212 ---\n\n[17:12:18] --- Generating Final Stacked Ensemble Predictions (Single Layer, 14 Base Models) ---\nüìÅ Final single-layer stacked ensemble submission saved to 'submission_stacked_ensemble_14_models.csv'\n\nFirst 5 rows of the final submission DataFrame (for display):\n       id             Fertilizer Name\n0  750000          10-26-26 20-20 DAP\n1  750001      17-17-17 10-26-26 Urea\n2  750002            20-20 28-28 Urea\n3  750003  14-35-14 10-26-26 17-17-17\n4  750004            Urea 20-20 28-28\n[17:12:19] --- Final Submission Generation Finished. Elapsed: 0:00:01.005981 ---\n\n[17:12:19] --- Single-Layer Stacked Ensemble Process Finished ---\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport optuna # Still imported but not used directly in the provided snippets\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier # Still imported but not used\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC # For SVM in intermediate layer\nfrom sklearn.naive_bayes import GaussianNB # For Naive Bayes in intermediate layer\nfrom sklearn.model_selection import StratifiedKFold, KFold, GridSearchCV\nfrom sklearn.preprocessing import OrdinalEncoder, LabelEncoder, StandardScaler\nfrom sklearn.metrics import log_loss\nimport warnings\nimport gc\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom datetime import datetime\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Suppress warnings for cleaner output\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T10:01:09.940661Z","iopub.execute_input":"2025-06-17T10:01:09.941017Z","iopub.status.idle":"2025-06-17T10:01:09.947819Z","shell.execute_reply.started":"2025-06-17T10:01:09.940989Z","shell.execute_reply":"2025-06-17T10:01:09.946862Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# --- 1. Configuration and Global Random State ---\nclass CFG:\n    seed = 42\n    target = 'Fertilizer Name'\n    n_splits = 5 # Number of folds for cross-validation\n    learning_rate = 0.03\n    num_boost_round = 5000\n    early_stopping_rounds = 50\n    verbose_eval = 200\n\n    # --- Output Directory for current session (will be cleared on session end) ---\n    OUTPUT_DIR = '/kaggle/working/outputs/'\n    \n    # --- Directory for User Uploaded Input Files (e.g., from a Kaggle Dataset) ---\n    # Set to /kaggle/input/ensemble2/ as the primary uploaded input dir for new models\n    UPLOADED_INPUT_DIR = '/kaggle/input/ensemble2/' \n\n    # Filenames for base model predictions\n    FNAME_XGB_API1_OOF = 'oof_preds_xgb_train_api1.npy'\n    FNAME_XGB_API1_TEST = 'test_preds_xgb_train_api1.npy'\n    \n    FNAME_XGB_SKLEARN_OOF = 'oof_preds_xgb_sklearn.npy'\n    FNAME_XGB_SKLEARN_TEST = 'test_preds_xgb_sklearn.npy'\n    \n    FNAME_LGBM_OOF = 'oof_preds_lgbm.npy'\n    FNAME_LGBM_TEST = 'test_preds_lgbm.npy'\n    \n    # Updated filenames for Model 4\n    FNAME_XGB_API2_OOF = 'oof_preds_xgb_train_api2.npy'\n    FNAME_XGB_API2_TEST = 'test_preds_xgb_train_api2.npy'\n    \n    FNAME_LGBM_GOSS_OOF = 'oof_preds_lgbm_goss.npy'\n    FNAME_LGBM_GOSS_TEST = 'test_preds_lgbm_goss.npy'\n\n    FNAME_LR_OOF = 'oof_preds_lr.npy'\n    FNAME_LR_TEST = 'test_preds_lr.npy'\n\n    FNAME_LGBM_OPTIMIZED_OOF = 'oof_preds_lgbm_optimized.npy'\n    FNAME_LGBM_OPTIMIZED_TEST = 'test_preds_lgbm_optimized.npy'\n\n    FNAME_VOTING_OOF = 'oof_preds_voting.npy'\n    FNAME_VOTING_TEST = 'test_preds_voting.npy'\n\n    # --- Filenames for the XGBoost models from /kaggle/input/ensemble3/ ---\n    FNAME_XGB_OPTIMIZED_ENSEMBLE3_OOF = 'oof_preds_xgb_optimized.npy' \n    FNAME_XGB_OPTIMIZED_ENSEMBLE3_TEST = 'test_preds_xgb_optimized.npy'\n\n    FNAME_XGB_CUSTOM_OOF = 'oof_preds_xgb_custom.npy'\n    FNAME_XGB_CUSTOM_TEST = 'test_preds_xgb_custom.npy'\n\n    # --- Filenames for the Gaussian Naive Bayes Model ---\n    FNAME_GAUSSIAN_NB_OOF = 'oof_preds_gaussian_nb.npy'\n    FNAME_GAUSSIAN_NB_TEST = 'test_preds_gaussian_nb.npy'\n\n    # --- NEW: Filenames for the KNN Base Model ---\n    FNAME_KNN_OOF = 'oof_preds_knn_base.npy'\n    FNAME_KNN_TEST = 'test_preds_knn_base.npy'\n\n\n# Create the output directory if it doesn't exist. This is essential for saving.\nos.makedirs(CFG.OUTPUT_DIR, exist_ok=True)\n\n# Define FOLDS based on CFG for consistency\nFOLDS = CFG.n_splits\nGLOBAL_RANDOM_STATE = CFG.seed\nnp.random.seed(GLOBAL_RANDOM_STATE)\n\n# --- Helper function to check and load predictions from either source ---\ndef load_predictions_if_exist(oof_filename, test_filename, X_shape, X_test_shape, num_classes):\n    \"\"\"\n    Checks if prediction files exist in the current session's OUTPUT_DIR,\n    the primary UPLOADED_INPUT_DIR (CFG.UPLOADED_INPUT_DIR),\n    the /kaggle/input/ensemble3/ directory, or\n    the secondary /kaggle/input/boosting-output/ directory, and loads them.\n    If files are not found, it returns zero-initialized NumPy arrays of the correct shape.\n\n    Args:\n        oof_filename (str): The filename for the OOF predictions.\n        test_filename (str): The filename for the test predictions.\n        X_shape (tuple): Shape of the full training features (e.g., (num_samples, num_features)).\n        X_test_shape (tuple): Shape of the full test features (e.g., (num_test_samples, num_features)).\n        num_classes (int): Number of target classes.\n\n    Returns:\n        tuple: (oof_preds_array, test_preds_array, loaded_from_disk_flag).\n               Returns (zero_initialized_oof_array, zero_initialized_test_array, False)\n               if files are not found.\n    \"\"\"\n    oof_path_output = os.path.join(CFG.OUTPUT_DIR, oof_filename)\n    test_path_output = os.path.join(CFG.OUTPUT_DIR, test_filename)\n    \n    uploaded_input_dir_exists = hasattr(CFG, 'UPLOADED_INPUT_DIR') and CFG.UPLOADED_INPUT_DIR\n    \n    oof_path_primary_input = None\n    test_path_primary_input = None\n    if uploaded_input_dir_exists:\n        oof_path_primary_input = os.path.join(CFG.UPLOADED_INPUT_DIR, oof_filename) # /kaggle/input/ensemble2/\n        test_path_primary_input = os.path.join(CFG.UPLOADED_INPUT_DIR, test_filename)\n\n    # NEW: Specific path for /kaggle/input/ensemble3/\n    oof_path_ensemble3_input = os.path.join('/kaggle/input/ensemble3/', oof_filename)\n    test_path_ensemble3_input = os.path.join('/kaggle/input/ensemble3/', test_filename)\n\n    # Original secondary input directory for boosting-output files\n    oof_path_boosting_output_input = os.path.join('/kaggle/input/boosting-output/', oof_filename)\n    test_path_boosting_output_input = os.path.join('/kaggle/input/boosting-output/', test_filename)\n\n\n    # Priority 1: Check in the current session's output directory\n    if os.path.exists(oof_path_output) and os.path.exists(test_path_output):\n        print(f\"Loading predictions from current session's OUTPUT_DIR: {oof_filename}, {test_filename}\")\n        return np.load(oof_path_output), np.load(test_path_output), True\n    # Priority 2: Check in the primary user-uploaded input directory (e.g., /kaggle/input/ensemble2/)\n    elif uploaded_input_dir_exists and os.path.exists(oof_path_primary_input) and os.path.exists(test_path_primary_input):\n        print(f\"Loading predictions from primary UPLOADED_INPUT_DIR ({CFG.UPLOADED_INPUT_DIR}): {oof_filename}, {test_filename}\")\n        return np.load(oof_path_primary_input), np.load(test_path_primary_input), True\n    # Priority 3: Check in the /kaggle/input/ensemble3/ directory\n    elif os.path.exists(oof_path_ensemble3_input) and os.path.exists(test_path_ensemble3_input):\n        print(f\"Loading predictions from /kaggle/input/ensemble3/: {oof_filename}, {test_filename}\")\n        return np.load(oof_path_ensemble3_input), np.load(test_path_ensemble3_input), True\n    # Priority 4: Check in the secondary user-uploaded input directory (e.g., /kaggle/input/boosting-output/)\n    elif os.path.exists(oof_path_boosting_output_input) and os.path.exists(test_path_boosting_output_input):\n        print(f\"Loading predictions from secondary /kaggle/input/boosting-output/: {oof_filename}, {test_filename}\")\n        return np.load(oof_path_boosting_output_input), np.load(test_path_boosting_output_input), True\n    else:\n        # If files are not found, return zero-initialized arrays\n        print(f\"No existing predictions found for {oof_filename} or {test_filename}. Will initialize as zeros.\")\n        return np.zeros((X_shape[0], num_classes)), np.zeros((X_test_shape[0], num_classes)), False\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T10:00:02.064331Z","iopub.execute_input":"2025-06-17T10:00:02.065273Z","iopub.status.idle":"2025-06-17T10:00:02.083779Z","shell.execute_reply.started":"2025-06-17T10:00:02.065230Z","shell.execute_reply":"2025-06-17T10:00:02.082561Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# --- 2. Data Loading and Initial Preprocessing ---\nprint(\"--- Data Loading and Initial Preprocessing ---\")\ndf_train = pd.read_csv('/kaggle/input/playground-series-s5e6/train.csv')\ndf_test = pd.read_csv('/kaggle/input/playground-series-s5e6/test.csv')\ndf_sub = pd.read_csv('/kaggle/input/playground-series-s5e6/sample_submission.csv')\ndf_original = pd.read_csv('/kaggle/input/original/Fertilizer Prediction .csv')\n\n# Drop 'id' columns if they exist in train/test sets as per original notebook\ndf_train = df_train.drop(columns=['id'])\nif 'id' in df_test.columns:\n    df_test = df_test.drop(columns=['id'])\n\n# Concatenate original dataset to the training data\ndf_train = pd.concat([df_train, df_original], axis=0, ignore_index=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T10:00:02.084929Z","iopub.execute_input":"2025-06-17T10:00:02.085260Z","iopub.status.idle":"2025-06-17T10:00:04.056359Z","shell.execute_reply.started":"2025-06-17T10:00:02.085225Z","shell.execute_reply":"2025-06-17T10:00:04.055130Z"}},"outputs":[{"name":"stdout","text":"--- Data Loading and Initial Preprocessing ---\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# --- 3. Ordinal and Label Encoding ---\nprint(\"--- Ordinal and Label Encoding ---\")\ncat_cols_for_ordinal = df_train.select_dtypes(include='object').columns.tolist()\nif 'Fertilizer Name' in cat_cols_for_ordinal:\n    cat_cols_for_ordinal.remove('Fertilizer Name')\n\nordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\ndf_train[cat_cols_for_ordinal] = ordinal_encoder.fit_transform(df_train[cat_cols_for_ordinal].astype(str)).astype(int)\n\ncat_cols_for_test = [col for col in cat_cols_for_ordinal if col in df_test.columns]\ndf_test[cat_cols_for_test] = ordinal_encoder.transform(df_test[cat_cols_for_test].astype(str)).astype(int)\n\nle = LabelEncoder()\ndf_train['Fertilizer Name'] = le.fit_transform(df_train['Fertilizer Name'])\nnum_classes = len(np.unique(df_train['Fertilizer Name']))\n\ny_encoded = df_train['Fertilizer Name'] # Target for training\nX = df_train.drop(columns=['Fertilizer Name']) # Features for training\nX_test = df_test # Features for final test prediction\n# Define numerical columns for scaling\n# Moved this definition here to ensure it's available globally before any model training or helper functions use it.\nnumerical_cols = X.select_dtypes(include=np.number).columns.tolist()\n# Filter out any columns that might have been ordinal encoded but are numeric-like\nnumerical_cols = [col for col in numerical_cols if col not in cat_cols_for_ordinal]\n\n\nend_time_data_load = datetime.now()\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Data Loading and Preprocessing Finished. Elapsed: {end_time_data_load - start_time_data_load} ---\\n\")\n\n# Define mapk function\ndef mapk(actual, predicted, k=3):\n    def apk(a, p, k):\n        p = p[:k]\n        score = 0.0\n        hits = 0\n        seen = set()\n        for i, pred in enumerate(p):\n            if pred in a and pred not in seen:\n                hits += 1\n                score += hits / (i + 1.0)\n                seen.add(pred)\n        return score / min(len(a), k) if min(len(a), k) > 0 else 0.0\n\n    if not isinstance(actual[0], (list, np.ndarray)):\n        actual = [[a] for a in actual]\n\n    return np.mean([apk(a, p, k) for a, p in zip(actual, predicted)])\n\nprint(\"--- Stacking Ensemble Setup Started ---\")\n\n# --- Initialize OOF and Test Prediction Arrays for Each Base Model ---\n# These will be initialized to zeros, then potentially loaded or filled during training\noof_preds_xgb_train_api1 = np.zeros((len(X), num_classes))\noof_preds_xgb_sklearn = np.zeros((len(X), num_classes))\noof_preds_lgbm = np.zeros((len(X), num_classes))\noof_preds_xgb_train_api2 = np.zeros((len(X), num_classes))\noof_preds_lgbm_goss = np.zeros((len(X), num_classes))\n\ntest_preds_xgb_train_api1 = np.zeros((len(X_test), num_classes))\ntest_preds_xgb_sklearn = np.zeros((len(X_test), num_classes))\ntest_preds_lgbm = np.zeros((len(X_test), num_classes))\ntest_preds_xgb_train_api2 = np.zeros((len(X_test), num_classes))\ntest_preds_lgbm_goss = np.zeros((len(X_test), num_classes))\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T10:00:04.058718Z","iopub.execute_input":"2025-06-17T10:00:04.059098Z","iopub.status.idle":"2025-06-17T10:00:05.235593Z","shell.execute_reply.started":"2025-06-17T10:00:04.059070Z","shell.execute_reply":"2025-06-17T10:00:05.233831Z"}},"outputs":[{"name":"stdout","text":"--- Ordinal and Label Encoding ---\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/1923824301.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mend_time_data_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[{datetime.now().strftime('%H:%M:%S')}] --- Data Loading and Preprocessing Finished. Elapsed: {end_time_data_load - start_time_data_load} ---\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# Define mapk function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'start_time_data_load' is not defined"],"ename":"NameError","evalue":"name 'start_time_data_load' is not defined","output_type":"error"}],"execution_count":5},{"cell_type":"code","source":"# --- Base Model 1: XGBoost (using xgb.train API - first block in original notebook) ---\nprint(\"\\n--- Training Base Model 1: XGBoost (xgb.train API - Original Block 1) ---\")\nif os.path.exists(CFG.FNAME_XGB_API1_OOF) and os.path.exists(CFG.FNAME_XGB_API1_TEST):\n    oof_preds_xgb_train_api1 = np.load(CFG.FNAME_XGB_API1_OOF)\n    test_preds_xgb_train_api1 = np.load(CFG.FNAME_XGB_API1_TEST)\n    print(f\"Loaded existing predictions for Model 1 from {CFG.OUTPUT_DIR}. Skipping training.\")\nelse:\n    kf_model1 = KFold(n_splits=FOLDS, shuffle=True, random_state=42)\n    model1_logloss_scores = []\n    model1_test_pred_sum = np.zeros((len(X_test), num_classes))\n\n    for i, (train_idx, valid_idx) in enumerate(kf_model1.split(X, y_encoded)):\n        print(f\"\\n{'#'*10} Fold {i+1} (Model 1) {'#'*10}\")\n\n        x_train_fold, y_train_fold = X.iloc[train_idx].copy(), y_encoded.iloc[train_idx]\n        x_valid_fold, y_valid_fold = X.iloc[valid_idx].copy(), y_encoded.iloc[valid_idx]\n\n        dtrain = xgb.DMatrix(x_train_fold, label=y_train_fold)\n        dvalid = xgb.DMatrix(x_valid_fold, label=y_valid_fold)\n        dtest = xgb.DMatrix(X_test)\n\n        params_model1 = {\n            'objective': 'multi:softprob',\n            'num_class': num_classes,\n            'max_depth': 16,\n            'learning_rate': 0.03,\n            'min_child_weight': 2,\n            'alpha': 0.8,\n            'reg_lambda': 4.0,\n            'colsample_bytree': 0.3,\n            'subsample': 0.8,\n            'max_bin': 128,\n            'colsample_bylevel': 1,\n            'colsample_bynode': 1,\n            'tree_method': 'hist',\n            'random_state': 42,\n            'eval_metric': 'mlogloss'\n        }\n\n        model1_instance = xgb.train(\n            params_model1,\n            dtrain,\n            num_boost_round=CFG.num_boost_round, # Use CFG value\n            evals=[(dvalid, 'valid')],\n            early_stopping_rounds=CFG.early_stopping_rounds, # Use CFG value\n            verbose_eval=CFG.verbose_eval\n        )\n\n        oof_preds_xgb_train_api1[valid_idx] = model1_instance.predict(dvalid, iteration_range=(0, model1_instance.best_iteration + 1))\n        model1_test_pred_sum += model1_instance.predict(dtest, iteration_range=(0, model1_instance.best_iteration + 1))\n\n        log_loss_value = log_loss(y_valid_fold, oof_preds_xgb_train_api1[valid_idx])\n        print(f\"Fold {i+1} log_loss: {log_loss_value:.4f}\")\n        model1_logloss_scores.append(log_loss_value)\n\n        del model1_instance, dtrain, dvalid, dtest\n        gc.collect()\n\n    test_preds_xgb_train_api1 = model1_test_pred_sum / FOLDS\n    avg_log_loss_model1 = np.mean(model1_logloss_scores)\n    print(f\"\\nModel 1 (xgb.train API 1) Final CV log_loss: {avg_log_loss_model1:.4f}\")\n    top_3_oof_preds_model1 = np.argsort(oof_preds_xgb_train_api1, axis=1)[:, -3:][:, ::-1]\n    map3_score_model1 = mapk(y_encoded.values, top_3_oof_preds_model1)\n    print(f\"Model 1 (xgb.train API 1) OOF MAP@3 Score: {map3_score_model1:.5f}\")\n\n    np.save(CFG.FNAME_XGB_API1_OOF, oof_preds_xgb_train_api1)\n    np.save(CFG.FNAME_XGB_API1_TEST, test_preds_xgb_train_api1)\n    print(f\"Saved Model 1 predictions to {CFG.OUTPUT_DIR}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T04:46:33.953796Z","iopub.execute_input":"2025-06-14T04:46:33.954067Z","execution_failed":"2025-06-14T04:46:42.374Z"}},"outputs":[{"name":"stdout","text":"\n--- Training Base Model 1: XGBoost (xgb.train API - Original Block 1) ---\n\n########## Fold 1 (Model 1) ##########\n[0]\tvalid-mlogloss:1.94564\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# --- Base Model 2: XGBoost (using XGBClassifier API - second block in original notebook) ---\nprint(\"\\n--- Training Base Model 2: XGBoost (XGBClassifier API) ---\")\nif os.path.exists(CFG.FNAME_XGB_SKLEARN_OOF) and os.path.exists(CFG.FNAME_XGB_SKLEARN_TEST):\n    oof_preds_xgb_sklearn = np.load(CFG.FNAME_XGB_SKLEARN_OOF)\n    test_preds_xgb_sklearn = np.load(CFG.FNAME_XGB_SKLEARN_TEST)\n    print(f\"Loaded existing predictions for Model 2 from {CFG.OUTPUT_DIR}. Skipping training.\")\nelse:\n    fixed_xgb_params_model2 = {\n        'max_depth': 12,\n        'colsample_bytree': 0.467,\n        'subsample': 0.86,\n        'n_estimators': 4000,\n        'learning_rate': 0.03,\n        'gamma': 0.26,\n        'max_delta_step': 4,\n        'reg_alpha': 2.7,\n        'reg_lambda': 1.4,\n        'objective': 'multi:softprob',\n        'random_state': 13,\n        'enable_categorical': True,\n        'tree_method': 'hist',\n        'early_stopping_rounds': 100,\n        'eval_metric': 'mlogloss',\n        'num_class': num_classes,\n        'n_jobs': -1\n    }\n    n_splits_cv_model2 = FOLDS # Use CFG.n_splits\n    model2_fold_scores = []\n    model2_test_pred_sum = np.zeros((len(X_test), num_classes))\n    kf_model2 = StratifiedKFold(n_splits=n_splits_cv_model2, shuffle=True, random_state=GLOBAL_RANDOM_STATE)\n\n    for fold, (train_idx, valid_idx) in enumerate(kf_model2.split(X, y_encoded)):\n        print(f\"--- Training Fold {fold+1} (Model 2) ---\")\n        fold_train_X, fold_valid_X = X.iloc[train_idx], X.iloc[valid_idx]\n        fold_train_y, fold_valid_y = y_encoded.iloc[train_idx], y_encoded.iloc[valid_idx]\n\n        model2_instance = XGBClassifier(**fixed_xgb_params_model2)\n        model2_instance.fit(fold_train_X, fold_train_y,\n                            eval_set=[(fold_valid_X, fold_valid_y)],\n                            verbose=False)\n\n        best_iteration = model2_instance.best_iteration if hasattr(model2_instance, 'best_iteration') else fixed_xgb_params_model2['n_estimators']\n        print(f\"Fold {fold+1} best iteration: {best_iteration}\")\n\n        oof_preds_xgb_sklearn[valid_idx] = model2_instance.predict_proba(fold_valid_X, iteration_range=(0, best_iteration))\n        model2_test_pred_sum += model2_instance.predict_proba(X_test, iteration_range=(0, best_iteration))\n\n        top_3_preds = np.argsort(oof_preds_xgb_sklearn[valid_idx], axis=1)[:, -3:][:, ::-1]\n        fold_score = mapk(fold_valid_y.values, top_3_preds)\n        model2_fold_scores.append(fold_score)\n        print(f\"Fold {fold+1} MAP@3: {fold_score:.4f}\")\n\n        del model2_instance\n        gc.collect()\n\n    test_preds_xgb_sklearn = model2_test_pred_sum / n_splits_cv_model2\n    avg_cv_score_model2 = np.mean(model2_fold_scores)\n    print(f\"\\nModel 2 (XGBClassifier) Average MAP@3 across {n_splits_cv_model2} folds: {avg_cv_score_model2:.4f}\")\n\n    np.save(CFG.FNAME_XGB_SKLEARN_OOF, oof_preds_xgb_sklearn)\n    np.save(CFG.FNAME_XGB_SKLEARN_TEST, test_preds_xgb_sklearn)\n    print(f\"Saved Model 2 predictions to {CFG.OUTPUT_DIR}\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-14T04:46:42.374Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Base Model 3: LightGBM Model ---\nprint(\"\\n--- Training Base Model 3: LightGBM ---\")\nif os.path.exists(CFG.FNAME_LGBM_OOF) and os.path.exists(CFG.FNAME_LGBM_TEST):\n    oof_preds_lgbm = np.load(CFG.FNAME_LGBM_OOF)\n    test_preds_lgbm = np.load(CFG.FNAME_LGBM_TEST)\n    print(f\"Loaded existing predictions for Model 3 from {CFG.OUTPUT_DIR}. Skipping training.\")\nelse:\n    fixed_lgbm_params = {\n        \"objective\": \"multiclass\",\n        \"num_class\": num_classes,\n        \"metric\": \"multi_logloss\",\n        \"boosting_type\": \"gbdt\",\n        \"n_estimators\": 1214,\n        \"learning_rate\": 0.064080948,\n        \"num_leaves\": 169,\n        \"max_depth\": 10,\n        \"subsample\": 0.642,\n        \"min_child_samples\": 19,\n        \"colsample_bytree\": 0.7,\n        \"reg_alpha\": 6.2941,\n        \"reg_lambda\": 5.556,\n        \"random_state\": GLOBAL_RANDOM_STATE,\n        \"n_jobs\": -1,\n        \"verbose\": -1,\n    }\n    n_splits_cv_model3 = FOLDS # Use CFG.n_splits\n    early_stopping_rounds_cv_model3 = 100\n    model3_fold_scores = []\n    model3_test_pred_sum = np.zeros((len(X_test), num_classes))\n    kf_model3 = StratifiedKFold(n_splits=n_splits_cv_model3, shuffle=True, random_state=GLOBAL_RANDOM_STATE)\n\n    for fold, (train_idx, valid_idx) in enumerate(kf_model3.split(X, y_encoded)):\n        print(f\"--- Training Fold {fold+1} (Model 3) ---\")\n        fold_train_X, fold_valid_X = X.iloc[train_idx], X.iloc[valid_idx]\n        fold_train_y, fold_valid_y = y_encoded.iloc[train_idx], y_encoded.iloc[valid_idx]\n\n        model3_instance = lgb.LGBMClassifier(**fixed_lgbm_params)\n\n        model3_instance.fit(fold_train_X, fold_train_y,\n                            eval_set=[(fold_valid_X, fold_valid_y)],\n                            callbacks=[lgb.early_stopping(early_stopping_rounds_cv_model3, verbose=False)])\n\n        best_iteration = model3_instance.best_iteration_ if hasattr(model3_instance, 'best_iteration_') else fixed_lgbm_params['n_estimators']\n        print(f\"Fold {fold+1} best iteration: {best_iteration}\")\n\n        oof_preds_lgbm[valid_idx] = model3_instance.predict_proba(fold_valid_X)\n        model3_test_pred_sum += model3_instance.predict_proba(X_test)\n\n        top_3_preds = np.argsort(oof_preds_lgbm[valid_idx], axis=1)[:, -3:][:, ::-1]\n        fold_score = mapk(fold_valid_y.values, top_3_preds)\n        model3_fold_scores.append(fold_score)\n        print(f\"Fold {fold+1} MAP@3: {fold_score:.4f}\")\n\n        del model3_instance\n        gc.collect()\n\n    test_preds_lgbm = model3_test_pred_sum / n_splits_cv_model3\n    avg_cv_score_model3 = np.mean(model3_fold_scores)\n    print(f\"\\nModel 3 (LightGBM) Average MAP@3 across {n_splits_cv_model3} folds: {avg_cv_score_model3:.4f}\")\n\n    np.save(CFG.FNAME_LGBM_OOF, oof_preds_lgbm)\n    np.save(CFG.FNAME_LGBM_TEST, test_preds_lgbm)\n    print(f\"Saved Model 3 predictions to {CFG.OUTPUT_DIR}\")\n\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-14T04:46:42.374Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Base Model 4: XGBoost (using xgb.train API with CFG parameters) ---\nprint(\"\\n--- Training Base Model 4: XGBoost (xgb.train API with CFG parameters) ---\")\nif os.path.exists(CFG.FNAME_XGB_API2_OOF) and os.path.exists(CFG.FNAME_XGB_API2_TEST):\n    oof_preds_xgb_train_api2 = np.load(CFG.FNAME_XGB_API2_OOF)\n    test_preds_xgb_train_api2 = np.load(CFG.FNAME_XGB_API2_TEST)\n    print(f\"Loaded existing predictions for Model 4 from {CFG.OUTPUT_DIR}. Skipping training.\")\nelse:\n    params_model4 = {\n        'objective': 'multi:softprob',\n        'num_class': num_classes,\n        'seed': CFG.seed,\n        'max_depth': 32,\n        'learning_rate': CFG.learning_rate,\n        'min_child_weight': 2,\n        'alpha': 5.6,\n        'reg_lambda': 0.06,\n        'subsample': 0.8,\n        'colsample_bytree': 0.3,\n        'colsample_bylevel': 1,\n        'colsample_bynode': 1,\n        'tree_method': 'hist',\n        'device': \"cuda\" if os.environ.get('ACCELERATOR_TYPE') == 'GPU' else 'cpu'\n    }\n    model4_fold_scores = []\n    model4_test_pred_sum = np.zeros((len(X_test), num_classes))\n    kf_model4 = StratifiedKFold(CFG.n_splits, shuffle=True, random_state=CFG.seed)\n\n    for fold, (trn_idx, val_idx) in enumerate(kf_model4.split(X, y_encoded)):\n        X_train_fold = X.iloc[trn_idx]\n        y_train_fold = y_encoded.iloc[trn_idx]\n        X_valid_fold = X.iloc[val_idx]\n        y_valid_fold = y_encoded.iloc[val_idx]\n\n        dtrain = xgb.DMatrix(X_train_fold, label=y_train_fold, enable_categorical=True)\n        dvalid = xgb.DMatrix(X_valid_fold, label=y_valid_fold, enable_categorical=True)\n        dtest = xgb.DMatrix(X_test, enable_categorical=True)\n\n        ES = xgb.callback.EarlyStopping(\n            rounds=CFG.early_stopping_rounds,\n            maximize=False,\n            save_best=True,\n        )\n\n        model4_instance = xgb.train(\n            params_model4,\n            dtrain,\n            num_boost_round=CFG.num_boost_round,\n            evals=[(dtrain, 'train'), (dvalid, 'validation')],\n            verbose_eval=CFG.verbose_eval,\n            callbacks=[ES]\n        )\n\n        oof_preds_xgb_train_api2[val_idx] = model4_instance.predict(dvalid, iteration_range=(0, model4_instance.best_iteration + 1))\n        model4_test_pred_sum += model4_instance.predict(dtest, iteration_range=(0, model4_instance.best_iteration + 1))\n\n        top3_preds = np.argsort(oof_preds_xgb_train_api2[val_idx], axis=1)[:, -3:][:, ::-1]\n        actual = [[label] for label in y_valid_fold.values]\n        map3_score_fold = mapk(actual, top3_preds)\n        model4_fold_scores.append(map3_score_fold)\n        print(\"----------------------------------------------------------------\")\n        print(f\"fold: {fold:02d}, map@3: {map3_score_fold:.6f}, best iteration: {model4_instance.best_iteration}, best score: {model4_instance.best_score: .6f}\\n\")\n\n        del model4_instance, dtrain, dvalid, dtest\n        gc.collect()\n\n    test_preds_xgb_train_api2 = model4_test_pred_sum / CFG.n_splits\n    avg_map3_score_model4 = np.mean(model4_fold_scores)\n    print(\"----------------------------------------------------------------\")\n    print(f\"Model 4 (xgb.train CFG) Average MAP@3: {avg_map3_score_model4:.6f}\")\n\n    np.save(CFG.FNAME_XGB_API2_OOF, oof_preds_xgb_train_api2)\n    np.save(CFG.FNAME_XGB_API2_TEST, test_preds_xgb_train_api2)\n    print(f\"Saved Model 4 predictions to {CFG.OUTPUT_DIR}\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-14T04:46:42.374Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- NEW Base Model 5: LightGBM (GOSS Boosting Type) ---\nprint(\"\\n--- Training Base Model 5: LightGBM (GOSS) ---\")\nif os.path.exists(CFG.FNAME_LGBM_GOSS_OOF) and os.path.exists(CFG.FNAME_LGBM_GOSS_TEST):\n    oof_preds_lgbm_goss = np.load(CFG.FNAME_LGBM_GOSS_OOF)\n    test_preds_lgbm_goss = np.load(CFG.FNAME_LGBM_GOSS_TEST)\n    print(f\"Loaded existing predictions for Model 5 from {CFG.OUTPUT_DIR}. Skipping training.\")\nelse:\n    lgbm_goss_params = {\n        \"objective\": \"multiclass\",\n        \"num_class\": num_classes,\n        \"metric\": \"multi_logloss\",\n        \"boosting_type\": \"goss\", # GOSS boosting type\n        \"colsample_bytree\": 0.39736332491996407,\n        \"learning_rate\": 0.008033740989500222,\n        \"min_child_samples\": 29,\n        \"min_child_weight\": 0.6732469853333759,\n        \"n_estimators\": 10000,\n        \"n_jobs\": -1,\n        \"num_leaves\": 89,\n        \"random_state\": GLOBAL_RANDOM_STATE,\n        \"reg_alpha\": 15.595856670965969,\n        \"reg_lambda\": 51.43625034648377,\n        \"subsample\": 0.07846482736630467,\n        \"verbose\": -1,\n    }\n    n_splits_cv_model5 = FOLDS # Use CFG.n_splits\n    early_stopping_rounds_cv_model5 = 100\n    model5_fold_scores = []\n    model5_test_pred_sum = np.zeros((len(X_test), num_classes))\n    kf_model5 = StratifiedKFold(n_splits=n_splits_cv_model5, shuffle=True, random_state=GLOBAL_RANDOM_STATE)\n\n    for fold, (train_idx, valid_idx) in enumerate(kf_model5.split(X, y_encoded)):\n        print(f\"--- Training Fold {fold+1} (Model 5) ---\")\n        fold_train_X, fold_valid_X = X.iloc[train_idx], X.iloc[valid_idx]\n        fold_train_y, fold_valid_y = y_encoded.iloc[train_idx], y_encoded.iloc[valid_idx]\n\n        model5_instance = lgb.LGBMClassifier(**lgbm_goss_params)\n\n        model5_instance.fit(fold_train_X, fold_train_y,\n                            eval_set=[(fold_valid_X, fold_valid_y)],\n                            callbacks=[lgb.early_stopping(early_stopping_rounds_cv_model5, verbose=False)])\n\n        best_iteration = model5_instance.best_iteration_ if hasattr(model5_instance, 'best_iteration_') else lgbm_goss_params['n_estimators']\n        print(f\"Fold {fold+1} best iteration: {best_iteration}\")\n\n        oof_preds_lgbm_goss[valid_idx] = model5_instance.predict_proba(fold_valid_X)\n        model5_test_pred_sum += model5_instance.predict_proba(X_test)\n\n        top_3_preds = np.argsort(oof_preds_lgbm_goss[valid_idx], axis=1)[:, -3:][:, ::-1]\n        fold_score = mapk(fold_valid_y.values, top_3_preds)\n        model5_fold_scores.append(fold_score)\n        print(f\"Fold {fold+1} MAP@3: {fold_score:.4f}\")\n\n        del model5_instance\n        gc.collect()\n\n    test_preds_lgbm_goss = model5_test_pred_sum / n_splits_cv_model5\n    avg_cv_score_model5 = np.mean(model5_fold_scores)\n    print(f\"\\nModel 5 (LightGBM GOSS) Average MAP@3 across {n_splits_cv_model5} folds: {avg_cv_score_model5:.4f}\")\n\n    np.save(CFG.FNAME_LGBM_GOSS_OOF, oof_preds_lgbm_goss)\n    np.save(CFG.FNAME_LGBM_GOSS_TEST, test_preds_lgbm_goss)\n    print(f\"Saved Model 5 predictions to {CFG.OUTPUT_DIR}\")\n\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-14T04:46:42.374Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Base Model 6: Logistic Regression (already added) ---\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Training Base Model 6: Logistic Regression ---\")\nstart_time_model6 = datetime.now()\nloaded_oof, loaded_test, loaded_from_disk = load_predictions_if_exist(CFG.FNAME_LR_OOF, CFG.FNAME_LR_TEST)\nif loaded_from_disk:\n    oof_preds_lr = loaded_oof\n    test_preds_lr = loaded_test\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Skipping training for Model 6.\")\nelse:\n    # --- IMPORTANT FIX: Initialize arrays if not loaded from disk ---\n    oof_preds_lr = np.zeros((len(X), num_classes))\n    test_preds_lr = np.zeros((len(X_test), num_classes))\n    # ---------------------------------------------------------------\n\n    lr_params = {\n        'C': 0.3691232889729139, 'fit_intercept': True, 'max_iter': 10000, 'random_state': 42,\n        'tol': 0.0021938847672756667, 'solver': \"liblinear\", 'penalty': \"l2\",\n        'multi_class': 'ovr', 'n_jobs': -1\n    }\n    n_splits_cv_model6 = FOLDS\n    model6_fold_scores = []\n    model6_test_pred_sum = np.zeros((len(X_test), num_classes)) # This is correctly initialized for the sum\n\n    kf_model6 = StratifiedKFold(n_splits=n_splits_cv_model6, shuffle=True, random_state=GLOBAL_RANDOM_STATE)\n\n    for fold, (train_idx, valid_idx) in enumerate(kf_model6.split(X, y_encoded)):\n        fold_start_time = datetime.now()\n        print(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Training Fold {fold+1}/{FOLDS} (Model 6: Logistic Regression) ---\")\n        fold_train_X, fold_valid_X = X.iloc[train_idx], X.iloc[valid_idx]\n        fold_train_y, fold_valid_y = y_encoded.iloc[train_idx], y_encoded.iloc[valid_idx]\n        model6_instance = LogisticRegression(**lr_params)\n        model6_instance.fit(fold_train_X, fold_train_y)\n        oof_preds_lr[valid_idx] = model6_instance.predict_proba(fold_valid_X)\n        model6_test_pred_sum += model6_instance.predict_proba(X_test)\n        fold_score = log_loss(fold_valid_y, oof_preds_lr[valid_idx])\n        model6_fold_scores.append(fold_score)\n        print(f\"[{datetime.now().strftime('%H:%M:%S')}] Fold {fold+1} Logistic Regression LogLoss: {fold_score:.4f}. Elapsed for fold: {datetime.now() - fold_start_time}\")\n        del model6_instance; gc.collect()\n    test_preds_lr = model6_test_pred_sum / n_splits_cv_model6\n    avg_cv_score_model6 = np.mean(model6_fold_scores)\n    print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] Model 6 (Logistic Regression) Average LogLoss across {n_splits_cv_model6} folds: {avg_cv_score_model6:.4f}\")\n    np.save(os.path.join(CFG.OUTPUT_DIR, CFG.FNAME_LR_OOF), oof_preds_lr)\n    np.save(os.path.join(CFG.OUTPUT_DIR, CFG.FNAME_LR_TEST), test_preds_lr)\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Saved Model 6 predictions to {CFG.OUTPUT_DIR}\")\nend_time_model6 = datetime.now()\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Model 6 Training Finished. Elapsed: {end_time_model6 - start_time_model6} ---\\n\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T09:51:32.633545Z","iopub.execute_input":"2025-06-14T09:51:32.634617Z","iopub.status.idle":"2025-06-14T09:52:57.908203Z","shell.execute_reply.started":"2025-06-14T09:51:32.634587Z","shell.execute_reply":"2025-06-14T09:52:57.907413Z"}},"outputs":[{"name":"stdout","text":"[09:51:32] --- Training Base Model 6: Logistic Regression ---\n[09:51:32] --- Training Fold 1/5 (Model 6: Logistic Regression) ---\n[09:51:49] Fold 1 Logistic Regression LogLoss: 1.9427. Elapsed for fold: 0:00:16.759779\n[09:51:50] --- Training Fold 2/5 (Model 6: Logistic Regression) ---\n[09:52:06] Fold 2 Logistic Regression LogLoss: 1.9427. Elapsed for fold: 0:00:16.309629\n[09:52:06] --- Training Fold 3/5 (Model 6: Logistic Regression) ---\n[09:52:23] Fold 3 Logistic Regression LogLoss: 1.9428. Elapsed for fold: 0:00:16.702962\n[09:52:23] --- Training Fold 4/5 (Model 6: Logistic Regression) ---\n[09:52:41] Fold 4 Logistic Regression LogLoss: 1.9425. Elapsed for fold: 0:00:17.571211\n[09:52:41] --- Training Fold 5/5 (Model 6: Logistic Regression) ---\n[09:52:57] Fold 5 Logistic Regression LogLoss: 1.9428. Elapsed for fold: 0:00:16.477340\n\n[09:52:57] Model 6 (Logistic Regression) Average LogLoss across 5 folds: 1.9427\n[09:52:57] Saved Model 6 predictions to /kaggle/working/outputs/\n[09:52:57] --- Model 6 Training Finished. Elapsed: 0:01:25.263252 ---\n\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# --- NEW Base Model 7: LightGBM (Optimized Hyperparameters) ---\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Training Base Model 7: LightGBM (Optimized Hyperparameters) ---\")\nstart_time_model7 = datetime.now()\n\nloaded_oof, loaded_test, loaded_from_disk = load_predictions_if_exist(CFG.FNAME_LGBM_OPTIMIZED_OOF, CFG.FNAME_LGBM_OPTIMIZED_TEST)\nif loaded_from_disk:\n    oof_preds_lgbm_optimized = loaded_oof\n    test_preds_lgbm_optimized = loaded_test\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Skipping training for Model 7.\")\nelse:\n    # --- IMPORTANT FIX: Initialize arrays if not loaded from disk ---\n    oof_preds_lgbm_optimized = np.zeros((len(X), num_classes))\n    test_preds_lgbm_optimized = np.zeros((len(X_test), num_classes))\n    # ---------------------------------------------------------------\n\n    lgbm_optimized_params = {\n        'n_estimators': 1446, 'num_leaves': 26, 'min_child_samples': 4,\n        'learning_rate': 0.14474325014552236, 'max_bin': 2**9 - 1, # log_max_bin 9 means 2^9-1\n        'colsample_bytree': 0.41898958941402753, 'reg_alpha': 0.02619349902619489,\n        'reg_lambda': 0.012988591514850135,\n        \"objective\": \"multiclass\", \"num_class\": num_classes, \"metric\": \"multi_logloss\",\n        \"boosting_type\": \"gbdt\", \"n_jobs\": -1, \"random_state\": GLOBAL_RANDOM_STATE, \"verbose\": -1,\n    }\n    n_splits_cv_model7 = FOLDS\n    early_stopping_rounds_cv_model7 = 100\n    model7_fold_scores = []\n    model7_test_pred_sum = np.zeros((len(X_test), num_classes))\n\n    for fold, (train_idx, valid_idx) in enumerate(kf_model7.split(X, y_encoded)):\n        fold_start_time = datetime.now()\n        print(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Training Fold {fold+1}/{FOLDS} (Model 7: LightGBM Optimized) ---\")\n        fold_train_X, fold_valid_X = X.iloc[train_idx], X.iloc[valid_idx]\n        fold_train_y, fold_valid_y = y_encoded.iloc[train_idx], y_encoded.iloc[valid_idx]\n\n        model7_instance = lgb.LGBMClassifier(**lgbm_optimized_params)\n        model7_instance.fit(fold_train_X, fold_train_y,\n                            eval_set=[(fold_valid_X, fold_valid_y)],\n                            callbacks=[lgb.early_stopping(early_stopping_rounds_cv_model7, verbose=False)])\n\n        best_iteration = model7_instance.best_iteration_ if hasattr(model7_instance, 'best_iteration_') else lgbm_optimized_params['n_estimators']\n        print(f\"[{datetime.now().strftime('%H:%M:%S')}] Fold {fold+1} best iteration: {best_iteration}\")\n\n        oof_preds_lgbm_optimized[valid_idx] = model7_instance.predict_proba(fold_valid_X)\n        model7_test_pred_sum += model7_instance.predict_proba(X_test)\n\n        top_3_preds = np.argsort(oof_preds_lgbm_optimized[valid_idx], axis=1)[:, -3:][:, ::-1]\n        fold_score = mapk(fold_valid_y.values, top_3_preds)\n        model7_fold_scores.append(fold_score)\n        print(f\"[{datetime.now().strftime('%H:%M:%S')}] Fold {fold+1} MAP@3: {fold_score:.4f}. Elapsed for fold: {datetime.now() - fold_start_time}\")\n\n        del model7_instance; gc.collect()\n\n    test_preds_lgbm_optimized = model7_test_pred_sum / n_splits_cv_model7\n    avg_cv_score_model7 = np.mean(model7_fold_scores)\n    print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] Model 7 (LightGBM Optimized) Average MAP@3 across {n_splits_cv_model7} folds: {avg_cv_score_model7:.4f}\")\n\n    np.save(os.path.join(CFG.OUTPUT_DIR, CFG.FNAME_LGBM_OPTIMIZED_OOF), oof_preds_lgbm_optimized)\n    np.save(os.path.join(CFG.OUTPUT_DIR, CFG.FNAME_LGBM_OPTIMIZED_TEST), test_preds_lgbm_optimized)\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Saved Model 7 predictions to {CFG.OUTPUT_DIR}\")\n\nend_time_model7 = datetime.now()\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Model 7 Training Finished. Elapsed: {end_time_model7 - start_time_model7} ---\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T10:13:46.147256Z","iopub.execute_input":"2025-06-14T10:13:46.148217Z","iopub.status.idle":"2025-06-14T10:42:26.475308Z","shell.execute_reply.started":"2025-06-14T10:13:46.148187Z","shell.execute_reply":"2025-06-14T10:42:26.474442Z"}},"outputs":[{"name":"stdout","text":"[10:13:46] --- Training Base Model 7: LightGBM (Optimized Hyperparameters) ---\n[10:13:46] --- Training Fold 1/5 (Model 7: LightGBM Optimized) ---\n[10:17:24] Fold 1 best iteration: 1446\n[10:19:29] Fold 1 MAP@3: 0.3543. Elapsed for fold: 0:05:42.819257\n[10:19:29] --- Training Fold 2/5 (Model 7: LightGBM Optimized) ---\n[10:23:14] Fold 2 best iteration: 1421\n[10:25:12] Fold 2 MAP@3: 0.3538. Elapsed for fold: 0:05:43.064845\n[10:25:12] --- Training Fold 3/5 (Model 7: LightGBM Optimized) ---\n[10:28:59] Fold 3 best iteration: 1441\n[10:31:05] Fold 3 MAP@3: 0.3537. Elapsed for fold: 0:05:52.618210\n[10:31:05] --- Training Fold 4/5 (Model 7: LightGBM Optimized) ---\n[10:34:44] Fold 4 best iteration: 1439\n[10:36:51] Fold 4 MAP@3: 0.3533. Elapsed for fold: 0:05:46.062700\n[10:36:51] --- Training Fold 5/5 (Model 7: LightGBM Optimized) ---\n[10:40:32] Fold 5 best iteration: 1444\n[10:42:26] Fold 5 MAP@3: 0.3513. Elapsed for fold: 0:05:34.685056\n\n[10:42:26] Model 7 (LightGBM Optimized) Average MAP@3 across 5 folds: 0.3533\n[10:42:26] Saved Model 7 predictions to /kaggle/working/outputs/\n[10:42:26] --- Model 7 Training Finished. Elapsed: 0:28:40.314931 ---\n\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# --- NEW Base Model 8: VotingClassifier ---\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Training Base Model 8: VotingClassifier ---\")\nstart_time_model8 = datetime.now()\n\nloaded_oof, loaded_test, loaded_from_disk = load_predictions_if_exist(CFG.FNAME_VOTING_OOF, CFG.FNAME_VOTING_TEST)\nif loaded_from_disk:\n    oof_preds_voting = loaded_oof\n    test_preds_voting = loaded_test\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Skipping training for Model 8.\")\nelse:\n    # --- IMPORTANT FIX: Initialize arrays if not loaded from disk ---\n    oof_preds_voting = np.zeros((len(X), num_classes))\n    test_preds_voting = np.zeros((len(X_test), num_classes))\n    # ---------------------------------------------------------------\n\n    n_splits_cv_model8 = FOLDS\n    model8_fold_scores = []\n    model8_test_pred_sum = np.zeros((len(X_test), num_classes))\n    kf_model8 = StratifiedKFold(n_splits=n_splits_cv_model8, shuffle=True, random_state=GLOBAL_RANDOM_STATE)\n\n    for fold, (train_idx, valid_idx) in enumerate(kf_model8.split(X, y_encoded)):\n        fold_start_time = datetime.now()\n        print(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Training Fold {fold+1}/{FOLDS} (Model 8: VotingClassifier) ---\")\n        fold_train_X, fold_valid_X = X.iloc[train_idx], X.iloc[valid_idx]\n        fold_train_y, fold_valid_y = y_encoded.iloc[train_idx], y_encoded.iloc[valid_idx]\n\n        # Define the internal estimators for the VotingClassifier for this fold.\n        # Using simplified parameters to keep them distinct from main base models but representative.\n        estimators_for_voting = [\n            ('xgb1', XGBClassifier(objective='multi:softprob', num_class=num_classes, n_estimators=500, learning_rate=0.05, max_depth=8, random_state=GLOBAL_RANDOM_STATE, n_jobs=-1, verbose=0, tree_method='hist')),\n            ('lgbm1', lgb.LGBMClassifier(objective='multiclass', num_class=num_classes, n_estimators=500, learning_rate=0.05, num_leaves=31, random_state=GLOBAL_RANDOM_STATE, n_jobs=-1, verbose=-1)),\n            ('xgb2', XGBClassifier(objective='multi:softprob', num_class=num_classes, n_estimators=300, learning_rate=0.08, max_depth=6, random_state=GLOBAL_RANDOM_STATE + 1, n_jobs=-1, verbose=0, tree_method='hist')),\n            ('lgbm_goss', lgb.LGBMClassifier(objective='multiclass', num_class=num_classes, boosting_type='goss', n_estimators=400, learning_rate=0.03, num_leaves=64, random_state=GLOBAL_RANDOM_STATE + 2, n_jobs=-1, verbose=-1)),\n            ('lr_vote', LogisticRegression(solver='liblinear', C=0.5, random_state=GLOBAL_RANDOM_STATE, n_jobs=-1, multi_class='ovr', max_iter=2000)) # A simple LR in the mix\n        ]\n        \n        model8_instance = VotingClassifier(estimators=estimators_for_voting, voting='soft', n_jobs=-1)\n        model8_instance.fit(fold_train_X, fold_train_y)\n\n        oof_preds_voting[valid_idx] = model8_instance.predict_proba(fold_valid_X)\n        model8_test_pred_sum += model8_instance.predict_proba(X_test)\n\n        top_3_preds = np.argsort(oof_preds_voting[valid_idx], axis=1)[:, -3:][:, ::-1]\n        fold_score = mapk(fold_valid_y.values, top_3_preds)\n        model8_fold_scores.append(fold_score)\n        print(f\"[{datetime.now().strftime('%H:%M:%S')}] Fold {fold+1} VotingClassifier MAP@3: {fold_score:.4f}. Elapsed for fold: {datetime.now() - fold_start_time}\")\n\n        del model8_instance; gc.collect()\n\n    test_preds_voting = model8_test_pred_sum / n_splits_cv_model8\n    avg_cv_score_model8 = np.mean(model8_fold_scores)\n    print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] Model 8 (VotingClassifier) Average MAP@3 across {n_splits_cv_model8} folds: {avg_cv_score_model8:.4f}\")\n\n    np.save(os.path.join(CFG.OUTPUT_DIR, CFG.FNAME_VOTING_OOF), oof_preds_voting)\n    np.save(os.path.join(CFG.OUTPUT_DIR, CFG.FNAME_VOTING_TEST), test_preds_voting)\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Saved Model 8 predictions to {CFG.OUTPUT_DIR}\")\n\nend_time_model8 = datetime.now()\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Model 8 Training Finished. Elapsed: {end_time_model8 - start_time_model8} ---\\n\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T11:50:29.167585Z","iopub.execute_input":"2025-06-14T11:50:29.167884Z","iopub.status.idle":"2025-06-14T14:38:43.985838Z","shell.execute_reply.started":"2025-06-14T11:50:29.167864Z","shell.execute_reply":"2025-06-14T14:38:43.984851Z"}},"outputs":[{"name":"stdout","text":"[11:50:29] --- Training Base Model 8: VotingClassifier ---\n[11:50:29] --- Training Fold 1/5 (Model 8: VotingClassifier) ---\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [11:50:32] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"verbose\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [11:50:32] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"verbose\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[12:22:17] Fold 1 VotingClassifier MAP@3: 0.3357. Elapsed for fold: 0:31:48.151599\n[12:22:17] --- Training Fold 2/5 (Model 8: VotingClassifier) ---\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [12:22:19] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"verbose\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [12:22:21] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"verbose\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[12:56:36] Fold 2 VotingClassifier MAP@3: 0.3342. Elapsed for fold: 0:34:19.297755\n[12:56:37] --- Training Fold 3/5 (Model 8: VotingClassifier) ---\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [12:56:38] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"verbose\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [12:56:41] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"verbose\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[13:31:46] Fold 3 VotingClassifier MAP@3: 0.3348. Elapsed for fold: 0:35:09.134209\n[13:31:46] --- Training Fold 4/5 (Model 8: VotingClassifier) ---\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [13:31:48] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"verbose\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [13:31:50] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"verbose\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[14:03:47] Fold 4 VotingClassifier MAP@3: 0.3343. Elapsed for fold: 0:32:01.144742\n[14:03:47] --- Training Fold 5/5 (Model 8: VotingClassifier) ---\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [14:03:49] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"verbose\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [14:03:52] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"verbose\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[14:38:43] Fold 5 VotingClassifier MAP@3: 0.3344. Elapsed for fold: 0:34:56.092369\n\n[14:38:43] Model 8 (VotingClassifier) Average MAP@3 across 5 folds: 0.3347\n[14:38:43] Saved Model 8 predictions to /kaggle/working/outputs/\n[14:38:43] --- Model 8 Training Finished. Elapsed: 2:48:14.806256 ---\n\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# --- NEW Base Model 6: XGBoost (Optimized Parameters) ---\nprint(\"\\n--- Training Base Model 6: XGBoost (Optimized Parameters) ---\")\noof_preds_xgb_optimized, test_preds_xgb_optimized, loaded_flag = load_predictions_if_exist(\n    CFG.FNAME_XGB_OPTIMIZED_OOF, CFG.FNAME_XGB_OPTIMIZED_TEST, X.shape, X_test.shape, num_classes\n)\nif loaded_flag:\n    print(f\"Loaded existing predictions for Model 6. Skipping training.\")\nelse:\n    print(f\"No existing predictions found for Model 6. Starting training.\")\n    # oof_preds_xgb_optimized and test_preds_xgb_optimized are already zero-initialized from load_predictions_if_exist\n    params_model6 = {\n        'objective': 'multi:softprob',\n        'num_class': num_classes, # Use `num_classes` as derived from `y_encoded`\n        'max_depth': 10,\n        'learning_rate': 0.03,\n        'min_child_weight' : 2,\n        'n_estimators': 10000, # Use as num_boost_round\n        'alpha': 0.8,\n        'reg_lambda': 4.0,\n        'colsample_bytree': 0.5,\n        'subsample': 0.7,\n        'max_bin': 128,\n        'colsample_bylevel': 1,\n        'colsample_bynode': 1,\n        'verbose': 0, # Suppress verbose output within xgb.train\n        'tree_method': 'hist',\n        'random_state': 42,\n        'eval_metric': 'mlogloss',\n    }\n    model6_fold_scores = []\n    model6_test_pred_sum = np.zeros((len(X_test), num_classes))\n    kf_model6 = StratifiedKFold(CFG.n_splits, shuffle=True, random_state=CFG.seed)\n\n    for fold, (trn_idx, val_idx) in enumerate(kf_model6.split(X, y_encoded)):\n        print(f\"\\n{'#'*10} Fold {fold+1} (Model 6) {'#'*10}\")\n        X_train_fold = X.iloc[trn_idx]\n        y_train_fold = y_encoded.iloc[trn_idx]\n        X_valid_fold = X.iloc[val_idx]\n        y_valid_fold = y_encoded.iloc[val_idx]\n\n        dtrain = xgb.DMatrix(X_train_fold, label=y_train_fold, enable_categorical=True)\n        dvalid = xgb.DMatrix(X_valid_fold, label=y_valid_fold, enable_categorical=True)\n        dtest = xgb.DMatrix(X_test, enable_categorical=True)\n\n        ES = xgb.callback.EarlyStopping(\n            rounds=CFG.early_stopping_rounds, # Use CFG value for early stopping\n            maximize=False,\n            save_best=True,\n        )\n\n        model6_instance = xgb.train(\n            params_model6,\n            dtrain,\n            num_boost_round=params_model6['n_estimators'], # Use n_estimators from params_model6 as num_boost_round\n            evals=[(dtrain, 'train'), (dvalid, 'validation')],\n            verbose_eval=CFG.verbose_eval, # Use CFG value for verbose_eval\n            callbacks=[ES]\n        )\n\n        oof_preds_xgb_optimized[val_idx] = model6_instance.predict(dvalid, iteration_range=(0, model6_instance.best_iteration + 1))\n        model6_test_pred_sum += model6_instance.predict(dtest, iteration_range=(0, model6_instance.best_iteration + 1))\n\n        top3_preds = np.argsort(oof_preds_xgb_optimized[val_idx], axis=1)[:, -3:][:, ::-1]\n        actual = [[label] for label in y_valid_fold.values]\n        map3_score_fold = mapk(actual, top3_preds)\n        model6_fold_scores.append(map3_score_fold)\n        print(\"----------------------------------------------------------------\")\n        print(f\"fold: {fold:02d}, map@3: {map3_score_fold:.6f}, best iteration: {model6_instance.best_iteration}, best score: {model6_instance.best_score: .6f}\\n\")\n\n        del model6_instance, dtrain, dvalid, dtest\n        gc.collect()\n\n    test_preds_xgb_optimized = model6_test_pred_sum / CFG.n_splits\n    avg_map3_score_model6 = np.mean(model6_fold_scores)\n    print(\"----------------------------------------------------------------\")\n    print(f\"Model 6 (xgb.train Optimized) Average MAP@3: {avg_map3_score_model6:.6f}\")\n\n    np.save(os.path.join(CFG.OUTPUT_DIR, CFG.FNAME_XGB_OPTIMIZED_OOF), oof_preds_xgb_optimized)\n    np.save(os.path.join(CFG.OUTPUT_DIR, CFG.FNAME_XGB_OPTIMIZED_TEST), test_preds_xgb_optimized)\n    print(f\"Saved Model 6 predictions to {CFG.OUTPUT_DIR}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T13:23:23.860033Z","iopub.execute_input":"2025-06-15T13:23:23.860378Z","iopub.status.idle":"2025-06-15T14:31:08.589933Z","shell.execute_reply.started":"2025-06-15T13:23:23.860349Z","shell.execute_reply":"2025-06-15T14:31:08.588798Z"}},"outputs":[{"name":"stdout","text":"\n--- Training Base Model 6: XGBoost (Optimized Parameters) ---\nNo existing predictions found for oof_preds_xgb_optimized.npy or test_preds_xgb_optimized.npy. Will initialize as zeros.\nNo existing predictions found for Model 6. Starting training.\n\n########## Fold 1 (Model 6) ##########\n[0]\ttrain-mlogloss:1.94531\tvalidation-mlogloss:1.94559\n[200]\ttrain-mlogloss:1.86911\tvalidation-mlogloss:1.91857\n[400]\ttrain-mlogloss:1.81506\tvalidation-mlogloss:1.90824\n[600]\ttrain-mlogloss:1.76771\tvalidation-mlogloss:1.90251\n[800]\ttrain-mlogloss:1.72459\tvalidation-mlogloss:1.89919\n[1000]\ttrain-mlogloss:1.68494\tvalidation-mlogloss:1.89745\n[1200]\ttrain-mlogloss:1.64787\tvalidation-mlogloss:1.89685\n[1242]\ttrain-mlogloss:1.64027\tvalidation-mlogloss:1.89686\n----------------------------------------------------------------\nfold: 00, map@3: 0.358741, best iteration: 1192, best score:  1.896845\n\n\n########## Fold 2 (Model 6) ##########\n[0]\ttrain-mlogloss:1.94531\tvalidation-mlogloss:1.94558\n[200]\ttrain-mlogloss:1.86841\tvalidation-mlogloss:1.91874\n[400]\ttrain-mlogloss:1.81462\tvalidation-mlogloss:1.90897\n[600]\ttrain-mlogloss:1.76700\tvalidation-mlogloss:1.90349\n[800]\ttrain-mlogloss:1.72385\tvalidation-mlogloss:1.90030\n[1000]\ttrain-mlogloss:1.68388\tvalidation-mlogloss:1.89866\n[1200]\ttrain-mlogloss:1.64691\tvalidation-mlogloss:1.89806\n[1311]\ttrain-mlogloss:1.62733\tvalidation-mlogloss:1.89809\n----------------------------------------------------------------\nfold: 01, map@3: 0.359025, best iteration: 1262, best score:  1.898019\n\n\n########## Fold 3 (Model 6) ##########\n[0]\ttrain-mlogloss:1.94531\tvalidation-mlogloss:1.94559\n[200]\ttrain-mlogloss:1.86809\tvalidation-mlogloss:1.91859\n[400]\ttrain-mlogloss:1.81435\tvalidation-mlogloss:1.90860\n[600]\ttrain-mlogloss:1.76743\tvalidation-mlogloss:1.90313\n[800]\ttrain-mlogloss:1.72420\tvalidation-mlogloss:1.90008\n[1000]\ttrain-mlogloss:1.68465\tvalidation-mlogloss:1.89846\n[1200]\ttrain-mlogloss:1.64757\tvalidation-mlogloss:1.89777\n[1240]\ttrain-mlogloss:1.64048\tvalidation-mlogloss:1.89781\n----------------------------------------------------------------\nfold: 02, map@3: 0.358004, best iteration: 1190, best score:  1.897762\n\n\n########## Fold 4 (Model 6) ##########\n[0]\ttrain-mlogloss:1.94530\tvalidation-mlogloss:1.94556\n[200]\ttrain-mlogloss:1.86895\tvalidation-mlogloss:1.91879\n[400]\ttrain-mlogloss:1.81464\tvalidation-mlogloss:1.90869\n[600]\ttrain-mlogloss:1.76756\tvalidation-mlogloss:1.90319\n[800]\ttrain-mlogloss:1.72437\tvalidation-mlogloss:1.89996\n[1000]\ttrain-mlogloss:1.68479\tvalidation-mlogloss:1.89829\n[1200]\ttrain-mlogloss:1.64795\tvalidation-mlogloss:1.89768\n[1381]\ttrain-mlogloss:1.61615\tvalidation-mlogloss:1.89771\n----------------------------------------------------------------\nfold: 03, map@3: 0.358592, best iteration: 1332, best score:  1.897605\n\n\n########## Fold 5 (Model 6) ##########\n[0]\ttrain-mlogloss:1.94530\tvalidation-mlogloss:1.94559\n[200]\ttrain-mlogloss:1.86866\tvalidation-mlogloss:1.91887\n[400]\ttrain-mlogloss:1.81442\tvalidation-mlogloss:1.90887\n[600]\ttrain-mlogloss:1.76718\tvalidation-mlogloss:1.90367\n[800]\ttrain-mlogloss:1.72401\tvalidation-mlogloss:1.90066\n[1000]\ttrain-mlogloss:1.68421\tvalidation-mlogloss:1.89922\n[1200]\ttrain-mlogloss:1.64713\tvalidation-mlogloss:1.89867\n[1266]\ttrain-mlogloss:1.63532\tvalidation-mlogloss:1.89869\n----------------------------------------------------------------\nfold: 04, map@3: 0.358101, best iteration: 1216, best score:  1.898635\n\n----------------------------------------------------------------\nModel 6 (xgb.train Optimized) Average MAP@3: 0.358493\nSaved Model 6 predictions to /kaggle/working/outputs/\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# --- NEW Base Model 7: XGBoost (Custom Parameters) ---\nprint(\"\\n--- Training Base Model 7: XGBoost (Custom Parameters) ---\")\noof_preds_xgb_custom, test_preds_xgb_custom, loaded_flag = load_predictions_if_exist(\n    CFG.FNAME_XGB_CUSTOM_OOF, CFG.FNAME_XGB_CUSTOM_TEST, X.shape, X_test.shape, num_classes\n)\nif loaded_flag:\n    print(f\"Loaded existing predictions for Model 7. Skipping training.\")\nelse:\n    print(f\"No existing predictions found for Model 7. Starting training.\")\n    # oof_preds_xgb_custom and test_preds_xgb_custom are already zero-initialized from load_predictions_if_exist\n    params_model7 = {\n        'objective': 'multi:softprob',\n        'num_class': num_classes, # Dynamically set num_class based on data\n        'max_depth': 16,\n        'learning_rate': 0.01,\n        'n_estimators': 100_000, # This will be passed as num_boost_round\n        'reg_alpha': 3,\n        'reg_lambda': 1.4,\n        'gamma': 0.26,\n        'max_delta_step': 5,\n        'subsample': 0.86,\n        'colsample_bytree': 0.4,\n        'min_child_weight': 5,\n        'random_state': 42,\n        'n_jobs': -1,\n        'eval_metric': 'mlogloss',\n        'enable_categorical': True,\n        'tree_method': 'hist', # Good for performance and categorical features\n    }\n    model7_fold_scores = []\n    model7_test_pred_sum = np.zeros((len(X_test), num_classes))\n    kf_model7 = StratifiedKFold(CFG.n_splits, shuffle=True, random_state=CFG.seed)\n\n    for fold, (trn_idx, val_idx) in enumerate(kf_model7.split(X, y_encoded)):\n        print(f\"\\n{'#'*10} Fold {fold+1} (Model 7) {'#'*10}\")\n        X_train_fold = X.iloc[trn_idx]\n        y_train_fold = y_encoded.iloc[trn_idx]\n        X_valid_fold = X.iloc[val_idx]\n        y_valid_fold = y_encoded.iloc[val_idx]\n\n        dtrain = xgb.DMatrix(X_train_fold, label=y_train_fold, enable_categorical=True)\n        dvalid = xgb.DMatrix(X_valid_fold, label=y_valid_fold, enable_categorical=True)\n        dtest = xgb.DMatrix(X_test, enable_categorical=True)\n\n        ES = xgb.callback.EarlyStopping(\n            rounds=CFG.early_stopping_rounds, # Use CFG value for early stopping\n            maximize=False,\n            save_best=True,\n        )\n\n        model7_instance = xgb.train(\n            params_model7,\n            dtrain,\n            num_boost_round=params_model7['n_estimators'], # Use n_estimators from params as num_boost_round\n            evals=[(dtrain, 'train'), (dvalid, 'validation')],\n            verbose_eval=CFG.verbose_eval, # Use CFG value for verbose_eval\n            callbacks=[ES]\n        )\n\n        oof_preds_xgb_custom[val_idx] = model7_instance.predict(dvalid, iteration_range=(0, model7_instance.best_iteration + 1))\n        model7_test_pred_sum += model7_instance.predict(dtest, iteration_range=(0, model7_instance.best_iteration + 1))\n\n        top3_preds = np.argsort(oof_preds_xgb_custom[val_idx], axis=1)[:, -3:][:, ::-1]\n        actual = [[label] for label in y_valid_fold.values]\n        map3_score_fold = mapk(actual, top3_preds)\n        model7_fold_scores.append(map3_score_fold)\n        print(\"----------------------------------------------------------------\")\n        print(f\"fold: {fold:02d}, map@3: {map3_score_fold:.6f}, best iteration: {model7_instance.best_iteration}, best score: {model7_instance.best_score: .6f}\\n\")\n\n        del model7_instance, dtrain, dvalid, dtest\n        gc.collect()\n\n    test_preds_xgb_custom = model7_test_pred_sum / CFG.n_splits\n    avg_map3_score_model7 = np.mean(model7_fold_scores)\n    print(\"----------------------------------------------------------------\")\n    print(f\"Model 7 (xgb.train Custom) Average MAP@3: {avg_map3_score_model7:.6f}\")\n\n    np.save(os.path.join(CFG.OUTPUT_DIR, CFG.FNAME_XGB_CUSTOM_OOF), oof_preds_xgb_custom)\n    np.save(os.path.join(CFG.OUTPUT_DIR, CFG.FNAME_XGB_CUSTOM_TEST), test_preds_xgb_custom)\n    print(f\"Saved Model 7 predictions to {CFG.OUTPUT_DIR}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T14:55:42.099746Z","iopub.execute_input":"2025-06-15T14:55:42.100148Z","iopub.status.idle":"2025-06-15T18:33:42.030376Z","shell.execute_reply.started":"2025-06-15T14:55:42.100124Z","shell.execute_reply":"2025-06-15T18:33:42.029254Z"}},"outputs":[{"name":"stdout","text":"\n--- Training Base Model 7: XGBoost (Custom Parameters) ---\nNo existing predictions found for oof_preds_xgb_custom.npy or test_preds_xgb_custom.npy. Will initialize as zeros.\nNo existing predictions found for Model 7. Starting training.\n\n########## Fold 1 (Model 7) ##########\n[0]\ttrain-mlogloss:1.94566\tvalidation-mlogloss:1.94578\n[200]\ttrain-mlogloss:1.90013\tvalidation-mlogloss:1.92738\n[400]\ttrain-mlogloss:1.86372\tvalidation-mlogloss:1.91591\n[600]\ttrain-mlogloss:1.83239\tvalidation-mlogloss:1.90780\n[800]\ttrain-mlogloss:1.80473\tvalidation-mlogloss:1.90192\n[1000]\ttrain-mlogloss:1.78030\tvalidation-mlogloss:1.89758\n[1200]\ttrain-mlogloss:1.75814\tvalidation-mlogloss:1.89416\n[1400]\ttrain-mlogloss:1.73840\tvalidation-mlogloss:1.89163\n[1600]\ttrain-mlogloss:1.72106\tvalidation-mlogloss:1.88976\n[1800]\ttrain-mlogloss:1.70679\tvalidation-mlogloss:1.88846\n[2000]\ttrain-mlogloss:1.69410\tvalidation-mlogloss:1.88753\n[2200]\ttrain-mlogloss:1.68313\tvalidation-mlogloss:1.88692\n[2400]\ttrain-mlogloss:1.67382\tvalidation-mlogloss:1.88644\n[2600]\ttrain-mlogloss:1.66536\tvalidation-mlogloss:1.88609\n[2800]\ttrain-mlogloss:1.65755\tvalidation-mlogloss:1.88584\n[3000]\ttrain-mlogloss:1.65043\tvalidation-mlogloss:1.88564\n[3200]\ttrain-mlogloss:1.64360\tvalidation-mlogloss:1.88552\n[3400]\ttrain-mlogloss:1.63739\tvalidation-mlogloss:1.88543\n[3494]\ttrain-mlogloss:1.63463\tvalidation-mlogloss:1.88542\n----------------------------------------------------------------\nfold: 00, map@3: 0.370313, best iteration: 3445, best score:  1.885414\n\n\n########## Fold 2 (Model 7) ##########\n[0]\ttrain-mlogloss:1.94566\tvalidation-mlogloss:1.94579\n[200]\ttrain-mlogloss:1.90003\tvalidation-mlogloss:1.92753\n[400]\ttrain-mlogloss:1.86361\tvalidation-mlogloss:1.91615\n[600]\ttrain-mlogloss:1.83226\tvalidation-mlogloss:1.90822\n[800]\ttrain-mlogloss:1.80449\tvalidation-mlogloss:1.90241\n[1000]\ttrain-mlogloss:1.77993\tvalidation-mlogloss:1.89813\n[1200]\ttrain-mlogloss:1.75760\tvalidation-mlogloss:1.89485\n[1400]\ttrain-mlogloss:1.73785\tvalidation-mlogloss:1.89239\n[1600]\ttrain-mlogloss:1.72014\tvalidation-mlogloss:1.89060\n[1800]\ttrain-mlogloss:1.70525\tvalidation-mlogloss:1.88930\n[2000]\ttrain-mlogloss:1.69261\tvalidation-mlogloss:1.88843\n[2200]\ttrain-mlogloss:1.68188\tvalidation-mlogloss:1.88784\n[2400]\ttrain-mlogloss:1.67239\tvalidation-mlogloss:1.88741\n[2600]\ttrain-mlogloss:1.66410\tvalidation-mlogloss:1.88712\n[2800]\ttrain-mlogloss:1.65653\tvalidation-mlogloss:1.88691\n[3000]\ttrain-mlogloss:1.64951\tvalidation-mlogloss:1.88675\n[3200]\ttrain-mlogloss:1.64315\tvalidation-mlogloss:1.88667\n[3400]\ttrain-mlogloss:1.63694\tvalidation-mlogloss:1.88663\n[3540]\ttrain-mlogloss:1.63263\tvalidation-mlogloss:1.88662\n----------------------------------------------------------------\nfold: 01, map@3: 0.369699, best iteration: 3491, best score:  1.886608\n\n\n########## Fold 3 (Model 7) ##########\n[0]\ttrain-mlogloss:1.94566\tvalidation-mlogloss:1.94578\n[200]\ttrain-mlogloss:1.89990\tvalidation-mlogloss:1.92735\n[400]\ttrain-mlogloss:1.86336\tvalidation-mlogloss:1.91579\n[600]\ttrain-mlogloss:1.83203\tvalidation-mlogloss:1.90767\n[800]\ttrain-mlogloss:1.80412\tvalidation-mlogloss:1.90176\n[1000]\ttrain-mlogloss:1.77968\tvalidation-mlogloss:1.89741\n[1200]\ttrain-mlogloss:1.75756\tvalidation-mlogloss:1.89408\n[1400]\ttrain-mlogloss:1.73779\tvalidation-mlogloss:1.89156\n[1600]\ttrain-mlogloss:1.72018\tvalidation-mlogloss:1.88973\n[1800]\ttrain-mlogloss:1.70590\tvalidation-mlogloss:1.88847\n[2000]\ttrain-mlogloss:1.69317\tvalidation-mlogloss:1.88760\n[2200]\ttrain-mlogloss:1.68213\tvalidation-mlogloss:1.88699\n[2400]\ttrain-mlogloss:1.67262\tvalidation-mlogloss:1.88657\n[2600]\ttrain-mlogloss:1.66421\tvalidation-mlogloss:1.88624\n[2800]\ttrain-mlogloss:1.65653\tvalidation-mlogloss:1.88601\n[3000]\ttrain-mlogloss:1.64938\tvalidation-mlogloss:1.88586\n[3200]\ttrain-mlogloss:1.64298\tvalidation-mlogloss:1.88576\n[3372]\ttrain-mlogloss:1.63739\tvalidation-mlogloss:1.88573\n----------------------------------------------------------------\nfold: 02, map@3: 0.370856, best iteration: 3322, best score:  1.885726\n\n\n########## Fold 4 (Model 7) ##########\n[0]\ttrain-mlogloss:1.94566\tvalidation-mlogloss:1.94578\n[200]\ttrain-mlogloss:1.90023\tvalidation-mlogloss:1.92747\n[400]\ttrain-mlogloss:1.86376\tvalidation-mlogloss:1.91606\n[600]\ttrain-mlogloss:1.83250\tvalidation-mlogloss:1.90801\n[800]\ttrain-mlogloss:1.80480\tvalidation-mlogloss:1.90217\n[1000]\ttrain-mlogloss:1.78043\tvalidation-mlogloss:1.89787\n[1200]\ttrain-mlogloss:1.75830\tvalidation-mlogloss:1.89458\n[1400]\ttrain-mlogloss:1.73835\tvalidation-mlogloss:1.89200\n[1600]\ttrain-mlogloss:1.72076\tvalidation-mlogloss:1.89022\n[1800]\ttrain-mlogloss:1.70644\tvalidation-mlogloss:1.88898\n[2000]\ttrain-mlogloss:1.69420\tvalidation-mlogloss:1.88812\n[2200]\ttrain-mlogloss:1.68326\tvalidation-mlogloss:1.88753\n[2400]\ttrain-mlogloss:1.67391\tvalidation-mlogloss:1.88711\n[2600]\ttrain-mlogloss:1.66552\tvalidation-mlogloss:1.88675\n[2800]\ttrain-mlogloss:1.65768\tvalidation-mlogloss:1.88651\n[3000]\ttrain-mlogloss:1.65072\tvalidation-mlogloss:1.88635\n[3200]\ttrain-mlogloss:1.64404\tvalidation-mlogloss:1.88623\n[3400]\ttrain-mlogloss:1.63775\tvalidation-mlogloss:1.88617\n[3493]\ttrain-mlogloss:1.63520\tvalidation-mlogloss:1.88617\n----------------------------------------------------------------\nfold: 03, map@3: 0.369616, best iteration: 3443, best score:  1.886160\n\n\n########## Fold 5 (Model 7) ##########\n[0]\ttrain-mlogloss:1.94566\tvalidation-mlogloss:1.94578\n[200]\ttrain-mlogloss:1.89981\tvalidation-mlogloss:1.92753\n[400]\ttrain-mlogloss:1.86316\tvalidation-mlogloss:1.91617\n[600]\ttrain-mlogloss:1.83172\tvalidation-mlogloss:1.90827\n[800]\ttrain-mlogloss:1.80389\tvalidation-mlogloss:1.90251\n[1000]\ttrain-mlogloss:1.77925\tvalidation-mlogloss:1.89832\n[1200]\ttrain-mlogloss:1.75715\tvalidation-mlogloss:1.89510\n[1400]\ttrain-mlogloss:1.73716\tvalidation-mlogloss:1.89267\n[1600]\ttrain-mlogloss:1.71968\tvalidation-mlogloss:1.89094\n[1800]\ttrain-mlogloss:1.70531\tvalidation-mlogloss:1.88976\n[2000]\ttrain-mlogloss:1.69267\tvalidation-mlogloss:1.88891\n[2200]\ttrain-mlogloss:1.68171\tvalidation-mlogloss:1.88835\n[2400]\ttrain-mlogloss:1.67244\tvalidation-mlogloss:1.88798\n[2600]\ttrain-mlogloss:1.66405\tvalidation-mlogloss:1.88769\n[2800]\ttrain-mlogloss:1.65607\tvalidation-mlogloss:1.88752\n[3000]\ttrain-mlogloss:1.64853\tvalidation-mlogloss:1.88736\n[3195]\ttrain-mlogloss:1.64210\tvalidation-mlogloss:1.88729\n----------------------------------------------------------------\nfold: 04, map@3: 0.369512, best iteration: 3145, best score:  1.887283\n\n----------------------------------------------------------------\nModel 7 (xgb.train Custom) Average MAP@3: 0.369999\nSaved Model 7 predictions to /kaggle/working/outputs/\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# --- NEW Base Model 11: Gaussian Naive Bayes ---\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Training Base Model 11: Gaussian Naive Bayes ---\")\nstart_time_model11 = datetime.now()\n\noof_preds_gaussian_nb, test_preds_gaussian_nb, loaded_from_disk = load_predictions_if_exist(\n    CFG.FNAME_GAUSSIAN_NB_OOF, CFG.FNAME_GAUSSIAN_NB_TEST, X.shape, X_test.shape, num_classes\n)\nif loaded_from_disk:\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Skipping training for Model 11.\")\nelse:\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] No existing predictions found for Model 11. Starting training.\")\n    # oof_preds_gaussian_nb and test_preds_gaussian_nb are already zero-initialized from load_predictions_if_exist\n    \n    n_splits_cv_model11 = FOLDS\n    model11_fold_scores = []\n    model11_test_pred_sum = np.zeros((len(X_test), num_classes))\n    kf_model11 = StratifiedKFold(n_splits=n_splits_cv_model11, shuffle=True, random_state=GLOBAL_RANDOM_STATE)\n\n    for fold, (train_idx, valid_idx) in enumerate(kf_model11.split(X, y_encoded)):\n        print(f\"--- Training Fold {fold+1} (Model 11: Gaussian Naive Bayes) ---\")\n        fold_train_X, fold_valid_X = X.iloc[train_idx], X.iloc[valid_idx]\n        fold_train_y, fold_valid_y = y_encoded.iloc[train_idx], y_encoded.iloc[valid_idx]\n\n        model11_instance = GaussianNB(var_smoothing=1e-9)\n        model11_instance.fit(fold_train_X, fold_train_y)\n\n        # Naive Bayes predict_proba outputs probabilities\n        oof_preds_gaussian_nb[valid_idx] = model11_instance.predict_proba(fold_valid_X)\n        model11_test_pred_sum += model11_instance.predict_proba(X_test)\n\n        top_3_preds = np.argsort(oof_preds_gaussian_nb[valid_idx], axis=1)[:, -3:][:, ::-1]\n        fold_score = mapk(fold_valid_y.values, top_3_preds)\n        model11_fold_scores.append(fold_score)\n        print(f\"Fold {fold+1} MAP@3: {fold_score:.4f}\")\n\n        del model11_instance\n        gc.collect()\n\n    test_preds_gaussian_nb = model11_test_pred_sum / n_splits_cv_model11\n    avg_cv_score_model11 = np.mean(model11_fold_scores)\n    print(f\"\\nModel 11 (Gaussian Naive Bayes) Average MAP@3 across {n_splits_cv_model11} folds: {avg_cv_score_model11:.4f}\")\n\n    np.save(os.path.join(CFG.OUTPUT_DIR, CFG.FNAME_GAUSSIAN_NB_OOF), oof_preds_gaussian_nb)\n    np.save(os.path.join(CFG.OUTPUT_DIR, CFG.FNAME_GAUSSIAN_NB_TEST), test_preds_gaussian_nb)\n    print(f\"Saved Model 11 predictions to {CFG.OUTPUT_DIR}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T17:38:58.256127Z","iopub.execute_input":"2025-06-16T17:38:58.256479Z","iopub.status.idle":"2025-06-16T17:39:05.508127Z","shell.execute_reply.started":"2025-06-16T17:38:58.256455Z","shell.execute_reply":"2025-06-16T17:39:05.507272Z"}},"outputs":[{"name":"stdout","text":"[17:38:58] --- Training Base Model 11: Gaussian Naive Bayes ---\nNo existing predictions found for oof_preds_gaussian_nb.npy or test_preds_gaussian_nb.npy. Will initialize as zeros.\n[17:38:58] No existing predictions found for Model 11. Starting training.\n--- Training Fold 1 (Model 11: Gaussian Naive Bayes) ---\nFold 1 MAP@3: 0.2835\n--- Training Fold 2 (Model 11: Gaussian Naive Bayes) ---\nFold 2 MAP@3: 0.2832\n--- Training Fold 3 (Model 11: Gaussian Naive Bayes) ---\nFold 3 MAP@3: 0.2819\n--- Training Fold 4 (Model 11: Gaussian Naive Bayes) ---\nFold 4 MAP@3: 0.2837\n--- Training Fold 5 (Model 11: Gaussian Naive Bayes) ---\nFold 5 MAP@3: 0.2823\n\nModel 11 (Gaussian Naive Bayes) Average MAP@3 across 5 folds: 0.2829\nSaved Model 11 predictions to /kaggle/working/outputs/\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# --- NEW Base Model 12: K-Nearest Neighbors (KNN) with Hyperparameter Tuning ---\nprint(f\"[{datetime.now().strftime('%H:%M:%S')}] --- Training Base Model 12: K-Nearest Neighbors (KNN) ---\")\nstart_time_model12 = datetime.now()\n\noof_preds_knn_base, test_preds_knn_base, loaded_from_disk = load_predictions_if_exist(\n    CFG.FNAME_KNN_OOF, CFG.FNAME_KNN_TEST, X.shape, X_test.shape, num_classes\n)\nif loaded_from_disk:\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Skipping training for Model 12.\")\nelse:\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] No existing predictions found for Model 12. Starting training.\")\n    \n    n_splits_cv_model12 = FOLDS\n    model12_fold_scores = []\n    model12_test_pred_sum = np.zeros((len(X_test), num_classes))\n    kf_model12 = StratifiedKFold(n_splits=n_splits_cv_model12, shuffle=True, random_state=GLOBAL_RANDOM_STATE)\n\n    # Define hyperparameter grid for KNN tuning within each fold\n    # This range is kept small for practical execution within an ensemble\n    knn_param_grid = {\n        'n_neighbors': [3, 5, 7, 9], # Common values for k\n        'weights': ['uniform', 'distance'], # How to weight neighbors\n        'metric': ['euclidean', 'manhattan'] # Distance metrics\n    }\n\n    for fold, (train_idx, valid_idx) in enumerate(kf_model12.split(X, y_encoded)):\n        print(f\"--- Training Fold {fold+1} (Model 12: KNN) ---\")\n        \n        X_train_fold, y_train_fold = X.iloc[train_idx], y_encoded.iloc[train_idx]\n        X_valid_fold, y_valid_fold = X.iloc[valid_idx], y_encoded.iloc[valid_idx]\n        \n        # Apply StandardScaler specifically to numerical columns for KNN\n        scaler_knn = StandardScaler()\n        X_train_scaled_knn = X_train_fold.copy()\n        X_valid_scaled_knn = X_valid_fold.copy()\n        X_test_scaled_knn = X_test.copy()\n\n        # Scale only the numerical columns. KNN is highly sensitive to scaling.\n        X_train_scaled_knn[numerical_cols] = scaler_knn.fit_transform(X_train_scaled_knn[numerical_cols])\n        X_valid_scaled_knn[numerical_cols] = scaler_knn.transform(X_valid_scaled_knn[numerical_cols])\n        X_test_scaled_knn[numerical_cols] = scaler_knn.transform(X_test_scaled_knn[numerical_cols])\n\n        # Perform GridSearchCV for hyperparameter tuning for KNN in this fold\n        grid_search_knn = GridSearchCV(\n            estimator=KNeighborsClassifier(),\n            param_grid=knn_param_grid,\n            scoring='log_loss', # Use log_loss as evaluation metric for probability models\n            cv=3, # Smaller internal CV for speed, can be increased if needed\n            n_jobs=-1, # Use all available cores\n            verbose=0 # Suppress verbose output from GridSearchCV\n        )\n        \n        grid_search_knn.fit(X_train_scaled_knn, y_train_fold)\n        \n        best_knn_model = grid_search_knn.best_estimator_\n        print(f\"  Fold {fold+1} KNN Best Params: {grid_search_knn.best_params_}\")\n        print(f\"  Fold {fold+1} KNN Best CV LogLoss: {-grid_search_knn.best_score_:.4f}\") # GridSearchCV returns negative score for loss metrics\n\n        # Predict probabilities\n        oof_preds_knn_base[valid_idx] = best_knn_model.predict_proba(X_valid_scaled_knn)\n        model12_test_pred_sum += best_knn_model.predict_proba(X_test_scaled_knn)\n\n        top_3_preds = np.argsort(oof_preds_knn_base[valid_idx], axis=1)[:, -3:][:, ::-1]\n        fold_score = mapk(y_valid_fold.values, top_3_preds)\n        model12_fold_scores.append(fold_score)\n        print(f\"Fold {fold+1} MAP@3: {fold_score:.4f}\")\n\n        del scaler_knn, grid_search_knn, best_knn_model\n        gc.collect()\n\n    test_preds_knn_base = model12_test_pred_sum / n_splits_cv_model12\n    avg_cv_score_model12 = np.mean(model12_fold_scores)\n    print(f\"\\nModel 12 (KNN Base) Average MAP@3 across {n_splits_cv_model12} folds: {avg_cv_score_model12:.4f}\")\n\n    np.save(os.path.join(CFG.OUTPUT_DIR, CFG.FNAME_KNN_OOF), oof_preds_knn_base)\n    np.save(os.path.join(CFG.OUTPUT_DIR, CFG.FNAME_KNN_TEST), test_preds_knn_base)\n    print(f\"Saved Model 12 predictions to {CFG.OUTPUT_DIR}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T10:01:20.725862Z","iopub.execute_input":"2025-06-17T10:01:20.726174Z","iopub.status.idle":"2025-06-17T10:01:21.203181Z","shell.execute_reply.started":"2025-06-17T10:01:20.726150Z","shell.execute_reply":"2025-06-17T10:01:21.201986Z"}},"outputs":[{"name":"stdout","text":"[10:01:20] --- Training Base Model 12: K-Nearest Neighbors (KNN) ---\nNo existing predictions found for oof_preds_knn_base.npy or test_preds_knn_base.npy. Will initialize as zeros.\n[10:01:20] No existing predictions found for Model 12. Starting training.\n--- Training Fold 1 (Model 12: KNN) ---\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_scorer.py\u001b[0m in \u001b[0;36mget_scorer\u001b[0;34m(scoring)\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_SCORERS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'log_loss'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/2240834915.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m         )\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mgrid_search_knn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaled_knn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mbest_knn_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search_knn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    774\u001b[0m             \u001b[0mscorers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscoring\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m             \u001b[0mscorers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    777\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0mscorers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_multimetric_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_scorer.py\u001b[0m in \u001b[0;36mcheck_scoring\u001b[0;34m(estimator, scoring, allow_none)\u001b[0m\n\u001b[1;32m    477\u001b[0m         )\n\u001b[1;32m    478\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_scorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    480\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0;31m# Heuristic to ensure user has not passed a metric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_scorer.py\u001b[0m in \u001b[0;36mget_scorer\u001b[0;34m(scoring)\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_SCORERS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    433\u001b[0m                 \u001b[0;34m\"%r is not a valid scoring value. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m                 \u001b[0;34m\"Use sklearn.metrics.get_scorer_names() \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: 'log_loss' is not a valid scoring value. Use sklearn.metrics.get_scorer_names() to get valid options."],"ename":"ValueError","evalue":"'log_loss' is not a valid scoring value. Use sklearn.metrics.get_scorer_names() to get valid options.","output_type":"error"}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
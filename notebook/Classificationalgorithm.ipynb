{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "8JdH1IWGHrFh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as mt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_classification\n",
        "from collections import Counter\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import multiprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('diabetes.csv')\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzcGeZe6LKKz",
        "outputId": "65c7ea4b-5683-46c8-a294-9dfa99501c69"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
            "0            6      148             72             35        0  33.6   \n",
            "1            1       85             66             29        0  26.6   \n",
            "2            8      183             64              0        0  23.3   \n",
            "3            1       89             66             23       94  28.1   \n",
            "4            0      137             40             35      168  43.1   \n",
            "\n",
            "   DiabetesPedigreeFunction  Age  Outcome  \n",
            "0                     0.627   50        1  \n",
            "1                     0.351   31        0  \n",
            "2                     0.672   32        1  \n",
            "3                     0.167   21        0  \n",
            "4                     2.288   33        1  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegressionScratch:\n",
        "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iterations = n_iterations\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def _sigmoid(self, z):\n",
        "        \"\"\"Sigmoid activation function.\"\"\"\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Train the Logistic Regression model.\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        for _ in range(self.n_iterations):\n",
        "            # Linear model\n",
        "            linear_model = np.dot(X, self.weights) + self.bias\n",
        "            # Apply sigmoid function\n",
        "            predictions = self._sigmoid(linear_model)\n",
        "\n",
        "            # Compute gradients\n",
        "            dw = (1 / n_samples) * np.dot(X.T, (predictions - y))\n",
        "            db = (1 / n_samples) * np.sum(predictions - y)\n",
        "\n",
        "            # Update weights and bias\n",
        "            self.weights -= self.learning_rate * dw\n",
        "            self.bias -= self.learning_rate * db\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"Return predicted probabilities.\"\"\"\n",
        "        linear_model = np.dot(X, self.weights) + self.bias\n",
        "        return self._sigmoid(linear_model)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Return class predictions.\"\"\"\n",
        "        probabilities = self.predict_proba(X)\n",
        "        return (probabilities >= 0.5).astype(int)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Generate synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=8, n_classes=2, random_state=42)\n",
        "df = pd.DataFrame(X, columns=['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI','DiabetesPedigreeFunction','Age'])\n",
        "df['Outcome'] = y\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:, :-1].values, df['Outcome'].values, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegressionScratch(learning_rate=0.1, n_iterations=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate model\n",
        "accuracy = np.mean(y_pred == y_test) * 100\n",
        "print(f\"Model Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYVuYsb0Tqdx",
        "outputId": "70848c3a-4262-4b9b-e870-602e8e30c72b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 87.50%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class KNN:\n",
        "    def __init__(self, k=4):\n",
        "        self.k = k\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Store the training data.\"\"\"\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "\n",
        "    def _euclidean_distance(self, x1, x2):\n",
        "        \"\"\"Calculate the Euclidean distance between two points.\"\"\"\n",
        "        return np.sqrt(np.sum((x1 - x2) ** 2))\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict the class for each sample in X.\"\"\"\n",
        "        predictions = [self._predict_single(x) for x in X]\n",
        "        return np.array(predictions)\n",
        "\n",
        "    def _predict_single(self, x):\n",
        "        \"\"\"Predict the class for a single sample.\"\"\"\n",
        "        # Compute distances between x and all training samples\n",
        "        distances = [self._euclidean_distance(x, x_train) for x_train in self.X_train]\n",
        "\n",
        "        # Sort by distance and get the indices of k nearest neighbors\n",
        "        k_indices = np.argsort(distances)[:self.k]\n",
        "\n",
        "        # Retrieve the labels of the k nearest neighbors\n",
        "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
        "\n",
        "        # Return the most common class label\n",
        "        most_common = Counter(k_nearest_labels).most_common(1)\n",
        "        return most_common[0][0]\n",
        "\n",
        "\n",
        "# Create a dataset with 2 classes\n",
        "X, y = make_classification(n_samples=100, n_features=8, n_classes=2, random_state=42)\n",
        "df = pd.DataFrame(X, columns=['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI','DiabetesPedigreeFunction','Age'])\n",
        "df['Outcome'] = y\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:, :-1].values, df['Outcome'].values, test_size=0.2, random_state=42)\n",
        "\n",
        "# Instantiate and train the KNN model\n",
        "knn = KNN()\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"KNN Model Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwBXz3pFVCYF",
        "outputId": "c47d883b-cc94-4238-989a-5d67bf26ffc2"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN Model Accuracy: 0.90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "skf = StratifiedKFold(n_splits=5)\n",
        "accuracies = []\n",
        "\n",
        "for train_index, test_index in skf.split(X, y):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    knn = KNN()\n",
        "    knn.fit(X_train, y_train)\n",
        "    y_pred = knn.predict(X_test)\n",
        "    accuracies.append(np.mean(y_pred == y_test))\n",
        "\n",
        "print(f\"Cross-validated Accuracy: {np.mean(accuracies):.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsBiL8_IXTsU",
        "outputId": "9a3638aa-3939-44bd-fda5-f95e3d4689ff"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validated Accuracy: 0.95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#implementing naive bayes\n",
        "class NaiveBayes:\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Fit the Naive Bayes model.\"\"\"\n",
        "        self.classes = np.unique(y)\n",
        "        self.priors = {c: np.mean(y == c) for c in self.classes}\n",
        "        self.likelihoods = {}\n",
        "\n",
        "        for c in self.classes:\n",
        "            X_c = X[y == c]\n",
        "            self.likelihoods[c] = {\n",
        "                \"mean\": X_c.mean(axis=0),\n",
        "                \"var\": X_c.var(axis=0),\n",
        "            }\n",
        "\n",
        "    def _gaussian_probability(self, x, mean, var):\n",
        "        \"\"\"Calculate Gaussian probability density.\"\"\"\n",
        "        eps = 1e-6  # To avoid division by zero\n",
        "        coeff = 1 / np.sqrt(2 * np.pi * (var + eps))\n",
        "        exponent = np.exp(-((x - mean) ** 2) / (2 * (var + eps)))\n",
        "        return coeff * exponent\n",
        "\n",
        "    def _calculate_posterior(self, x):\n",
        "        \"\"\"Calculate posterior probabilities for each class.\"\"\"\n",
        "        posteriors = {}\n",
        "        for c in self.classes:\n",
        "            prior = self.priors[c]\n",
        "            likelihood = np.prod(\n",
        "                self._gaussian_probability(x, self.likelihoods[c][\"mean\"], self.likelihoods[c][\"var\"])\n",
        "            )\n",
        "            posteriors[c] = prior * likelihood\n",
        "        return posteriors\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict the class labels.\"\"\"\n",
        "        predictions = []\n",
        "        for x in X:\n",
        "            posteriors = self._calculate_posterior(x)\n",
        "            predictions.append(max(posteriors, key=posteriors.get))\n",
        "        return np.array(predictions)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X, y = make_classification(n_samples=500, n_features=8, n_classes=2, random_state=42)\n",
        "df = pd.DataFrame(X, columns=['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI','DiabetesPedigreeFunction','Age'])\n",
        "df[\"Outcome\"] = y\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df.iloc[:, :-1].values, df[\"Outcome\"].values, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train the Naive Bayes model\n",
        "nb = NaiveBayes()\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = nb.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Naive Bayes Model Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlf1ufZeZviB",
        "outputId": "d2a959d0-01b6-437f-87de-629ee11d4031"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes Model Accuracy: 0.91\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from collections import Counter\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "class AdvancedXGBoost:\n",
        "    def __init__(self, learning_rate=0.1, n_estimators=100, max_depth=3, gamma=0, lambda_=1, alpha=0):\n",
        "        self.learning_rate = learning_rate  # Learning rate\n",
        "        self.n_estimators = n_estimators  # Number of trees\n",
        "        self.max_depth = max_depth  # Depth of decision trees\n",
        "        self.gamma = gamma  # Regularization term for tree complexity\n",
        "        self.lambda_ = lambda_  # L2 regularization on leaf weights (Ridge)\n",
        "        self.alpha = alpha  # L1 regularization on leaf weights (Lasso)\n",
        "        self.models = []  # List to store individual trees\n",
        "        self.y_mean = None  # Initial mean value\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Fit the model to the training data.\"\"\"\n",
        "        # Initialize predictions with a simple constant value (mean of target values)\n",
        "        self.y_mean = np.mean(y)\n",
        "        predictions = np.full_like(y, self.y_mean, dtype=np.float32)\n",
        "\n",
        "        for _ in range(self.n_estimators):\n",
        "            # Compute residuals (difference between actual and predicted)\n",
        "            residuals = y - predictions\n",
        "\n",
        "            # Train a decision tree on the residuals\n",
        "            model = DecisionTreeClassifier(max_depth=self.max_depth)\n",
        "            model.fit(X, residuals)\n",
        "\n",
        "            # Predict the residuals with the trained tree\n",
        "            residual_preds = model.predict(X)\n",
        "\n",
        "            # Regularization: Modify the residual predictions with the regularization terms\n",
        "            residual_preds = self._apply_regularization(residual_preds)\n",
        "\n",
        "            # Update the model's predictions\n",
        "            predictions += self.learning_rate * residual_preds\n",
        "\n",
        "            # Save the model\n",
        "            self.models.append(model)\n",
        "\n",
        "    def _apply_regularization(self, residual_preds):\n",
        "        \"\"\"Apply L1 and L2 regularization on the leaf values.\"\"\"\n",
        "        # L1 regularization (Lasso)\n",
        "        residual_preds = np.sign(residual_preds) * np.maximum(np.abs(residual_preds) - self.alpha, 0)\n",
        "\n",
        "        # L2 regularization (Ridge)\n",
        "        residual_preds -= self.lambda_ * residual_preds\n",
        "        return residual_preds\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Make predictions using the fitted model.\"\"\"\n",
        "        predictions = np.full(X.shape[0], self.y_mean, dtype=np.float32)\n",
        "\n",
        "        # Add the predictions from each tree (scaled by the learning rate)\n",
        "        for model in self.models:\n",
        "            residual_preds = model.predict(X)\n",
        "            residual_preds = self._apply_regularization(residual_preds)\n",
        "            predictions += self.learning_rate * residual_preds\n",
        "\n",
        "        return np.round(predictions).astype(int)  # Round to get binary class predictions\n",
        "\n",
        "    def _parallel_tree_building(self, X, y, start_idx, end_idx):\n",
        "        \"\"\"Parallelize tree building (simulated)\"\"\"\n",
        "        model = DecisionTreeClassifier(max_depth=self.max_depth)\n",
        "        model.fit(X[start_idx:end_idx], y[start_idx:end_idx])\n",
        "        return model\n",
        "\n",
        "    def fit_parallel(self, X, y):\n",
        "        \"\"\"Fit the model to the training data using parallelism.\"\"\"\n",
        "        # Initialize predictions with a simple constant value (mean of target values)\n",
        "        self.y_mean = np.mean(y)\n",
        "        predictions = np.full_like(y, self.y_mean, dtype=np.float32)\n",
        "\n",
        "        # Use parallelism to fit trees\n",
        "        chunk_size = len(X) // self.n_estimators\n",
        "        results = Parallel(n_jobs=-1)(delayed(self._parallel_tree_building)(X, y, i*chunk_size, (i+1)*chunk_size) for i in range(self.n_estimators))\n",
        "\n",
        "        for model in results:\n",
        "            # Compute residuals (difference between actual and predicted)\n",
        "            residuals = y - predictions\n",
        "\n",
        "            # Predict the residuals with the trained tree\n",
        "            residual_preds = model.predict(X)\n",
        "\n",
        "            # Regularization: Modify the residual predictions with the regularization terms\n",
        "            residual_preds = self._apply_regularization(residual_preds)\n",
        "\n",
        "            # Update the model's predictions\n",
        "            predictions += self.learning_rate * residual_preds\n",
        "\n",
        "            # Save the model\n",
        "            self.models.append(model)\n",
        "\n",
        "\n",
        "# Generate synthetic dataset for classification\n",
        "X, y = make_classification(n_samples=500, n_features=8, n_classes=2, random_state=42)\n",
        "df = pd.DataFrame(X, columns=['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI','DiabetesPedigreeFunction','Age'])\n",
        "df[\"Outcome\"] = y\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:, :-1].values, df[\"Outcome\"].values, test_size=0.2, random_state=42)\n",
        "\n",
        "# Ensure y_train is an integer type\n",
        "y_train = y_train.astype(int)\n",
        "y_test = y_test.astype(int)\n",
        "\n",
        "# Instantiate and train the Advanced XGBoost model with regularization and parallelism\n",
        "model = AdvancedXGBoost(learning_rate=0.1, n_estimators=100, max_depth=3, gamma=0, lambda_=1, alpha=0)\n",
        "model.fit_parallel(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Advanced XGBoost Model Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tzgbn0ANb0nV",
        "outputId": "a83134cb-8fc1-476b-96fb-29aa937056ae"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Advanced XGBoost Model Accuracy: 0.49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NQdeo8FJdmcc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
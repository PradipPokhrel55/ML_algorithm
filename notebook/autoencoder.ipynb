{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1rGEThzdigH",
        "outputId": "bf0232af-9f25-4e3e-b3eb-dda88b307783"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         x1        x2\n",
            "0  0.496714 -0.138264\n",
            "1  0.647689  1.523030\n",
            "2 -0.234153 -0.234137\n",
            "3  1.579213  0.767435\n",
            "4 -0.469474  0.542560\n",
            "Epoch 0, Loss: 2.3696, Reconstruction Loss: 2.3692, KL Loss: 0.0004\n",
            "Epoch 100, Loss: 2.1065, Reconstruction Loss: 2.1057, KL Loss: 0.0008\n",
            "Epoch 200, Loss: 2.0183, Reconstruction Loss: 2.0158, KL Loss: 0.0024\n",
            "Epoch 300, Loss: 1.9871, Reconstruction Loss: 1.9805, KL Loss: 0.0066\n",
            "Epoch 400, Loss: 1.9826, Reconstruction Loss: 1.9675, KL Loss: 0.0152\n",
            "Epoch 500, Loss: 1.9879, Reconstruction Loss: 1.9542, KL Loss: 0.0338\n",
            "Epoch 600, Loss: 2.0618, Reconstruction Loss: 1.9369, KL Loss: 0.1250\n",
            "Epoch 700, Loss: nan, Reconstruction Loss: nan, KL Loss: nan\n",
            "Epoch 800, Loss: nan, Reconstruction Loss: nan, KL Loss: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-c0e2712b86e4>:12: RuntimeWarning: overflow encountered in exp\n",
            "  return 1 / (1 + np.exp(-x))\n",
            "<ipython-input-4-c0e2712b86e4>:62: RuntimeWarning: overflow encountered in exp\n",
            "  kl_loss = -0.5 * np.mean(np.sum(1 + logvar - mu**2 - np.exp(logvar), axis=1))\n",
            "<ipython-input-4-c0e2712b86e4>:46: RuntimeWarning: overflow encountered in matmul\n",
            "  logvar = hidden_enc @ W_logvar + b_logvar\n",
            "<ipython-input-4-c0e2712b86e4>:54: RuntimeWarning: overflow encountered in matmul\n",
            "  hidden_dec = relu(z @ W_dec1 + b_dec1)\n",
            "<ipython-input-4-c0e2712b86e4>:55: RuntimeWarning: overflow encountered in matmul\n",
            "  reconstructed = sigmoid(hidden_dec @ W_out + b_out)\n",
            "<ipython-input-4-c0e2712b86e4>:62: RuntimeWarning: overflow encountered in square\n",
            "  kl_loss = -0.5 * np.mean(np.sum(1 + logvar - mu**2 - np.exp(logvar), axis=1))\n",
            "<ipython-input-4-c0e2712b86e4>:71: RuntimeWarning: invalid value encountered in matmul\n",
            "  d_W_out = hidden_dec.T @ d_reconstructed\n",
            "<ipython-input-4-c0e2712b86e4>:75: RuntimeWarning: overflow encountered in matmul\n",
            "  d_W_dec1 = z.T @ d_hidden_dec\n",
            "<ipython-input-4-c0e2712b86e4>:79: RuntimeWarning: overflow encountered in matmul\n",
            "  d_z = d_hidden_dec @ W_dec1.T\n",
            "<ipython-input-4-c0e2712b86e4>:81: RuntimeWarning: invalid value encountered in multiply\n",
            "  d_logvar = d_z * eps * 0.5 * np.exp(0.5 * logvar)\n",
            "<ipython-input-4-c0e2712b86e4>:84: RuntimeWarning: overflow encountered in matmul\n",
            "  d_W_mu = hidden_enc.T @ d_mu\n",
            "<ipython-input-4-c0e2712b86e4>:84: RuntimeWarning: invalid value encountered in matmul\n",
            "  d_W_mu = hidden_enc.T @ d_mu\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:88: RuntimeWarning: invalid value encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 900, Loss: nan, Reconstruction Loss: nan, KL Loss: nan\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Generate synthetic data for demonstration (e.g., a simple 2D Gaussian blob)\n",
        "np.random.seed(42)\n",
        "data = pd.DataFrame(np.random.randn(1000, 2), columns=[\"x1\", \"x2\"])\n",
        "print(data.head())\n",
        "data = data.values  # Convert DataFrame to NumPy array\n",
        "\n",
        "# Define helper functions\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "# Initialize weights and biases for encoder and decoder\n",
        "input_dim = 2       # Input features\n",
        "hidden_dim = 4      # Hidden layer size\n",
        "latent_dim = 2      # Latent space size\n",
        "\n",
        "# Encoder weights and biases\n",
        "W_enc1 = np.random.randn(input_dim, hidden_dim) * 0.1\n",
        "b_enc1 = np.zeros((1, hidden_dim))\n",
        "W_mu = np.random.randn(hidden_dim, latent_dim) * 0.1\n",
        "b_mu = np.zeros((1, latent_dim))\n",
        "W_logvar = np.random.randn(hidden_dim, latent_dim) * 0.1\n",
        "b_logvar = np.zeros((1, latent_dim))\n",
        "\n",
        "# Decoder weights and biases\n",
        "W_dec1 = np.random.randn(latent_dim, hidden_dim) * 0.1\n",
        "b_dec1 = np.zeros((1, hidden_dim))\n",
        "W_out = np.random.randn(hidden_dim, input_dim) * 0.1\n",
        "b_out = np.zeros((1, input_dim))\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.01\n",
        "epochs = 1000\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass\n",
        "    # 1. Encoder\n",
        "    hidden_enc = relu(data @ W_enc1 + b_enc1)\n",
        "    mu = hidden_enc @ W_mu + b_mu\n",
        "    logvar = hidden_enc @ W_logvar + b_logvar\n",
        "\n",
        "    # 2. Reparameterization trick\n",
        "    std = np.exp(0.5 * logvar)\n",
        "    eps = np.random.randn(*std.shape)  # Random noise\n",
        "    z = mu + eps * std  # Sampled latent vector\n",
        "\n",
        "    # 3. Decoder\n",
        "    hidden_dec = relu(z @ W_dec1 + b_dec1)\n",
        "    reconstructed = sigmoid(hidden_dec @ W_out + b_out)\n",
        "\n",
        "    # Compute the losses\n",
        "    # Reconstruction loss (MSE)\n",
        "    reconstruction_loss = np.mean(np.sum((data - reconstructed)**2, axis=1))\n",
        "\n",
        "    # KL divergence loss\n",
        "    kl_loss = -0.5 * np.mean(np.sum(1 + logvar - mu**2 - np.exp(logvar), axis=1))\n",
        "\n",
        "    # Total loss\n",
        "    total_loss = reconstruction_loss + kl_loss\n",
        "\n",
        "    # Backward pass (manual gradient calculation)\n",
        "    # Gradients for decoder\n",
        "    d_reconstructed = 2 * (reconstructed - data) / data.shape[0]  # Derivative of MSE\n",
        "    d_hidden_dec = d_reconstructed @ W_out.T * (hidden_dec > 0)\n",
        "    d_W_out = hidden_dec.T @ d_reconstructed\n",
        "    d_b_out = np.sum(d_reconstructed, axis=0, keepdims=True)\n",
        "\n",
        "    # Gradients for decoder weights\n",
        "    d_W_dec1 = z.T @ d_hidden_dec\n",
        "    d_b_dec1 = np.sum(d_hidden_dec, axis=0, keepdims=True)\n",
        "\n",
        "    # Gradients for encoder\n",
        "    d_z = d_hidden_dec @ W_dec1.T\n",
        "    d_mu = d_z\n",
        "    d_logvar = d_z * eps * 0.5 * np.exp(0.5 * logvar)\n",
        "\n",
        "    d_hidden_enc = (d_mu @ W_mu.T + d_logvar @ W_logvar.T) * (hidden_enc > 0)\n",
        "    d_W_mu = hidden_enc.T @ d_mu\n",
        "    d_b_mu = np.sum(d_mu, axis=0, keepdims=True)\n",
        "    d_W_logvar = hidden_enc.T @ d_logvar\n",
        "    d_b_logvar = np.sum(d_logvar, axis=0, keepdims=True)\n",
        "\n",
        "    # Update weights for encoder\n",
        "    W_mu -= learning_rate * d_W_mu\n",
        "    b_mu -= learning_rate * d_b_mu\n",
        "    W_logvar -= learning_rate * d_W_logvar\n",
        "    b_logvar -= learning_rate * d_b_logvar\n",
        "    W_enc1 -= learning_rate * (data.T @ d_hidden_enc)\n",
        "    b_enc1 -= learning_rate * np.sum(d_hidden_enc, axis=0, keepdims=True)\n",
        "\n",
        "    # Update weights for decoder\n",
        "    W_out -= learning_rate * d_W_out\n",
        "    b_out -= learning_rate * d_b_out\n",
        "    W_dec1 -= learning_rate * d_W_dec1\n",
        "    b_dec1 -= learning_rate * d_b_dec1\n",
        "\n",
        "    # Print loss\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {total_loss:.4f}, Reconstruction Loss: {reconstruction_loss:.4f}, KL Loss: {kl_loss:.4f}\")\n",
        "\n",
        "# Final output\n",
        "print(\"\\nTraining complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EJxbq2lDebbd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}